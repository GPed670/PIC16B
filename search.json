[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog!",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome!\n\nThis is a photo I took in Kauai, Hawaii this past summer."
  },
  {
    "objectID": "posts/hw6/index.html",
    "href": "posts/hw6/index.html",
    "title": "Natural Language Processing: How Text Classification can Identify Fake News",
    "section": "",
    "text": "With so much information at hand, it is hard to distinguishing the truth from lies. The rise of fake news, has blurred the lines between fact and fiction. Text classification is a powerful technique that can be used to identify fake news. These models are often called NLPs, or Natural Language Processors.\nFor this tutorial, we will be using data from the article:\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\nWhich can be accessed through Kaggle. Each row of the data corresponds to an article with the title column giving the title of the article, and the text column giving the full article text. The fake column is 0 if the article is true and 1 if the article contains fake news.\nBefore writing any code we need to import some essential libraries for our data manipulation and the creation of our models. Additionally, we will need to install and install and updated version of keras.\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 12.5 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\nNext, we need to download our training set, using the url provided, to a pandas dataframe.\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(train_url)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn order to clean our data for easier processing we want to remove unnecessary words like “and”, “or”, and “the” that don’t contribute much to the meaning of the text. We also want to make everything lowercase. Finally we want to put our dataframe into a tensorflow Dataset object. We can do all of this by defining a function make_dataset:\nAdditionally, we can easily import a list of stopwords using the snipet bellow.\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\ndef make_dataset(df):\n    \"\"\"\n    Preprocesses the input DataFrame for text classification.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing 'title', 'text', and 'fake' columns.\n\n    Returns:\n    - tf.data.Dataset: A TensorFlow Dataset containing preprocessed text and label tensors.\n    \"\"\"\n\n    # Make text all lowercase\n    df['text'] = df['text'].apply(lambda x: x.lower())\n    df['title'] = df['title'].apply(lambda x: x.lower())\n\n    # Remove stopwords\n    stop = stopwords.words('english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n    # Make a tf Dataset\n    title_tensor = tf.constant(df['title'].values, dtype=tf.string)\n    text_tensor = tf.constant(df['text'].values, dtype=tf.string)\n    fake_tensor = tf.constant(df['fake'].values, dtype=tf.int32)\n\n    dataset = tf.data.Dataset.from_tensor_slices(({\"title\": title_tensor, \"text\": text_tensor}, fake_tensor))\n\n    # Batch the dataset\n    dataset = dataset.batch(100)\n\n    return dataset\n\n\nds = make_dataset(df)\n\nNext, we want to split the dataset into a training set and a validation set with a 80-20 ratio. We can do this by using the take and skip methods of the tensorflow Dataset object. When passed a number n, take will take only the first n items in the dataset, and skip will skip the first n items.\n\n# Shuffle the data and determine the validation set size\nds = ds.shuffle(buffer_size = len(ds), reshuffle_each_iteration=False)\nval_size = int(0.2 * len(ds))\n\n# Split the dataset\nval = ds.take(val_size)\ntrain = ds.skip(val_size)\n\n\nlen(train), len(val)\n\n(180, 45)\n\n\nIn order to get a good idea of the performance of our machine learning models we can compare them to the base rate. The base rate provides a baseline understanding of the distribution of fake news. We can calculate the base rate by comparing the number of fake vs true articles.\n\nfake_count = 0\ntotal_count = 0\n\nfor _, labels in train:\n    fake_count += tf.reduce_sum(labels).numpy()  # Sum of 0s and 1s where 1 represents 'fake'\n    total_count += len(labels)\n\nbase_rate = fake_count / total_count\n\nprint(\"Base rate:\", base_rate)\n\nBase rate: 0.5254331717644437\n\n\nAs we can see a random guess would have around a 52% probability of being correct in determining if an article is fake or not."
  },
  {
    "objectID": "posts/hw6/index.html#model-creation",
    "href": "posts/hw6/index.html#model-creation",
    "title": "Natural Language Processing: How Text Classification can Identify Fake News",
    "section": "Model Creation:",
    "text": "Model Creation:\nWe will be creating three models to determine the answer to the question: “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?” 1. A model only using the title as input. 2. A model only using the test as imput. 3. A model using both as input.\nIn order to answer this question we will be using the Functional API. The Functional API allows you to define models with complex multiple inputs and outputs, shared layers, and non-linear connectivity patterns. These features are essential for the three models we want to create using our dataset\nTo start off, understanding and representing textual data in a machine-readable format is an important initial step for text classification. We can use TextVectorization layer in TensorFlow to achieve this.\nTo use TextVectorization we must first import these functions.\n\nimport keras\nimport re\nimport string\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\n\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\nvectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\nThe standardization function within the TextVectorization layer converts text to lowercase and removes punctuation, offering a uniform representation that lessens the impact of irrelevant information in the dataset. TextVectorization also building a vocabulary from the training data. This vocabulary is limited to a specified number of tokens, providing a manageable structure to help control complexity and prevent overfitting.\n\nshared_embedding_layer = layers.Embedding(size_vocabulary, 10, name=\"embedding\")\n\nWe also must add this embedding layer. The Embedding layer is a transformer that converts words into dense vectors, known as word embeddings. These vectors encode semantic relationships and contextual information, providing a representation of words within the vocabulary.\nBoth of these layers will be shared layers for both of our inputs, title and text, thanks to our use of the Functional API.\n\nModel 1: Article Titles Only\nIn this TensorFlow Keras model designed for text classification, the Functional API is employed to construct a neural network architecture for discerning between real and fake news based on input titles.\nThe model begins with an input layer, ‘title_input,’ representing the titles of the texts and specifying what it expects for its input. The next steps involve using text vectorization, converting the titles into integer sequences using the shared vectorization layer from before. The shared embedding layer is then applied to transform these integer-encoded words into dense vectors, allowing the model to capture relationships between words.\nNext, Dropout layers are strategically inserted to mitigate overfitting, and Global average pooling ensures a fixed-length output, regardless of input length for generalization. Then a series of Dense layers are used.\n\n# Assuming you have already defined the title_input\ntitle_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n\n# Create the model for title input\ntitle_features = vectorize_layer(title_input)\ntitle_features = shared_embedding_layer(title_features)\ntitle_features = layers.Dropout(0.5)(title_features)\ntitle_features = layers.Conv1D(128, 5, activation='relu')(title_features)\ntitle_features = layers.MaxPooling1D(5)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dense(64, activation='relu')(title_features)\ntitle_features = layers.Dropout(0.3)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\n# Define the output layer for title input\ntitle_output = layers.Dense(1, activation='sigmoid', name=\"title_output\")(title_features)\n\n# Create the model\nmodel1 = tf.keras.Model(inputs=title_input, outputs=title_output)\nmodel1.summary()\n\nModel: \"functional_21\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 10)             │          20,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_15 (Dropout)                 │ (None, 500, 10)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d (Conv1D)                      │ (None, 496, 128)            │           6,528 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d (MaxPooling1D)         │ (None, 99, 128)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_4           │ (None, 128)                 │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (Dense)                     │ (None, 64)                  │           8,256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_16 (Dropout)                 │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (Dense)                     │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ title_output (Dense)                 │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 36,897 (144.13 KB)\n\n\n\n Trainable params: 36,897 (144.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nThe summary function can be used to view the structure of model1, but, even better, we can use the utils function as shown below. The visualization provides an overview of the model’s architecture, including the connectivity between layers, the shapes of the tensors, and the names of each layer.\n\nfrom keras import utils\n\n\n# Visualize the structure of the model\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nFinaly we can compile an fit our data to the model:\n\n# Compile and fit the model\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory1 = model1.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 25s 123ms/step - accuracy: 0.5228 - loss: 0.6906 - val_accuracy: 0.7187 - val_loss: 0.5513\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 114ms/step - accuracy: 0.7714 - loss: 0.4684 - val_accuracy: 0.8996 - val_loss: 0.2606\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 117ms/step - accuracy: 0.9065 - loss: 0.2415 - val_accuracy: 0.9213 - val_loss: 0.2036\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 110ms/step - accuracy: 0.9260 - loss: 0.1990 - val_accuracy: 0.9244 - val_loss: 0.1949\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 115ms/step - accuracy: 0.9326 - loss: 0.1805 - val_accuracy: 0.9229 - val_loss: 0.1925\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 20s 111ms/step - accuracy: 0.9416 - loss: 0.1609 - val_accuracy: 0.9247 - val_loss: 0.1885\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 111ms/step - accuracy: 0.9438 - loss: 0.1541 - val_accuracy: 0.9271 - val_loss: 0.1850\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 116ms/step - accuracy: 0.9455 - loss: 0.1516 - val_accuracy: 0.9296 - val_loss: 0.1830\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 111ms/step - accuracy: 0.9480 - loss: 0.1425 - val_accuracy: 0.9313 - val_loss: 0.1792\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 116ms/step - accuracy: 0.9519 - loss: 0.1357 - val_accuracy: 0.9289 - val_loss: 0.1821\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 109ms/step - accuracy: 0.9543 - loss: 0.1254 - val_accuracy: 0.9322 - val_loss: 0.1754\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 22s 118ms/step - accuracy: 0.9530 - loss: 0.1275 - val_accuracy: 0.9340 - val_loss: 0.1755\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 110ms/step - accuracy: 0.9571 - loss: 0.1211 - val_accuracy: 0.9320 - val_loss: 0.1767\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 115ms/step - accuracy: 0.9573 - loss: 0.1191 - val_accuracy: 0.9331 - val_loss: 0.1750\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 114ms/step - accuracy: 0.9616 - loss: 0.1112 - val_accuracy: 0.9362 - val_loss: 0.1729\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 20s 109ms/step - accuracy: 0.9613 - loss: 0.1101 - val_accuracy: 0.9342 - val_loss: 0.1757\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 22s 115ms/step - accuracy: 0.9577 - loss: 0.1153 - val_accuracy: 0.9349 - val_loss: 0.1740\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 114ms/step - accuracy: 0.9602 - loss: 0.1106 - val_accuracy: 0.9376 - val_loss: 0.1740\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 41s 111ms/step - accuracy: 0.9625 - loss: 0.1020 - val_accuracy: 0.9371 - val_loss: 0.1734\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 21s 118ms/step - accuracy: 0.9625 - loss: 0.1033 - val_accuracy: 0.9344 - val_loss: 0.1776\n\n\nIn the visualization below, we can see the results of our training in a better format. As we can see, the model achieves a validation accuracy average around 93.5% using only the titles of the articles as input. Below, you can see that there is some overfitting of the model.\n\n# Visualize the results of the training using matplotlib\nplt.plot(history1.history['accuracy'], label='Training Accuracy')\nplt.plot(history1.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel 2: Article Text Only\nUsing the same structure of layers for the second model but using the article text instead, we can further determine the answer to our question.\n\n# Create the input layer for the article text\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n\n# Create the model for text input\ntext_features = vectorize_layer(text_input)\ntext_features = shared_embedding_layer(text_features)\ntext_features = layers.Dropout(0.5)(text_features)\ntext_features = layers.Conv1D(128, 5, activation='relu')(text_features)\ntext_features = layers.MaxPooling1D(5)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dense(64, activation='relu')(text_features)\ntext_features = layers.Dropout(0.3)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# Create the text_output final layer\ntext_output = layers.Dense(1, activation='sigmoid', name=\"text_output\")(text_features)\n\nmodel2 = tf.keras.Model(inputs=text_input, outputs=text_output)\nmodel2.summary()\n\nModel: \"functional_41\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 10)             │          20,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_36 (Dropout)                 │ (None, 500, 10)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_9 (Conv1D)                    │ (None, 496, 128)            │           6,528 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_8 (MaxPooling1D)       │ (None, 99, 128)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_12          │ (None, 128)                 │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_37 (Dense)                     │ (None, 64)                  │           8,256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_37 (Dropout)                 │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_38 (Dense)                     │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_output (Dense)                  │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 36,897 (144.13 KB)\n\n\n\n Trainable params: 36,897 (144.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Visualize the structure of the model\nutils.plot_model(model2, \"model2.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we compile and fit the model to the training data:\n\n# Compile and fit the model\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory2 = model2.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 73s 192ms/step - accuracy: 0.8695 - loss: 0.3665 - val_accuracy: 0.9636 - val_loss: 0.1192\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 37s 205ms/step - accuracy: 0.9802 - loss: 0.0599 - val_accuracy: 0.9724 - val_loss: 0.1042\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 32s 180ms/step - accuracy: 0.9892 - loss: 0.0372 - val_accuracy: 0.9711 - val_loss: 0.0961\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 31s 172ms/step - accuracy: 0.9901 - loss: 0.0293 - val_accuracy: 0.9711 - val_loss: 0.1045\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 34s 135ms/step - accuracy: 0.9924 - loss: 0.0253 - val_accuracy: 0.9727 - val_loss: 0.1057\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 25s 138ms/step - accuracy: 0.9914 - loss: 0.0253 - val_accuracy: 0.9707 - val_loss: 0.1118\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 135ms/step - accuracy: 0.9927 - loss: 0.0234 - val_accuracy: 0.9738 - val_loss: 0.1129\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 24s 133ms/step - accuracy: 0.9933 - loss: 0.0203 - val_accuracy: 0.9724 - val_loss: 0.1163\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 25s 138ms/step - accuracy: 0.9934 - loss: 0.0233 - val_accuracy: 0.9740 - val_loss: 0.1184\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 24s 134ms/step - accuracy: 0.9931 - loss: 0.0228 - val_accuracy: 0.9731 - val_loss: 0.1091\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 41s 132ms/step - accuracy: 0.9933 - loss: 0.0208 - val_accuracy: 0.9742 - val_loss: 0.1131\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 42s 137ms/step - accuracy: 0.9925 - loss: 0.0226 - val_accuracy: 0.9731 - val_loss: 0.1216\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 26s 143ms/step - accuracy: 0.9933 - loss: 0.0226 - val_accuracy: 0.9733 - val_loss: 0.1089\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 25s 136ms/step - accuracy: 0.9921 - loss: 0.0242 - val_accuracy: 0.9722 - val_loss: 0.1263\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 41s 137ms/step - accuracy: 0.9911 - loss: 0.0249 - val_accuracy: 0.9704 - val_loss: 0.1212\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 44s 153ms/step - accuracy: 0.9943 - loss: 0.0202 - val_accuracy: 0.9716 - val_loss: 0.1027\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 38s 136ms/step - accuracy: 0.9947 - loss: 0.0194 - val_accuracy: 0.9722 - val_loss: 0.1183\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 42s 139ms/step - accuracy: 0.9935 - loss: 0.0217 - val_accuracy: 0.9722 - val_loss: 0.1279\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 42s 147ms/step - accuracy: 0.9929 - loss: 0.0210 - val_accuracy: 0.9740 - val_loss: 0.1233\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 143ms/step - accuracy: 0.9940 - loss: 0.0184 - val_accuracy: 0.9704 - val_loss: 0.1188\n\n\nAs we can see bellow, this model achieves a better validation accuracy average than model1, at around 97.3%. This is probably because there is more information given by the article text than from the title, so more relationships between words can be determined. Additionally, there is once again some overfitting since adding another dropout layers increased the overfitting.\n\nplt.plot(history2.history['accuracy'], label='Training Accuracy')\nplt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 2: Texts')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel 3: Article Title and Text\nFinally, we will use both the title and text in this model. Since we are using Functional API, we can use the layers we created for each input in the previous models and combine them for this model using a concatinate layer as shown below:\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\n\nIn order to prevent overfitting and to stabalize the model I have added some more Dense and Dropout layers:\n\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dropout(0.5)(main)\noutput = layers.Dense(1, name = \"fake\")(main)\n\nThen, we define the model with both the title input and text input:\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\n\nmodel3.summary()\n\nModel: \"functional_43\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 10)        │         20,000 │ text_vectorization_1[… │\n│                           │                        │                │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_15 (Dropout)      │ (None, 500, 10)        │              0 │ embedding[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_36 (Dropout)      │ (None, 500, 10)        │              0 │ embedding[9][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d (Conv1D)           │ (None, 496, 128)       │          6,528 │ dropout_15[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_9 (Conv1D)         │ (None, 496, 128)       │          6,528 │ dropout_36[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d             │ (None, 99, 128)        │              0 │ conv1d[0][0]           │\n│ (MaxPooling1D)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d_8           │ (None, 99, 128)        │              0 │ conv1d_9[0][0]         │\n│ (MaxPooling1D)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 128)            │              0 │ max_pooling1d[0][0]    │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 128)            │              0 │ max_pooling1d_8[0][0]  │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_18 (Dense)          │ (None, 64)             │          8,256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_37 (Dense)          │ (None, 64)             │          8,256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_16 (Dropout)      │ (None, 64)             │              0 │ dense_18[0][0]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_37 (Dropout)      │ (None, 64)             │              0 │ dense_37[0][0]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_19 (Dense)          │ (None, 32)             │          2,080 │ dropout_16[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_38 (Dense)          │ (None, 32)             │          2,080 │ dropout_37[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_8             │ (None, 64)             │              0 │ dense_19[0][0],        │\n│ (Concatenate)             │                        │                │ dense_38[0][0]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_39 (Dense)          │ (None, 32)             │          2,080 │ concatenate_8[0][0]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_38 (Dropout)      │ (None, 32)             │              0 │ dense_39[0][0]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 1)              │             33 │ dropout_38[0][0]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 55,841 (218.13 KB)\n\n\n\n Trainable params: 55,841 (218.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nThrough this visualization of the model architecture, we can see how Functional API implements the layers of the different imputs and finally combines everything using the concatinatelayer. This visualization really highlights the advantiges of using Functional API to create an complex neural network.\n\nutils.plot_model(model3, \"model3.png\",\n                 show_shapes=True,\n                 show_layer_names=True,\n                 rankdir='LR') # output it left to right because does not output otherwise because of complexity\n\n\n\n\n\n\n\n\nNext, we can compile and fit the model to the training data:\n\nmodel3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory3 = model3.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 49s 250ms/step - accuracy: 0.9180 - loss: 0.8561 - val_accuracy: 0.9660 - val_loss: 0.4228\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 81s 244ms/step - accuracy: 0.9904 - loss: 0.0998 - val_accuracy: 0.9684 - val_loss: 0.4066\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 83s 250ms/step - accuracy: 0.9848 - loss: 0.1587 - val_accuracy: 0.9724 - val_loss: 0.3130\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 81s 243ms/step - accuracy: 0.9897 - loss: 0.0756 - val_accuracy: 0.9673 - val_loss: 0.4252\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 80s 233ms/step - accuracy: 0.9862 - loss: 0.1294 - val_accuracy: 0.9749 - val_loss: 0.2787\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 42s 233ms/step - accuracy: 0.9941 - loss: 0.0418 - val_accuracy: 0.9736 - val_loss: 0.3162\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 87s 262ms/step - accuracy: 0.9907 - loss: 0.0670 - val_accuracy: 0.9704 - val_loss: 0.2943\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 78s 240ms/step - accuracy: 0.9934 - loss: 0.0567 - val_accuracy: 0.9733 - val_loss: 0.2928\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 82s 240ms/step - accuracy: 0.9949 - loss: 0.0374 - val_accuracy: 0.9667 - val_loss: 0.3318\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 82s 240ms/step - accuracy: 0.9922 - loss: 0.0568 - val_accuracy: 0.9716 - val_loss: 0.3410\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 43s 238ms/step - accuracy: 0.9946 - loss: 0.0410 - val_accuracy: 0.9756 - val_loss: 0.2609\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 82s 236ms/step - accuracy: 0.9728 - loss: 0.1577 - val_accuracy: 0.9724 - val_loss: 0.2557\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 84s 244ms/step - accuracy: 0.9817 - loss: 0.1109 - val_accuracy: 0.9744 - val_loss: 0.1485\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 83s 252ms/step - accuracy: 0.9838 - loss: 0.0637 - val_accuracy: 0.9727 - val_loss: 0.2564\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 82s 255ms/step - accuracy: 0.9893 - loss: 0.0625 - val_accuracy: 0.9731 - val_loss: 0.2256\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 80s 243ms/step - accuracy: 0.9925 - loss: 0.0436 - val_accuracy: 0.9749 - val_loss: 0.2388\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 83s 246ms/step - accuracy: 0.9903 - loss: 0.0434 - val_accuracy: 0.9738 - val_loss: 0.2468\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 45s 250ms/step - accuracy: 0.9932 - loss: 0.0275 - val_accuracy: 0.9760 - val_loss: 0.2479\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 44s 243ms/step - accuracy: 0.9931 - loss: 0.0347 - val_accuracy: 0.9751 - val_loss: 0.2711\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 46s 257ms/step - accuracy: 0.9940 - loss: 0.0421 - val_accuracy: 0.9747 - val_loss: 0.2884\n\n\nAs we can see through the visualization, this model has an average validation accuracy of around 97.4%. This is better than both of the validation accuracies of model1 and model2, but it seems less stable than model2 as it fluctuates more between epochs.\n\nplt.plot(history3.history['accuracy'], label='Training Accuracy')\nplt.plot(history3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTesting on the Best Model:\nAs we concluded above, model3 has the best average validation accuracy at the end, but model 2 is more stable. For now, we will only use model3 to evaluate the test data.\nFirst, we need to dowload the test data and put it into the proper format using the make_dataset function we defined previously.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\n\n# Read the CSV file into a DataFrame\ndf_test = pd.read_csv(test_url)\ndf_test.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n420\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\n14902\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\n322\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\n16108\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\n10304\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntest_ds = make_dataset(df_test)\n\nNext we can evaluate the test data using the model. As can be seen below, it achieves an accuracy of 97.37% similar to the average of the validation accuracy.\n\ntest_loss, test_accuracy = model3.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 14s 62ms/step - accuracy: 0.9737 - loss: 0.3040\nTest Accuracy: 97.46%\n\n\n\n\nVisualize Embedding:\nFinally, we want to visualize the embedding done by our shared embedding layer in our model3. This will give us insight into the relationship our model determines between words in the vocabulary.\nIn order to visualize this we can use sklearn’s PCA, Principal Component Analysis. PCA transform a dataset with correlated features into a new set of uncorrelated features, known as principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain in the data.\n\nweights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nThen, we can use a plotly scatterplot to visualize these relationships:\n\nimport plotly.express as px\nimport numpy as np\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\",\n                 title='Word Embeddings Visualization')\n\nfig.update_layout(\n    xaxis=dict(title='Principal Component 1'),\n    yaxis=dict(title='Principal Component 2'),\n)\n\nfig.show()\n\n\n\n\nIn the visualization, words that are close together might have similar semantic meanings or play a significant role in distinguishing between real and fake news. For example, “interview” and “jobs” are closely grouped on the lower half of the center vertical axis which is probably because they are often words found in the same articles and in the same context. Also, “gay,” “transgender,” and “democrat” are grouped near the center on the horizontal axis which might suggest that the model might classify them as having similar political meanings.\n\n\nConclusion:\nIn conclusion, natural language processors and text classification can be a powerful tool for determining if an article contains fake news or not. Model3 resulted in a 97.46% test accuracy using both the article title and the article text. This was achieved by using Functional API features such as shared layers and multiple inputs and by using embeding and word processing to define the semantic relationships between different words."
  },
  {
    "objectID": "posts/hw4/index.html",
    "href": "posts/hw4/index.html",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "",
    "text": "Building upon the one-dimensional heat diffusion example from our lecture notes, where we represented one-dimensional heat diffusion as a sequence of matrix-vector multiplications, now we will foccus on modeling 2-D heat diffusion.\nTwo-dimensional heat equation: \\[\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2 f}{\\partial x^2 }+ \\frac{\\partial^2 f}{\\partial y^2 }\\;.\\]\nIn a discrete approximation, we can write this as \\[x_i = i \\Delta x,\\;\\; y_j = j \\Delta y,\\;\\; t_k = k \\Delta t\\;,\\] for \\[i = 0, \\cdots, N-1; j = 0, \\cdots, N-1; k = 0, 1, 2 \\cdots\\] Let \\[u_{i, j}^k = f(x_i, y_j, t_k)\\] then we can write the update equation in real-time as \\[u_{i, j}^{k+1} \\approx u_{i, j}^k + \\epsilon \\left(u_{i+1, j}^k  + u_{i-1, j}^k + u_{i, j+1}^k + u_{i, j-1}^k - 4 u_{i, j}^k\\right),\\] where epsilon is a small parameter.\n# We will use:\nN = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/hw4/index.html#part-1-matrix-multiplication-method",
    "href": "posts/hw4/index.html#part-1-matrix-multiplication-method",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "Part 1: Matrix Multiplication Method",
    "text": "Part 1: Matrix Multiplication Method\nAs described above let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution. Each iteration of the update is created by advance_time_matvecmul using matrix-vector multiplication.\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    # update u\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThen using the indexing of u we create a matrix A that has the size of N^2 x N^2, without all-zero rows or all-zero columns. We use the get_A function to create the matrix.\n\n\ndef get_A(N):\n    \"\"\"\n    Takes the value N as the argument and returns the corresponding matrix A\n    Args:\n        N: simulation size\n        \n    Returns:\n        A: the corresponding matrix\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    \n    # add together arrays to create N*N matrix\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    \n    # return matrix\n    return A\n\n\n\nNow we will initialize and visualize u0, the initial condition of 1 unit of heat at the midpoint of the visualization. We will also create the matric A using get_A.\n\n# Initialize A and other parameters as needed\nA = get_A(N)\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\nplt.imshow(u0)\nplt.title('Initial Condition')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nNow we will use a time-iteration loop for simulating heat diffusion in a 2D space using matrix-vector multiplication. The simulation is performed for a total of 2700 iterations, with results visualized at every 300th interval. The advance_time_matvecmul function updates the temperature distribution at each iteration based on the discrete heat diffusion equation.\n%%timeit -r 1 -n 1: This line is a Jupyter magic command (%%timeit) that measures the execution time of the subsequent code. -r 1 specifies a single run, and -n 1 indicates a single execution per run. The execution time is measured using the %timeit magic command, providing insights into the computational efficiency of the simulation.\n\n%%timeit -r 1 -n 1\nu = u0\nsubplot_num = 1\nsolution_array1 = [u0.copy()]\n\nfor t in range(1, 2701):\n    if t % 300 == 0:\n        solution_array1.append(u.copy())\n        plt.subplot(3, 3, subplot_num)\n        plt.imshow(u)\n        plt.title(f'Iteration {t}')\n        subplot_num += 1\n    u = advance_time_matvecmul(A, u, epsilon)\n\n2min 10s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nHere is the result of the loop we ran and we can see the visualizations model the diffusion of the heat from the 300th iteration to the 2700th. As we can see it took around two minutes for the code to create the solutions which is a long time, so we can try other methods to see if we can create solutions faster."
  },
  {
    "objectID": "posts/hw4/index.html#part-2-jax-sparce-matrix-method",
    "href": "posts/hw4/index.html#part-2-jax-sparce-matrix-method",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "Part 2: JAX Sparce Matrix Method",
    "text": "Part 2: JAX Sparce Matrix Method\nNow we want the improve the performance inefficiency observed in Part 1. This optimization strategy involves using the sparse matrix data structure with the batched coordinate (BCOO) format, which efficiently handles matrices with many zero elements. The JAX package is utilized for this purpose. The goal is to redefine the matrix A in sparse format using the function get_sparse_A(N) and then repeat the simulation from Part 1 using the sparse matrix representation along with the JIT-compiled version of the advance_time_matvecmul function. This approach significantly reduces space usage and accelerates the update computations.\n\nfrom jax import jit\nimport jax.numpy as jnp\n\n\n\ndef get_sparse_A(N):\n    \"\"\"\n    Generates a sparse matrix in BCOO format from a dense matrix.\n\n    Parameters:\n    - N (int): The size of the square dense matrix to be generated.\n\n    Returns:\n    sparse.BCOO: A sparse matrix in BCOO format.\n    \"\"\"\n    \n    # Get dense matrix A\n    dense_A = get_A(N)\n\n    # Convert dense matrix to sparse BCOO format\n    A_sp_matrix = sparse.BCOO.fromdense(dense_A)\n\n    return A_sp_matrix\n\n\n\nUsing the similar code as with Part 1 but replacing get_A with get_sparse_A and using the jit version of advance_time_matvecmul:\n\nA = get_sparse_A(N)\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n\n%%timeit -r 1 -n 1\nu = jnp.array(u0)\nsubplot_num = 1\nsolution_array2 = [u0.copy()]\n\nfor t in range(1, 2701):\n    if t % 300 == 0:\n        solution_array2.append(u.copy())\n        plt.subplot(3, 3, subplot_num)\n        plt.imshow(u)\n        plt.title(f'Iteration {t}')\n        subplot_num += 1\n    u = advance_time_matvecmul_jit(A, u, epsilon)\n\n1.77 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nWe can see that using this method we obtain the same visualizations; however, the it only took around 2 seconds to run in comparison to the 2 minutes of Part 1. This is much better, but we can still improve the efficiency."
  },
  {
    "objectID": "posts/hw4/index.html#part-3-direct-operation-with-numpy-method",
    "href": "posts/hw4/index.html#part-3-direct-operation-with-numpy-method",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "Part 3: Direct Operation with Numpy Method",
    "text": "Part 3: Direct Operation with Numpy Method\nAlternatively, instead of relying on matrix-vector multiplication, we can use vectorized array operations, specifically the np.roll() function, to advance the solution by one timestep. This approach is particularly relevant for problems like the heat equation, where the use of matrix-vector multiplication may not be a necessity in terms of computation. The function, advance_time_numpy(u, epsilon), is designed to advance the solution using a padded array and np.roll().\n\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of a 2D grid-based diffusion process over a given time step using NumPy.\n\n    Args:\n        u (numpy.ndarray): The input 2D array representing the current state of the simulation.\n        epsilon (float): The diffusion coefficient.\n\n    Returns:\n        numpy.ndarray: The updated 2D array representing the state of the simulation after the time step.\n    \"\"\"\n    \n    # Pad the input array with zeros to form an (N+2) x (N+2) array internally\n    pad_u = np.pad(u, 1, mode='constant', constant_values=0)\n\n    # Roll the padded array along both axes to simulate diffusion\n    u_shift_right = np.roll(pad_u, shift=1, axis=1)\n    u_shift_left = np.roll(pad_u, shift=-1, axis=1)\n    u_shift_up = np.roll(pad_u, shift=-1, axis=0)\n    u_shift_down = np.roll(pad_u, shift=1, axis=0)\n\n    # Update the solution using vectorized array operations and return to NxN\n    u_shifted = (u_shift_right + u_shift_left + u_shift_up + u_shift_down)[1:-1, 1:-1]\n    u_new = u + epsilon * (u_shifted - 4 * u)\n\n    return u_new\n\n\n\n\npad_u = np.pad(u, 1, mode=‘constant’, constant_values=0): Pads the input array u with zeros to form an (N+2) x (N+2) array internally. The padding ensures that the edges of the grid are considered during diffusion.\nnp.roll(pad_u, shift=1, axis=1): Rolls the padded array to the right along the second axis. This is used for all directions of the array.\n\nUsing the similar code as with Part 1 and 2 but updating the solution u using advance_time_numpy:\n\n%%timeit -r 1 -n 1\nu = u0\nsubplot_num = 1\nsolution_array3 = [u0.copy()]\n\nfor t in range(1, 2701):\n    if t % 300 == 0:\n        solution_array3.append(u.copy())\n        plt.subplot(3, 3, subplot_num)\n        plt.imshow(u)\n        plt.title(f'Iteration {t}')\n        subplot_num += 1\n    u = advance_time_numpy(u, epsilon)\n\n986 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nWe can see from the time mesurement that this method is slightly faster than the previous method by almost 1 second."
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "",
    "text": "Structured Query Language, or SQL for short, is a programming language designed for managing and manipulating relational databases. In a relational database, data is stored in tables, which consist of rows and columns. The relationships between tables are defined by using keys, such as primary keys and foreign keys. A primary key is a unique identifier for each record in a table. It ensures that each row can be uniquely identified and allows for the creation of relationships between tables. A foreign key is a field in one table that refers to the primary key in another table. SQL establishes relationships between tables, allowing data to be linked across tables using these keys. It provides a way for users to interact with databases, to retrieve and manipulate data. Users retrieve data from databases by writing queries which specify conditions to filter and sort the data.\n\n\n\nUsing databases can be more efficient than working only with Pandas DataFrames because databases offer long-term storange and handle large datasets more efficiently.\n\n# import necessary libraries\nimport pandas as pd\nimport sqlite3\n\n\nconn = sqlite3.connect(\"climate2.db\") # create a database in current directory called climate.db\n\n\n# create a dataframe iterator for the temperature data \ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf_iter\n\n&lt;pandas.io.parsers.readers.TextFileReader at 0x14a375f3550&gt;\n\n\nUsing pd.read_csv() with the chunksize parameter is used when dealing with large datasets that may not fit into memory. It allows you to read and process the data in smaller, manageable chunks rather than loading the entire dataset at once.\ndf_iter returns an iterator, enabling you to process the data iteratively. You can perform operations on each chunk without loading the entire dataset into memory. df_iter is initialized with pd.read_csv(), specifying the file path and chunksize. It returns an iterator object where each element corresponds to a chunk of data with the specified number of rows (chunksize).\nTo access each chunk, you can iterate over df_iter using the next() magic method. The actual data is read and processed on-the-fly during the iteration which allows you to work with one chunk at a time.\n\ndf = df_iter.__next__()\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ndf.shape\n\n(100000, 14)\n\n\nThe prepare_df function below reorganizes the structure of the DataFrame (df) and performs some data manipulation steps so that each temperature reading has its own row with the corresponding station, month, and year. The transformed DataFrame has a new structure with the columns “ID,” “Year,” “Month,” and “Temp.”\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf = prepare_df(df)\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\nThis next portion is a loop that iterates over chunks of a DataFrame iterator (df_iter). It transforms each chunk by using the prepare_df function and then writes the resulting DataFrame to a SQL table named “temperatures.” The following blocks of code use the station-metadata.csv and the fips-country-codes.csv files to create the stations and countries tables in the same SQL database.\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\n\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    \nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\nstations = pd.read_csv(\"fips-10-4-to-iso-country-codes.csv\")\nstations.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nHere we can see the results of the previous commands and can see all of the different tables and their categories in the database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\nconn.commit()  #commit() is a method used to finalize and confirm any \n               #        changes made during a series of database operations within a transaction.\nconn.close()   #close() is a method used to terminate the connection to the database"
  },
  {
    "objectID": "posts/hw1/index.html#visualization-1-average-temperature-over-time",
    "href": "posts/hw1/index.html#visualization-1-average-temperature-over-time",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 1: Average Temperature Over Time",
    "text": "Visualization 1: Average Temperature Over Time\nThe average_temperature_line_plot function creates a line plot using Plotly to visualize the average temperature over time for a specific country within a given year range. The function queries db_file using the avg_temp_years_query function to retrieve temperature data for a specified country and time range and calculates the average temperature for each year. Then it creates a line graph using the plotly method line.\n\nfrom climate_database import avg_temp_years_query\nprint(inspect.getsource(avg_temp_years_query))\n\ndef avg_temp_years_query(db_file, country, year_begin, year_end):\n    \n    \"\"\"\n    Query the climate database to calculate the average temperature for each year within a specified country and time range.\n\n    Parameters:\n    - db_file (str): The path to the SQLite database file.\n    - country (str): The name of the country for which average temperature data is requested.\n    - year_begin (int): The starting year of the desired time range.\n    - year_end (int): The ending year of the desired time range.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing average temperature data for each year, including columns 'Year' and 'avg_temp'.\n\n    Note:\n    - The function performs a SQL query to join temperature, station, and country data based on common IDs.\n    - It filters data based on the specified country and time range.\n    - The resulting DataFrame includes information about the average temperature for each year.\n    \"\"\"\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n        SELECT t.Year, AVG(t.Temp) AS avg_temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.ID\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY year\n    \"\"\"\n    \n    # Execute the query and fetch the results into a Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    \n    return df\n\n\n\n\nfrom climate_database import average_temperature_line_plot\nprint(inspect.getsource(average_temperature_line_plot))\n\ndef average_temperature_line_plot(db_file, country, year_begin, year_end):\n    \n    \"\"\"\n    Generate a line plot using Plotly Express to visualize the average temperature over time for a specified country.\n\n    Parameters:\n    - db_file (str): The path to the SQLite database file.\n    - country (str): The name of the country for which average temperature data is visualized.\n    - year_begin (int): The starting year of the desired time range.\n    - year_end (int): The ending year of the desired time range.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A line plot representing the average temperature over time for the specified country.\n\n    Note:\n    - The function uses the avg_temp_years_query function to retrieve average temperature data.\n    - The 'avg_temp' column is rounded to 2 decimal places for clarity.\n    - The line plot is created using Plotly Express with the x-axis representing years and y-axis representing average temperature.\n    - The plot has a title indicating the country and the specified time range.\n    \"\"\"\n    \n    # Get data using the query function\n    data = avg_temp_years_query(db_file, country, year_begin, year_end)\n    \n    data['avg_temp'] = data['avg_temp'].round(2)\n\n    # Create line plot with facets using Plotly Express\n    fig = px.line(data, \n                  x='Year', \n                  y='avg_temp',\n                  labels={'avg_temp': 'Average Temperature (°C)'},\n                  title=f'Average Temperature Over Time in {country} ({year_begin}-{year_end})')\n\n    return fig\n\n\n\nThe line plot allows users to observe trends and fluctuations in the average temperature over time. It provides insights into the climate patterns of the selected country during the specified period. Here is an example of the increase in temperature over the years in Thailand. This type of visualization could be useful for observing climate change and how it affects different countries.\n\nfig_line = average_temperature_line_plot(\"climate2.db\", \"Thailand\", 1980, 2020)\nfig_line.show()"
  },
  {
    "objectID": "posts/hw1/index.html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "href": "posts/hw1/index.html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 2: Bar Chart of Average Temperatures Each Month",
    "text": "Visualization 2: Bar Chart of Average Temperatures Each Month\nThe monthly_temperature_bar_chart creates a multifaceted bar chart that shows the monthly temparute for a given country throughout a range of years. In order to create this bar chart we must first find the global average temperature in order to range the colors of the bars correctly.\nThe global_average_temperature function simplifies the process of retrieving and calculating the average temperature across all records in the “temperatures” table of an SQLite database\n\nfrom climate_database import global_average_temperature\nprint(inspect.getsource(global_average_temperature))\n\ndef global_average_temperature(db_file):\n    \"\"\"\n    Calculate the global average temperature from a database.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n\n    Returns:\n    - float: Global average temperature in Celsius.\n    \"\"\"\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # SQL query to calculate the global average temperature\n    query = \"\"\"\n        SELECT AVG(Temp) AS global_avg_temp\n        FROM temperatures\n    \"\"\"\n\n    # Execute the query and fetch the result\n    result = conn.execute(query).fetchone()\n    \n    # Close the database connection\n    conn.close()\n\n    # Extract the global average temperature from the result\n    global_avg_temp = result[0]\n\n    return global_avg_temp\n\n\n\nThen we can query the database and create a bar chart with avg_monthly_temperature_bar_chart for the average temoerature of each month in a given range of years.\nFirst, the function connectes to the database specified by db_file. Then, it uses an SQL query to retrieve the temperature for each month within the specified country and year range. The query is executed using the pd.read_sql_query method, which putts the data into a DataFrame (df). Next, the month numbers are mapped to the corresponding month names. The function calls the global_average_temperature function to calculate the global average temperature from the entire dataset, so that it can center the color scheme correctly with, with white being the average, red being hotter temperatures, and blue being cooler temperstures.Then it uses the plotly bar function to create a multifaceted bar chart using the facet_col parameter.\n\nfrom climate_database import monthly_temperature_bar_chart\nprint(inspect.getsource(monthly_temperature_bar_chart))\n\ndef global_average_temperature(db_file):\n    \"\"\"\n    Calculate the global average temperature from a database.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n\n    Returns:\n    - float: Global average temperature in Celsius.\n    \"\"\"\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # SQL query to calculate the global average temperature\n    query = \"\"\"\n        SELECT AVG(Temp) AS global_avg_temp\n        FROM temperatures\n    \"\"\"\n\n    # Execute the query and fetch the result\n    result = conn.execute(query).fetchone()\n    \n    # Close the database connection\n    conn.close()\n\n    # Extract the global average temperature from the result\n    global_avg_temp = result[0]\n\n    return global_avg_temp\n\n\n\nIn summary, this function connects to the database, retrieves monthly temperature data for a specified country and year range, and creates an interactive, multifacted bar chart using Plotly, where the color of each bar represents the temperature value. The resulting visualization provides an representation of how monthly temperatures vary over a range of years for the selected country.\nThis visualization could help answer the question “How does the average monthly temperature vary over the specified years for a particular country?” This visualization provides insights into the seasonal temperature changes in different countries over certain years. By examining the color of the bars, users can identify periods of higher or lower average temperatures throughout the specified time range.\nBelow is an example for the function in use to find the temperatures of each month in Finland between 2000 and 2020 and Egypt between 1996 and 1999:\n\nfig = monthly_temperature_bar_chart('climate2.db', 'Finland', 2000, 2003)\nfig.show()\n\n                                                \n\n\n\nfig = monthly_temperature_bar_chart('climate2.db', 'Egypt', 1996, 1999)\nfig.show()"
  },
  {
    "objectID": "posts/final_project/index.html",
    "href": "posts/final_project/index.html",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "",
    "text": "The goal of this project was to create a web application that allows users to discover new recipes based on the produce they have in their fridge. Users can upload images of their available produce and select their preferred cuisine. The application then uses a machine learning model to identify the ingredients from the images and compares them with a dataset of recipes. The website subsequently displays recipes from the chosen cuisine that match the identified ingredients, ranked from most to least matching. This application aims to prevent food waste and provide an opportunity for users to explore new cultures and cuisines.\nThis web application can be particularly beneficial to college students, like us, who are looking to save money. Many college students have limited budgets and may not have much cooking experience. By using this application, they can easily find recipes that match the ingredients they already have, reducing the need to buy more food. This not only helps to minimize food waste but also saves money that would otherwise be spent on unused ingredients. Moreover, the application’s ability to suggest recipes from different cuisines introduces users to a variety of dishes they might not have tried before. This could potentially save them money on dining out, as they can recreate these dishes at home. Furthermore, learning to cook diverse recipes can be a valuable skill that continues to save money in the long run.\nThe project requires several tasks to be completed. First, web scraping is needed to gather different recipes from various cuisines from all recipes. Additionally, images of different ingredients need to be web-scraped to train our model. Second, research and implementation of an advanced image classification model are necessary to recognize the different ingredients in the images uploaded by the user. Finally, a web application needs to be developed where users can upload their images and receive recipe recommendations.\nAlong the way, some changes were made to the initial plan. The focus was narrowed down to produce because there are too many possible ingredients that a user could upload, and it is challenging to web scrape many pictures of the same thing. Additionally, the database of recipes was limited to around 2000 to simplify the recommendation algorithm. These changes were necessary to ensure the feasibility and effectiveness of the application."
  },
  {
    "objectID": "posts/final_project/index.html#i.-recipes",
    "href": "posts/final_project/index.html#i.-recipes",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "I. Recipes",
    "text": "I. Recipes\nOne of the first steps for this project is scraping a a dataset of recipes in order to recommend recipes to the user of the web app. We chose to scrape the recipes from AllRecipes.com because it specifically has a cuisines page that lists 49 different cuisines, and each recipe page has a detailed list of ingredients. One downfall of scraping this page that we noticed later on was that some common cuisines are missing from the main page, such as Mexican cuisine. However, we chose to keep the database to only these recipes in order to simplify the recommendation system since the other pages do not order the recipes by cuisine which we wanted to be one of the determining factors for recipe recommendation.\nFurthermore, choosing an appropriate webscraper was also important for this task. We used BeautifulSoup which is a Python library that is specifically designed for parsing HTML and XML documents, making it ideal for web scraping. Additionally, it provides simple methods and code for navigating, searching, and modifying a parse tree. BeautifulSoup also allows for direct scraping within a notebook rather than needing to define separate Python file, like with Proxy. Working within a notebook simplifies debugging and makes it easier to run the code.\nThis is a simple overview of how the web scraper function scrape_allREcipes_cuisines() scrapes the page: - Step 1: Scrape through the main cuisine page and make a dataframe of all of the cuisines and their URLs. - Step 2: Loop through the list and scrape the cuisine URLs for recipe URLs and make a new dataframe with the cuisine type, recipe name, and recipe page URL. - Step 3: Loop through the new list and scrape the recipe pages for the ingredient lists and add them to the list defined in the previous step.\n\n\ndef scrape_allRecipes_cuisines():\n  \"\"\"\n  Scrapes recipe data from Allrecipes.com based on different cuisines.\n\n  Returns:\n  pandas.DataFrame: DataFrame containing the scraped recipe information including Name, URL, Cuisine, and Ingredients.\n  \"\"\"\n\n  # URL of the page listing all cuisines on Allrecipes.com\n  url = 'https://www.allrecipes.com/cuisine-a-z-6740455'\n  result = requests.get(url)\n  doc = BeautifulSoup(result.text, \"html.parser\")\n  cuisines = doc.select('ul.loc.mntl-link-list a')\n\n  # Send a GET request to the page listing all cuisines\n  cuisine_dict = {}\n  for link in cuisines:\n      cuisine = link.get_text(strip=True)\n      url = link['href']\n      cuisine_dict[cuisine] = url\n\n  # Parse the HTML content of the page\n  df = pd.DataFrame(list(cuisine_dict.items()), columns=['Cuisine', 'URL'])\n\n  # Create an empty list to store recipe information\n  recipes_data = []\n\n  # Iterate over rows in the cuisine DataFrame\n  for index, row in df.iterrows():\n      cuisine_url = row['URL']\n      cuisine = df['Cuisine'][index]\n      result = requests.get(cuisine_url)\n      doc = BeautifulSoup(result.text, 'html.parser')\n\n      # {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card--image-top card card--no-image'}\n      # Extract information for each recipe\n      recipe_info1 = doc.find_all('a', {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card card--no-image'})\n      recipe_info2 = doc.find_all('a', {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card--image-top card card--no-image'})\n      recipe_info = recipe_info1 + recipe_info2\n\n      # Iterate over each recipe and extract relevant information\n      for recipe_card in recipe_info:\n          name = recipe_card.find('span', {'class': 'card__title-text'}).text.strip()\n          url = recipe_card['href']\n\n          # Extract information from individual recipe URLs\n          if not pd.isna(url):\n              result2 = requests.get(url)\n              doc2 = BeautifulSoup(result2.text, 'html.parser')\n\n              # Create a list to store ingredients\n              ingredients_list = []\n              ingredients_container = doc2.find('div', {'class': 'mntl-lrs-ingredients'})\n\n              # Check if the container is found\n              if ingredients_container:\n                  # Find the list of ingredients\n                  ingredients_list_element = ingredients_container.find('ul', {'class': 'mntl-structured-ingredients__list'})\n\n                  # Check if the list of ingredients is found\n                  if ingredients_list_element:\n                      # Extract and append each ingredient to the list\n                      for ingredient_item in ingredients_list_element.find_all('li', {'class': 'mntl-structured-ingredients__list-item'}):\n                          ingredient = ingredient_item.find('span', {'data-ingredient-name': 'true'})\n                          quantity = ingredient_item.find('span', {'data-ingredient-quantity': 'true'})\n                          unit = ingredient_item.find('span', {'data-ingredient-unit': 'true'})\n\n                          if ingredient and quantity and unit:\n                              ingredient_text = f\"{quantity.text.strip()} {unit.text.strip()} {ingredient.text.strip()}\"\n                              ingredients_list.append(ingredient_text)\n\n              # Append recipe information to the list\n              recipes_data.append({\n                  'Name': name,\n                  'URL': url,\n                  'Cuisine': cuisine,\n                  'Ingredients': ingredients_list,\n              })\n\n  # Create a DataFrame from the list of recipes\n  recipes_df = pd.DataFrame(recipes_data)\n  return recipes_df\n\n\n\nThe function scrape_allRecipes_cuisines() is designed to gather recipe data from Allrecipes.com cuisine page. It operates by first accessing a URL listing various cuisines on the website. It then extracts the names and corresponding URLs of these cuisines. Subsequently, for each cuisine, the function accesses its specific URL to retrieve recipe information. It identifies recipe details such as name and URL by parsing the HTML content. Upon obtaining a recipe’s URL, it accesses the individual recipe page to extract the ingredients list. The function then compiles all gathered data into a structured format, returning a DataFrame containing recipe names, URLs, associated cuisines, and ingredients lists.\nIt moves through the pages as shown in the order bellow:\n\nAll Cuisines Page:\n\n\n\n\n\n\n\n\n\n\n\nSpecific Cuisine Page\n\n\n\n\n\n\n\n\n\n\n\nRecipe Page:\n\n\n\n\n\n\n\n\n\n\nBy running this function we produce a dataset like this:\n\n\n\n  \n    \n\n\n\n\n\n\nName\nURL\nCuisine\nIngredients\n\n\n\n\n0\nBest Vinegar Coleslaw\nhttps://www.allrecipes.com/recipe/59318/amish-...\nAmish and Mennonite\n['1 large head cabbage, cored and finely shred...\n\n\n1\nPennsylvania Dutch Pickled Beets and Eggs\nhttps://www.allrecipes.com/recipe/13743/pennsy...\nAmish and Mennonite\n['8 large eggs', '2 (15 ounce) cans whole pick...\n\n\n2\nAmish Macaroni Salad\nhttps://www.allrecipes.com/recipe/74915/amish-...\nAmish and Mennonite\n['2 cups uncooked elbow macaroni', '3 large ha...\n\n\n3\nAmish Friendship Bread Starter\nhttps://www.allrecipes.com/recipe/7063/amish-f...\nAmish and Mennonite\n['1 (.25 ounce) package active dry yeast', '¼ ...\n\n\n4\nMy Amish Friend's Caramel Corn\nhttps://www.allrecipes.com/recipe/74950/my-ami...\nAmish and Mennonite\n['7 quarts plain popped popcorn', '2 cups dry ...\n\n\n...\n...\n...\n...\n...\n\n\n2316\nVietnamese Grilled Pork Skewers\nhttps://www.allrecipes.com/recipe/261122/vietn...\nVietnamese\n['1 pound pork belly, cubed', '1 fresh red ch...\n\n\n2317\nGoi Ga (Vietnamese Chicken and Cabbage Salad)\nhttps://www.allrecipes.com/recipe/271155/goi-g...\nVietnamese\n['4 skinless cooked chicken breasts, shredded...\n\n\n2318\nVietnamese Fresh Spring Rolls\nhttps://www.allrecipes.com/recipe/24239/vietna...\nVietnamese\n['2 ounces rice vermicelli', '8 rice wrappers...\n\n\n2319\nPho (Vietnamese Noodle Soup)\nhttps://www.allrecipes.com/recipe/228443/authe...\nVietnamese\n['4 pounds beef soup bones (shank and knee)', ...\n\n\n2320\nVietnamese Grilled Lemongrass Chicken\nhttps://www.allrecipes.com/recipe/241607/vietn...\nVietnamese\n['2 tablespoons canola oil', '2 tablespoons fi...\n\n\n\n\n2321 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nUnfortunately the ingrediants lists are not organized in a good way, so we need to isolate key ingrediants that are common in many recipes. For the machine learning model we are trying to concentrate on produce, so this list will contain more produce than anything else. Additionally, to simplify the machine learning model we will not be including the measurements of the ingrediants because it is a lot harder to train a model to also recognize how much of an ingredient is in an image.\nFirst, we need to remove any punctuation from the string and then seacrch for the key words. To do all of this we will be using the functions remove_punctuation(text) and find_key_ingredients(df, key_ingredients). find_key_ingredients compares the each item from the key_ingredients list to the string to find if each word is in the string. For some of the ingredients there can be errors, such as ‘corn starch’ gets recognized as only ‘corn,’ so some if statements are implemented to try and prevent this mistake.\n\n\ndef remove_punctuation(text):\n  '''\n  - Removes the punctuation from the string of recipe ingredients\n  - Input: string of ingredients\n  - Output: the cleaned string\n  '''\n  translator = str.maketrans('', '', string.punctuation)\n  return text.translate(translator)\n\ndef find_key_ingredients(df, key_ingredients):\n  '''\n  - Compares the lists of ingredients and creates a new column in the dataframe\n    with a list of key ingredients for each recipe\n  - Input: dataframe of recipes, list of key ingredients\n  - Output: dataframe with new column of key ingredients\n  '''\n\n  # Function to check if any key ingredient is found in a list of ingredients\n  def find_word_in_string(text):\n    '''\n    - Finds key ingredients in ingredient string\n    - Input: text from ingredient column of df\n    - Output: List of key ingredients found in ingredient column\n    '''\n    R = []\n    for i in key_ingredients:\n      if i in text:\n        # a lot of recipes use corn starch and corn flour\n        if i == 'corn' and ('corn starch' in text or 'corn flour' in text):\n          break;\n        R.append(i)\n        # Sometimes only need one egg\n        if i == 'eggs' and 'egg' in text and 'eggs' not in R:\n          R.append(i)\n    return R\n\n  # Remove punctuation\n  df['Ingredients'] = df['Ingredients'].apply(remove_punctuation)\n\n  # Find key ingredients in each row\n  df['key_ingredients'] = df['Ingredients'].apply(find_word_in_string)\n  return df\n\n\n\nAfter running these function on the recipe dataframe we get a new dataframe with a column for key ingredients:\n\ncleaned_df\n\n\n  \n    \n\n\n\n\n\n\nName\nURL\nCuisine\nIngredients\nkey_ingredients\n\n\n\n\n0\nBest Vinegar Coleslaw\nhttps://www.allrecipes.com/recipe/59318/amish-...\nAmish and Mennonite\n1 large head cabbage cored and finely shredded...\n['cabbage', 'celery', 'onion', 'sugar']\n\n\n1\nPennsylvania Dutch Pickled Beets and Eggs\nhttps://www.allrecipes.com/recipe/13743/pennsy...\nAmish and Mennonite\n8 large eggs 2 15 ounce cans whole pickled bee...\n['onion', 'beet', 'eggs', 'sugar']\n\n\n2\nAmish Macaroni Salad\nhttps://www.allrecipes.com/recipe/74915/amish-...\nAmish and Mennonite\n2 cups uncooked elbow macaroni 3 large hardcoo...\n['celery', 'onion', 'bell pepper', 'eggs', 'su...\n\n\n3\nAmish Friendship Bread Starter\nhttps://www.allrecipes.com/recipe/7063/amish-f...\nAmish and Mennonite\n1 25 ounce package active dry yeast ¼ cup warm...\n['milk', 'flour', 'sugar']\n\n\n4\nMy Amish Friend's Caramel Corn\nhttps://www.allrecipes.com/recipe/74950/my-ami...\nAmish and Mennonite\n7 quarts plain popped popcorn 2 cups dry roast...\n['corn', 'sugar']\n\n\n...\n...\n...\n...\n...\n...\n\n\n2316\nVietnamese Grilled Pork Skewers\nhttps://www.allrecipes.com/recipe/261122/vietn...\nVietnamese\n1 pound pork belly cubed 1 fresh red chile pe...\n['garlic', 'pork', 'sugar']\n\n\n2317\nGoi Ga (Vietnamese Chicken and Cabbage Salad)\nhttps://www.allrecipes.com/recipe/271155/goi-g...\nVietnamese\n4 skinless cooked chicken breasts shredded 1 ...\n['cabbage', 'cilantro', 'onion', 'carrot', 'ch...\n\n\n2318\nVietnamese Fresh Spring Rolls\nhttps://www.allrecipes.com/recipe/24239/vietna...\nVietnamese\n2 ounces rice vermicelli 8 rice wrappers 85 i...\n['cilantro', 'lettuce', 'rice']\n\n\n2319\nPho (Vietnamese Noodle Soup)\nhttps://www.allrecipes.com/recipe/228443/authe...\nVietnamese\n4 pounds beef soup bones shank and knee 1 medi...\n['cilantro', 'garlic', 'onion', 'rice', 'beef']\n\n\n2320\nVietnamese Grilled Lemongrass Chicken\nhttps://www.allrecipes.com/recipe/241607/vietn...\nVietnamese\n2 tablespoons canola oil 2 tablespoons finely ...\n['garlic', 'chicken', 'sugar']\n\n\n\n\n2321 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThis is our final dataset that we will be using to compare with the results of the model and to recommend recipes. The functions used in this section can be found in recipe_manip.py."
  },
  {
    "objectID": "posts/final_project/index.html#ii.-images-for-ingredients",
    "href": "posts/final_project/index.html#ii.-images-for-ingredients",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "II. Images for Ingredients",
    "text": "II. Images for Ingredients\nThe web scraping model I created to gather images of ingredients from Google Images uses both BeautifulSoup and Selenium, which are used for parsing HTML and automating web interactions. By using Selenium, and web driver like geckodriver for FireFox of chromedriver for Google Chrome, the script is able to load pages by scrolling to the end, ensuring that all images are loaded before the HTML is parsed by BeautifulSoup. Additionally, Selenium allows you to interact with elements on the page, which I used to clicking on each thumbnail to access the URLs of the corresponding images.\nThe main function initializes a web driver object and prompts the user for inputs including a list of keywords, the desired number of images to download for each keyword, and the name of the file to save the downloaded images. It also creates a new file if the specified one does not already exist in the directory. Subsequently, for each keyword provided by the user, the script generates a Google Images URL and calls the scrape_images function to extract the URLs of the images before proceeding to download them using the download_image function.\nThe scrape_images function is responsible for extracting image URLs from the provided Google Images URL. It utilizes Selenium to scroll through the webpage, ensuring all images are loaded, and then parses the HTML using BeautifulSoup. Thumbnails of images are identified and clicked to access the full-sized image URLs. These URLs are collected and returned as a set.\nThe download_image function handles the downloading and saving of images. It makes a request to the provided image URL and retrieves the image content. The content is then processed, ensuring it is in a usable format. The image is then saved as a JPEG file in the specified download path with the provided file name. This ensures that the images are successfully downloaded and stored in files designated to the ingredient classes for classification with the model later on.\nTo be able to run the code in the terminal or in a notebook, a web driver exicutable file must be stored within the same directory as the py file. This is also where the ingredient files will be downloaded.\n\n\ndef main():\n    wd = webdriver.Chrome()\n    data = input('Enter your search keyword: ')\n    data_list = data.split(\", \")\n    \n    num_images = int(input('Enter the number of images you want: '))\n    file_name = input('Enter file location: ')\n    \n    if not os.path.exists(file_name):\n        os.mkdir(file_name)\n    \n    count = 0\n    for i in data_list:\n        search_url = Google_Image + 'q=' + i #'q=' because its a query\n        print(search_url)\n        urls = scrape_images(search_url, wd, 1, num_images)\n        print(f\"Found {len(urls)}\")\n    \n        for j, url in enumerate(urls):\n            download_image(file_name + \"/\", url, str(count) + \".jpg\")\n            count += 1\n        \n        print(\"Success!\")\n            \n    print(\"Complete!\")\n    wd.quit()\n\ndef scrape_images(initial_url, wd, delay, max_images):\n    url = initial_url\n    wd.get(url)\n    \n    # scroll height of webpage\n    last_height = wd.execute_script(\"return document.body.scrollHeight\") \n    while True:\n        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    # The driver scrolls down the webpage here.\n        time.sleep(8)\n        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n        if new_height == last_height:   # Breaks here if the driver reached the bottom of the webpage (when it cannot scroll down anymore).\n            break\n        last_height = new_height        # If the driver failed to scroll to the bottom of the webpage, the current scroll height is recorded. Following, it is compared to the scroll height in the next iteration of the loop to decide if the driver reached the bottom of the webpage or not.\n    wd.execute_script(\"window.scrollTo(0, 0);\")\n    \n    page_html = wd.page_source  # The HTML of the webpage is obtained by the driver.\n    pageSoup = bs4.BeautifulSoup(page_html, 'html.parser')  # Using Beautiful Soup to parse the HTML of the webpage.\n    thumbnails = wd.find_elements(By.CLASS_NAME, \"Q4LuWd\")  # This class name is obtained from the thumbnail of each image in Google Image Search.\n    time.sleep(3)\n    \n    len_thumbnails = len(thumbnails)    # The number of images found is recorded and printed.\n    print(\"Found %s image candidates\"%(len_thumbnails))\n    \n    image_urls = set()\n        \n    for img in thumbnails[len(image_urls): max_images]: # Loops through the images of the webpage to obtain and store the number of image URLs requested.\n        try: \n            img.click()\n            time.sleep(0.5)\n\n        except:\n            continue\n\n        images = wd.find_elements(By.CLASS_NAME, \"iPVvYb\") # This class name is obtained from the actual image and not its thumbnail in Google Image Search.\n        for image in images:\n            if image.get_attribute('src') in image_urls: # Prevents an image URL from being stored if it is already there.\n                max_images += 1 # Accounts for a duplicate image URL by ensuring that the function still returns the number of image URLs requested.\n                break\n\n            if image.get_attribute('src') and 'http' in image.get_attribute('src'): # Checks if an image URL is usable.\n                    image_urls.add(image.get_attribute('src')) # Stores an image URL if it is usable.\n                    #print(f\"Found {len(image_urls)}\")\n        \n    return image_urls # Returns all of the usable image URLs.\n\ndef download_image(download_path, url, file_name):\n\n    try:\n        image_content = requests.get(url).content\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to download image from {url} - {e}\")\n        return  # Return without attempting to process the image further\n\n    try:\n        image_file = io.BytesIO(image_content)\n        image = Image.open(image_file)\n    except (IOError, OSError) as e:\n        print(f\"Failed to open image file - {e}\")\n        return  # Return without attempting to process the image further\n\n    # Convert image to RGB mode\n    image = image.convert('RGB')\n\n    file_path = os.path.join(download_path, file_name)\n    try:\n        with open(file_path, \"wb\") as f:\n            image.save(f, \"JPEG\")\n        print(f\"Downloaded: {file_name}\")\n    except Exception as e:\n        print(f\"Failed to save image - {e}\")\n\n\n\nUnfortunately, despite Selenium allowing us to scroll to the bottom of the page, the Google Images page only loads around 400 images for a given search word. Therefore, to get a decent amount of images for training we need to input multiple key words for the same ingredient.\nFor example, for the ingredient onion, there are many different types of onions so we can make a list of these and input them when prompted: yellow onion, white onion, sweet onion, red onion, shallots, onion raw. Additionally, for certain produce or ingredients often when you look them up they come up cooked, to prevent this with many of the broader ingredients you should add ‘raw’ to the key frase.\n\n\n\n\n\n\n\n\n\nAlternatively, name is defined in the py file so you can run this in the terminal by calling “python image_scraper.py”\nBelow is what shows up during each step of the web scraping of the image URLs: 1. Scrolling to the end of the page and parsing HTML:\n\n\n\n\n\n\n\n\n\n\nClicking on each thumbnail and parsing HTML for image URLs:\n\n\n\n\n\n\n\n\n\n\nIn total, we were able to web scrape around 1000 images for 20 ingredients: Tomatoes, Potatoes, Zucchini, Broccoli, Atrichoke, Beans, Onion, Asparagus, Green Onion, Cabbage, Lentils, Garlic, Bell Peppers, Chili Peppers, Corn, Carrots, Mushrooms, Cherries, Apples, Rice.\nWe were not able to scrape more than this due to a few different issues. In order to fully load each page we had to allow time for it to pause between clicking and scrolling, so it took a long time to download a good amount of images for each ingredient. Additionally, the webscraper would often stop scraping randomly, most likely due to connection issues with the web driver and sometimes it would find zero items on the page."
  },
  {
    "objectID": "posts/final_project/index.html#i.-implementation-of-the-model",
    "href": "posts/final_project/index.html#i.-implementation-of-the-model",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "I. Implementation of the Model",
    "text": "I. Implementation of the Model\nThe machine learning aspect of our project entailed the implementation of a CNN in pytorch. After researching many different models, and weighing the pros and cons of each architecture, we settled on using a Resnet to carry out image classification. While we had the option to utilize a pretrained model taken from the torchvision database, we felt that it would be better to implement and train our own rendition of the network. This allowed for minute adjustments to layer functions, such as changing the filter size in a convolution, or removing negligible operations to counteract overfitting. The code itself is quite complex. Let’s walk through the actual Resnet class:\nclass ResNet(nn.Module):\n    def __init__(self, block, num_block, num_classes=100,num_channel=3):\n        \"\"\"Initializes the ResNet model.\n\n        Args:\n            block: Basic block or bottleneck block.\n            num_block (list): List containing the number of blocks for each layer.\n            num_classes (int): Number of output classes.\n            num_channel (int): Number of input channels, default is 3.\n        \"\"\"\n        super().__init__()\n        self.in_channels = DIM\n        self.conv1 = nn.Sequential(\n          nn.Conv2d(num_channel, DIM, kernel_size=3, padding=1, bias=False),\n          nn.BatchNorm2d(DIM),\n          nn.ReLU(inplace=True))\n        self.conv2_x = self._make_layer(block, DIM, num_block[0], 1)\n        self.conv3_x = self._make_layer(block, DIM*2, num_block[1], 2)\n        self.conv4_x = self._make_layer(block, DIM*4, num_block[2], 2)\n        self.conv5_x = self._make_layer(block, DIM*8, num_block[3], 2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(DIM*8 * block.expansion, num_classes)\n        self.sigmoid = nn.Sigmoid()\nThis is just a snippet of the code that goes into creating a Resnet. The model starts out with a simple 2D convolution followed by a batch normalization and ReLU activation. Here, num_channels refers to the number of color channels in the input image (three for all our data) and DIM the number of output channels after the convolution is processed (DIM is variable, but we find a value of 64 works best). Then, we proceed to the meat of the architecture, utilizing a complex function called _make_layer() (definition not pictured) that is used to create a model “layer” consisting of a variable number of convolutions, batch normalizations, max pools, and activations, all depending on the type and number of “blocks” specified. Currently, we don’t need to worry about the explicit definition of a block – it’s quite involved, so it suffices to understand that they’re used as the framework of a layer in the model. After this constructor, model operations are put into motion using a feed-forward function:\ndef forward(self, x):\n  \"\"\"Forward pass of the ResNet model.\n\n    Args:\n      x (torch.Tensor): Input tensor.\n\n    Returns:\n      torch.Tensor: Output tensor.\n  \"\"\"\n  output = self.conv1(x)\n  output = self.conv2_x(output)\n  output = self.conv3_x(output)\n  output = self.conv4_x(output)\n  output = self.conv5_x(output)\n  output = self.avg_pool(output)\n  output = output.view(output.size(0), -1)\n  output = self.fc(output)\n\n  return self.sigmoid(output)\nData is fed through each layer (as outlined above) and its class probability distribution is returned using the sigmoid activation function. We use a Resnet18:\ndef resnet18(num_classes,num_channel):\n  \"\"\" returns a ResNet 18 object\n  \"\"\"\n  return ResNet(BasicBlock, [2, 2, 2, 2],num_classes,num_channel)\nas our main model, but by adjusting the block type and number, we can also possibly return a Resnet34, or Resnet50, all the way up to a Resnet152 (here, the number associated with a Resnet can be thought of as the number of layers in the model). This concludes the construction of our model – now we’re ready to create our dataset!"
  },
  {
    "objectID": "posts/final_project/index.html#ii-dataset-creation",
    "href": "posts/final_project/index.html#ii-dataset-creation",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "II: Dataset Creation",
    "text": "II: Dataset Creation\nIn order to train, we must first create a dataset from which our model can learn from. Depending on how you store your training images, you can write a funciton to retrieve data in the form of numpy arrays. Our data was stored in a directory containing subdirectories of a given class to partition the set of different tyes of ingredients. To create numpy arrays of the images and their corresponding labels, we utilized the following:\ndef load_images_and_labels(parent_folder, target_size=(150,150)):\n  \"\"\"Returns image data in form of a numpy array\n    \n  args:\n    parent_folder: folder containing classes of images to move through sequentially\n    target_size: size of resulting images in image array\n  \"\"\"\n  images = []\n  labels = []\n  class_mapping = {\"beans\" : 0, \"bell_pepper\" : 1, \"potato\" : 2, \"tomato\" : 3}\n\n  for class_folder in os.listdir(parent_folder):\n    class_path = os.path.join(parent_folder, class_folder)\n    if os.path.isdir(class_path) and class_folder in class_mapping:\n      label = class_mapping[class_folder]\n      for image_file in os.listdir(class_path):\n        image_path = os.path.join(class_path, image_file)\n        try:\n          image = PIL.Image.open(image_path)\n          if image is not None:\n            if image.mode != 'RGB':\n              image = image.convert('RGB')\n            image = image.resize(target_size)\n            images.append(image)\n            labels.append(label)\n            except Exception as e:\n              print(f'Error reading image {image_path}: {e}')\n\n  return np.array(images), np.array(labels)\nThis function iterates through each file and adds the images to python using PIL, then appends each image to a list called images, as well as its label (corresponding to the stated mapping) to a list called labels. At the end, a numpy array version of both these lists is returned. Using these, we can then create a pytorch dataset: ```python class Ingredients(Dataset): “““Class creating pytorch dataset from images and their labels.\nArgs:\n    images (list): List of images.\n    labels (list): List of corresponding labels.\n    transform (callable, optional): Augmentation of the images.\n\nAttributes:\n    images (list): List of images.\n    labels (list): List of corresponding labels.\n    transform (callable, optional): Augmentation of the images.\n\nMethods:\n    __len__(self): Returns the number of samples in the dataset.\n    __getitem__(self, idx): Retrieves the item at the given index.\n\n\"\"\"\ndef __init__(self, images, labels, transform=None):\n    \"\"\"Initializes the Ingredients dataset.\n\n    Args:\n        images (list): List of images.\n        labels (list): List of corresponding labels.\n        transform (callable, optional): Augmentation of the images.\n    \"\"\"\n    self.images = images\n    self.labels = labels\n    self.transform = transform\n\ndef __len__(self):\n    \"\"\"Returns the number of samples in the dataset.\n\n    Returns:\n        int: Number of samples in the dataset.\n    \"\"\"\n    return len(self.images)\n\ndef __getitem__(self, idx):\n    \"\"\"Retrieves the item at the given index.\n\n    Args:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the image and its corresponding label.\n    \"\"\"\n    image = self.images[idx]\n    image = PIL.Image.fromarray(image)\n    label = self.labels[idx]\n    if self.transform:\n        image = self.transform(image)\n    return image, label\nEvery pytorch dataset is a class inheriting from the dataset class (which you'll have to import through ```torchvision```) and needs to define the following three functions: a constructor (```__init__```), ```__len__()```, and ```__getitem__()```. The constructor records the data, ```__len__``` returns the length of the dataset, and ```__getitem__``` returns an image-label combo at an index of the set. Once we've created our dataset, there are multiple steps we would like to take to process our data. For one, we'd like to change the pixel values of our images to live in the range [-1,1] from the standard [0,255], done through the following:\n```python\ndef preprocess(images_arr):\n  \"\"\"Preprocess an array of images.\n\n    Args:\n        images_arr (numpy.ndarray): Array of images to be preprocessed.\n\n    Returns:\n        numpy.ndarray: Preprocessed array of images, pixel values between -1 and 1.\n  \"\"\"\n  images_arr = images_arr.astype(np.float32)\n  images_arr = images_arr/255.0\n  images_arr = (images_arr * 2.0) - 1.0\n\n  return images_arr\nIn this function, we convert each image to a 32-bit float and then dilate the channels to fit within the desired interval. Additionally, we have written a function to find the mean and standard deviation across each channel:\ndef normalization_vals(images_arr):\n  \"\"\"Calculate mean and standard deviation values for normalizing an array of images.\n\n    Args:\n        images_arr (numpy.ndarray): Array of images for which to calculate normalization values.\n\n    Returns:\n        tuple: A tuple containing the mean and standard deviation values for each channel.\n  \"\"\"\n  means = np.mean(images_arr, axis=(0,1,2))\n  stds = np.std(images_arr, axis=(0,1,2))\n  means = tuple(means)\n  stds = tuple(stds)\n  return means, stds\nThese values will be used to normalize the pixel values of our training set. With all this, we are now ready to begin training."
  },
  {
    "objectID": "posts/final_project/index.html#iii.-training-the-model",
    "href": "posts/final_project/index.html#iii.-training-the-model",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "III. Training the Model",
    "text": "III. Training the Model\nThis part is relatively simple. The whole process is contained in one file, main.py, which imports useful functions as defined in other files. Here is main:\nimport torch\nimport torch.optim as optim\n# from torchinfo import summary\n\nfrom logger import Logger\nfrom train import run_epoch\nfrom resnet import resnet14, resnet18, resnet34, resnet50\nfrom data_loader import get_data\nfrom test import run_test\n\ntorch.manual_seed(0)\n\nEPOCHS = 200\nEARLY_STOP = 5\nLR = 0.1\nMOMENTUM = 0.9\nWEIGHT_DECAY = 0.0005 # 0.0005 test\nVAL_SPLIT = 0.1\n\nmodel_save_path = f'./model_logs/Ingredients1'\n\nlog = Logger()\nlog.set_logger(f'{model_save_path}.log')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_loader, val_loader, test_loader, num_classes, num_channels = get_data(VAL_SPLIT)\nmodel = resnet18(num_classes, num_channels)\noptimizer = optim.SGD(model.parameters(), LR, MOMENTUM, weight_decay=WEIGHT_DECAY)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\nrun_epoch(model, train_loader, val_loader, optimizer, scheduler, EPOCHS, EARLY_STOP, f'{model_save_path}.pth', device, log)\naccuracy = run_test(model, test_loader, device, f\"{model_save_path}.pth\")\nlog.logger.info(\"Accuracy: {:.10f}\".format(accuracy))\nOf note are get_data(), run_epoch(), and run_test(). The former is defined in another file, data_loader.py, and is as follows:\ndef get_data(val_split=0.5):\n    \"\"\"Prepares DataLoader instances for training, validation, and testing splits of an image dataset.\n    \n    Parameters:\n    - val_split (float, optional): Proportion of the training set to use for validation. Default is 0.5.\n\n    Returns:\n    - Tuple[DataLoader, DataLoader, DataLoader, int, int]: A tuple containing DataLoader instances for the\n      training, validation, and test datasets, the number of unique labels in the dataset, and the number 3,\n      whose specific meaning may depend on context (e.g., number of color channels).\n    \"\"\"\n    parent_folder = \"/content/drive/MyDrive/proj_files/ingredients/ingredients\"\n    images, labels = load_images_and_labels(parent_folder)\n    preprocess(images)\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(32),\n        transforms.ToTensor(),\n        transforms.Normalize((0.38046584, 0.10854615, -0.13485776), (0.5249659, 0.59474176, 0.6634378))\n    ])\n\n    dataset = Ingredients(images, labels, transform)\n        \n    batch_size = 64 # change to appropriate value for dataset in use\n\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    train_test_split = int(np.floor(0.8 * dataset_size))\n    train_val_split = int(np.floor(val_split * dataset_size))\n\n    np.random.shuffle(indices)\n\n    train_idx, test_idx = indices[train_test_split:], indices[:train_test_split]\n    val_idx = train_idx[:train_val_split]\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_size,\n                              sampler=SubsetRandomSampler(train_idx),\n                              num_workers=4)\n    \n    test_loader = DataLoader(dataset,\n                             batch_size=batch_size,\n                             sampler=SubsetRandomSampler(test_idx),\n                             num_workers=4)\n\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_size,\n                            sampler=SubsetRandomSampler(val_idx),\n                            num_workers=4)\n    \n    return train_loader, val_loader, test_loader, len(np.unique(labels)), 3\nIt begins by identifying the parent directory in which all our data is located (you will have to change this to depending on where your images are located). Then it utilizes the preprocessing functions we defined above. Next, we define a dataset based on the preprocessed image data, and split this dataset into a training set, validation set, and test set. We then create an iterator of sorts called a DataLoader that allows us to effectively load data in batches to train the model. We create three separate data loaders, one for our training set, one for our valiadation set, and one for our test set. These, in addition to the number of classes and color channels in the data, are returned and further used in our main function. Thus, the data pipeline portion of the process has been completed. Now let’s define the actual training algorithm. The function run_epoch() is as follows:\ndef run_epoch(model:torch.nn.Module, train_loader:torch.utils.data.DataLoader, val_loader:torch.utils.data.DataLoader, optimizer, scheduler, epochs:int, early_stop:int, model_save_path:str, device:torch.device, log, loading=False):\n    \"\"\"Run training and validation for the given number of epochs.\n\n    Args:\n        model (torch.nn.Module): The model.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        optimizer: The optimizer to update the model's parameters.\n        scheduler: Learning rate scheduler.\n        epochs (int): Number of epochs for training.\n        early_stop (int): Number of epochs to wait before early stopping if validation loss doesn't improve.\n        model_save_path (str): Path to save the best model.\n        device (torch.device): Device to be used for training (cuda or cpu).\n        log: Logger object for logging.\n        loading (bool): Flag indicating whether to load a pre-trained model. Default is False.\n\n    Returns:\n        None\n    \"\"\"\n    train_losses = []\n    val_losses = []\n    if loading==True:\n        model.load_state_dict(torch.load(model_save_path))\n        log.logger.info(\"-------------Model Loaded------------\")\n        \n    best_loss=0\n    #curr_early_stop = early_stop\n    model.to(device)\n    for epoch in range(epochs):\n        train_loss = train(model, train_loader, optimizer, device)\n        val_loss = val(model, val_loader, device)\n        scheduler.step()\n       \n        log.logger.info((f\"Epoch: {epoch+1} - loss: {train_loss:.10f} - test_loss: {val_loss:.10f}\"))\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if epoch == 0:\n            best_loss=val_loss\n        if val_loss&lt;=best_loss:\n            torch.save(model.state_dict(), model_save_path)    \n            best_loss=val_loss\n            log.logger.info(\"-------- Save Best Model! --------\")\nThis function utilizes both our training and validation data loaders. We defined functions for training and validation (train and val, respectively) that implement the classic backpropogation process. Additionally, we include a learning rate scheduler to help boost training results. Logger is used to create files that streamline the access and view of our results. Using validation loss as our metric, we save every model that is an improvement over the current best model. If needed, one could easily implement an early stopping system (in fact, our own early stop code is commented out of the original file on github), but based off our research, which involved looking into the processes outlined in the original Resnet research paper, it seemed best to evaluate the model of a full period of 200 epochs. Lastly, we look at the run_test() function, which is again, relatively simple:\ndef run_test(model, test_loader, device, model_save_path):\n    \"\"\"Run testing on the test data using the trained model.\n\n    Args:\n        model (torch.nn.Module): The trained model.\n        test_loader (torch.utils.data.DataLoader): DataLoader containing the test data.\n        device (torch.device): The device to be used for testing (cuda or cpu).\n        model_save_path (str): Path to the saved model.\n\n    Returns:\n        float: Accuracy achieved by the model on the test data.\n    \"\"\"\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n    model.eval()\n\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predictions = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predictions.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_predictions)\n\n    return accuracy\nHere, we use the test_loader to run through the dataset one more time, evaluating it using our final, fully trained model and calculating accuracy using scikit-learn’s accuray algorithm. We trained our model on a small subset of ingredients, which resulted in an accuracy score of around 70%. This score should improve immensley when a large number and variety of images is used as training data, unfortunately, we were unable to achieve this premier model due to a lack of resources. Our dataset creation function is quite itensive. Thus, when trying to load our set, both google colab and jupyter lab would run out of GPU RAM. Given the proper resources, it would be more than possible to produce a fully trained model on an expansive dataset. Still, we produced a more than adequate network using only a small subset. It’s training results are visualized below:\n\n\n\nimage.png\n\n\nAs we proceed through the cycle of epochs, the loss continues to decrease. There is not much overfitting present, thought the values start to stagnate near the end of the training process. Here, we achieved a 68% accuracy, but tuning hyperparameters further should increase this. Additionally, as mentioned before, a larger, more diverse dataset would do wonders for performance. Unfortunately, that’s just not possible with our currently available resources. Now with this, the training process is done. We proceed to put everything together in the form of a webapp made in Flask."
  },
  {
    "objectID": "posts/final_project/index.html#i.-home-page",
    "href": "posts/final_project/index.html#i.-home-page",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "I. Home Page",
    "text": "I. Home Page\nWhile creating the home page itself simple involved design we also first had to establish the HTML code to set up the Web APP. To start off we created a a base.html and a main.html. Main pourpose of the base.html is to provides a structured layout with a navigation bar and placeholders for dynamic content. The navigation bar ks able to naviage between the three pages”:\n\nHome page: Links to the main page of the application. The url_for(‘main’) function dynamically generates the URL to the view function named main.\nGenerate a Recipe: Links to the page where users can generate recipes. The url_for(‘generate’) generates the URL to the generate view function.\nRecipe List: Links to a page displaying a list of recipes. The url_for(‘receipe_page’) (note the typo in ‘recipe’) generates the URL to the receipe_page view function. !\n\n\n&lt;!doctype html&gt;\n\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} - Recipe Generator&lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Recipe Generator&lt;/h1&gt;\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('main') }}\"&gt;Home page&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('generate')}}\"&gt;Generate a Recipe&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\"{{ url_for('receipe_page')}}\"&gt;Recipe List&lt;/a&gt;&lt;/li&gt;\n    &lt;!-- &lt;li&gt;&lt;a href=\"{{ url_for('recipe_results')}}\"&gt;Recipes&lt;/a&gt;&lt;/li&gt; --&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nBelow the main.html sets up the structure of the mian page. we used img src to present three images and imporve the appreace of our model but most importantly we added a button called “Generate a Receipe”. The style of the button is added at style.css. The picture below depics how our home page looks like. As you can see you can navigate to the generate a recipe page both through the button in the middle or through the navigation bar. Below the image we will dive into how we formulated the generate function.\n\n\n {% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %} Eat Your Veggies{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n\n&lt;div class=\"centered-container\"&gt;\n  &lt;h2&gt;Get a List of Receipes Tailored For You and Your Ingredients&lt;/h2&gt;\n\n  &lt;p&gt; Welcome to our recipe generator! Have leftover vegetables? Not sure what to do with them? Do not fret we got you covered! It's simple: upload photos of your available vegetables and other ingredients, select your preferred cuisine type, and voilà! We'll provide you with a curated list of recipes tailored just for you. Let's make cooking at home easy, fun, and absolutely delightful. Start your culinary adventure with us today!&lt;/p&gt;\n\n  &lt;p&gt; Simply click on the generate button below or explore our recipes in the tab above&lt;/p&gt;\n&lt;a href=\"/generate\" class=\"myButton\"&gt;Generate a Recipe&lt;/a&gt;\n&lt;p&gt;The best seasoning is hunger!&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"image-container\"&gt;\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ-IZ1xJCPpfvhKZeLbKinyBX6Zvilg_RULXA&usqp=CAU.jpg\" alt=\"Image1\"&gt;\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqE81NwU51B4OED72TNNtVQHk1QJN2GZNZSw&usqp=CAU.jpg\" alt=\"Image 2\"&gt;\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSq_n1J37kfrfyPyt7lLnjGB_ZaAFVa95Om_A&usqp=CAU.jpg\" alt=\"Image 3\"&gt;\n&lt;/div&gt;\n\n{% endblock %}\n\n\n\n\nrecipe-homepage.png\n\n\nIn order to be able to ensure that the website is functioning and is interactive with users, we had to implement the generate function. When navigating to the generate page users and asked to choose their preferred cuisine and upload pictures of their products. To be able to process the data the user inputs and to be able to provide them with the appropriate recipes, we created the gen_recipe() function. Below is the code for when a user goes to the home page and when a user goes to the generate a recipe page. But more importantly, it also includes our recipe function which can intake the data process it and redirect users to the page with the recipe list.\nThe function has both a GET and a POST request lets quickly explore when occure under each:\n\nIf the request method is GET (e.g., when a user navigates to /generate/ directly), the server responds by rendering and returning the generate2.html template. This page ( presented below the function code) allows users to input data (like selecting a cuisine or uploading images) needed to generate a recipe.\nIf the method is POST (e.g., when a user submits a form on generate2.html), the server tries to process the submitted data, to do so it needs to retrieves the selected cusine from the datafarme.\n\nOnce the data is retived it intializes our pre-trained deep learning model (resnet18) configured for a 4-class classification problem (the classes being different ingredients). The function then loads the model weights from a specified file and prepares a series of image transformations (e.g., flipping, cropping, normalization) to apply to the uploaded images. For each uploaded image, it applies the transformations which can be seen when the in the function at the transform= transforms.Compose section.\nThe code converts the image to a tensor, and then uses the model to predict the ingredient class.Each predicted ingredient is then added to a list. After processing all images, it redirects the user to the recipe_results route, passing along the selected cuisine and the list of predicted ingredients. In case there is an error occuring during the processing of the POST request the error handling is included to ensure it catches the exception and re-renders the generate2.html template with an error indicator.\nThe end of the functions redirects the user to a new page (recipe_results) where recipes based on the identified ingredients and selected cuisine are shown. This is done by passing the cuisine type and the list of identified ingredients as arguments to the url_for function, which generates the appropriate URL for the redirect.\n\n\n@app.route('/')\ndef main():\n    return render_template('main_better.html')\n\n@app.route('/generate/', methods=['POST', 'GET'])\ndef generate():\n    \"\"\"Handle the generation of ingredient predictions based on uploaded images.\n\n    If the request method is GET, renders the 'generate2.html' template.\n    If the request method is POST, processes the uploaded images, predicts ingredients,\n    and redirects to the 'recipe_results' function.\n\n    Returns:\n        Response: Rendered template or redirection to 'recipe_results' function.\n    \"\"\"\n    if request.method == 'GET':\n        return render_template('generate2.html')\n    else:\n        try:\n            cuisine = request.form.get('cuisine')\n            classes = ['beans', 'bell_pepper', 'potato', 'tomato']\n            model = resnet18(4, 3) # model trained on small dataset\n            model.load_state_dict(torch.load('model/model_logs/Ingredients8.pth', map_location='cpu'))\n\n            #set to eval for testing\n            model.eval()\n            files = request.files.getlist('images[]')\n            transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomResizedCrop(32),\n                transforms.ToTensor(),\n                transforms.Normalize((0.38046584, 0.10854615, -0.13485776), (0.5249659, 0.59474176, 0.6634378))\n            ])\n            ingredients = []\n            images = []\n            for file in files:\n                try:\n                    image = Image.open(file)\n                    if image is not None:\n                        if image.mode != 'RGB':\n                            image = image.convert('RGB')\n                        # image = image.resize((150,150))\n                        image = np.array(image)\n                        images.append(image)\n                except Exception as e:\n                    print(f'Error reading file {file}: {e}')\n\n            # allow for the transform to work on pil_image\n            pil_images = [Image.fromarray(image) for image in images]\n            transformed_images = [transform(image) for image in pil_images]\n            images = torch.stack(transformed_images)\n\n            # classify each image\n            for image in images:\n                # don't track gradient for efficient testing\n                with torch.no_grad():\n                    outputs = model(image.unsqueeze(0))\n                    _, pred_class = torch.max(outputs, 1)\n                    ingredient = classes[pred_class.item()]\n                    ingredients.append(ingredient)\n\n            # send list of ingredients to recipe_results function\n            return redirect(url_for('recipe_results', cuisine=cuisine, ingredients=ingredients))\n\n        except:\n            return render_template('generate2.html', error=True)\n\n@app.route('/generate/&lt;name&gt;')\ndef generate_name(name):\n    return render_template('generate2.html', name=name)\n\n\nThe generate page where user upload their images and chose their prefred cuisine!\n\n\n\ngenerate.png\n\n\nOnce the user upload the images they submit their request and are immidetaly redirected to the following page. Below the name of each receipe, we also included a short list of all the key ingredients that are need to make it easier for users to select the receipe they want.\nThis is when the gen_recipes function is used to produce a list of recipes with the most matched ingredients.\nThe gen_recipe function generates recipe recommendations based on a specified cuisine and the list of ingredients returned by the image classification model. It first reads a recipe dataset, filtering it by cuisine if provided. When a list of ingredients is provided, it further filters the dataset based on the presence of matching ingredients. The function applies different matching thresholds depending on the length of the ingredient list. Recipes with a higher number of matching ingredients are given priority. Finally, the function returns a sorted dataframe of recommended recipes tailored to the user’s cuisine and available ingredients.\n\n\ndef gen_recipe(cuisine, list_of_ingredients=None):\n  '''\n  - Generates recipe recommendations from the recipe dataset based off of the\n    detected ingredients from user input images and prefered cuisine.\n  - Input: cuisine, list of ingredients\n  - Output: dataframe of recomended recipes\n  '''\n  link = 'https://raw.githubusercontent.com/Nkannan12/Project-16B-GNN/main/Recipes_cleaned.csv'\n  df = pd.read_csv(link)\n\n  if list_of_ingredients == None:\n    cuisine_df = df[df[\"Cuisine\"] == cuisine].copy()\n    return cuisine_df\n  else:\n    cuisine_df = df[df[\"Cuisine\"] == cuisine].copy()\n    # Create a new column to count matching ingredients\n    cuisine_df['Matched Ingredients'] = cuisine_df['key_ingredients'].apply(lambda x: sum(1 for ingredient in list_of_ingredients if ingredient in x))\n    if len(list_of_ingredients) &gt; 2:\n      cuisine_df = cuisine_df[cuisine_df['Matched Ingredients'] &gt;= 2]\n    else:\n      cuisine_df = cuisine_df[cuisine_df['Matched Ingredients'] &gt;= 1]\n    # Sort dataframe based on the number of matched ingredients in descending order\n    cuisine_df = cuisine_df.sort_values(by='Matched Ingredients', ascending=False)\n\n    # Reset index\n    cuisine_df.reset_index(drop=True, inplace=True)\n\n    return cuisine_df\n\n\n\nThe results of the gen_recipe function are then output on the webapp as displayed:\n\n\n\nfilter2.png\n\n\nOnce selecting a recipe the user clicks on the name of the recipe and immediately gets redirected to the recipe page where they can get the cooking instructions and list of the rest of the needed ingredients\n\n\n\nScreenshot 2024-03-20 071406.png"
  },
  {
    "objectID": "posts/final_project/index.html#ii.-list-of-recipes",
    "href": "posts/final_project/index.html#ii.-list-of-recipes",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "II. List of recipes",
    "text": "II. List of recipes\nAnother component of the website is the recipe list page. On this page, participants are simply able to explore the different cuisine and the recipes within them regardless of the ingredients they have.\nThe code from the app.py file below will present how the receipe list page work and how we navigate to the correct url of the recipe\nThe def filter_ recipe takes in the user selected cuisine and filter through the data farme to find the matching receipes. Lets break down the decorator and the function.\nfirst the @app.route(‘/filter-recipes’, methods=[‘POST’]) decorator tells Flask to use the filter_recipes function as the handler for requests to the URL /filter-recipes when the HTTP method is POST.\nWhen the create the function which runs when the POST request is made for filter-recipes. The “cuisine = request.form.get(‘cuisine’)” code extracts the value of cuisine from the form data submitted with the POST request. If cuisine is not present in the form data, None is returned instead. The next line “ingredients = None: Initializes ingredients with None” is added so when we are generating receipes we can also incldue ingredients howevere in this case users only select a cusine.\nThe function then calls the function gen_recipe with cuisine and ingredients as arguments. This function isreturns a collection of recommended recipes based on the provided cuisine (and potentially ingredients, if implemented).\nWe then convert the recommended_recipes into a list of dictionaries. Each dictionary represents a recipe, and the orient=‘records’ parameter means each row in the DataFrame is converted to a dictionary, with column names as keys.\nLastly we render the receipe result page.\n\n@app.route('/filter-recipes', methods=['POST'])\ndef filter_recipes():\n    \"\"\"Filter recipes based on selected cuisine and ingredients.\n\n    Retrieves the selected cuisine and ingredients from the request form, then generates\n    recommended recipes using the 'gen_recipe' function. Converts the recommended recipes\n    into a dictionary format and renders the 'recipe_results.html' template, passing the\n    recipes data.\n\n    Returns:\n        Response: Rendered template 'recipe_results.html' with recipes data.\n    \"\"\"\n    cuisine = request.form.get('cuisine')\n    ingredients = None\n\n    recommended_recipes = gen_recipe(cuisine, ingredients)\n    recipes_data = recommended_recipes.to_dict(orient='records')\n\n    # Render the recipe_results.html template, passing the recipes data\n    return render_template('recipe_results.html', recipes=recipes_data)\n\n\nNow that we are at the recipe results page lets examine the recipe_resulst function:\nWhen a user accesses the /recipe_results/ URL, the application can respond to both GET and POST requests. For GET requests, it’s expected that the user has passed information through the URL’s query parameters, specifying both a type of cuisine and ingredients they’re interested in. The application checks if both pieces of information are provided; if not, it displays a message asking for both the cuisine and ingredients. If the information is present, it uses a function gen_recipe from earlier to fetch recommended recipes matching the specified criteria, converts this data into a format that can be easily used in the HTML template, and then displays it to the user on the recipe_results.html page. For POST requests, which are not explicitly handled with form data in this code, it simply displays a message encouraging the user to select a cuisine to view recipes, suggesting that the main interaction mode expected is through GET requests\n\n@app.route('/recipe_results/', methods=['GET', 'POST'])\ndef recipe_results():\n    \"\"\"Display recipe results based on selected cuisine and ingredients.\n\n    If the request method is GET, retrieves the selected cuisine and ingredients from the request arguments,\n    generates recommended recipes using the 'gen_recipe' function, converts the recommended recipes into a\n    dictionary format, and renders the 'recipe_results.html' template, passing the recipes data. If either\n    cuisine or ingredients is not provided, displays an error message.\n\n    If the request method is not GET, displays a message prompting the user to select a cuisine to view recipes.\n\n    Returns:\n        Response: Rendered template 'recipe_results.html' with recipes data or an error message.\n    \"\"\"\n    if request.method == 'GET':\n        # fetch cuisine and ingredients passed form generate\n        cuisine = request.args.get('cuisine')\n        ingredients = request.args.get('ingredients')\n        if cuisine is None or ingredients is None:\n            message = \"Provide both a cuisine and ingredients.\"\n            return render_template('recipe_results.html', message=message)\n        # print(cuisine + ' ' + ingredients)\n        recommended_recipes = gen_recipe(cuisine, ingredients)\n        # print(recommended_recipes)\n        recipes_data = recommended_recipes.to_dict(orient='records')\n        print(recipes_data)  # Debugging line to check data\n        return render_template('recipe_results.html', recipes=recipes_data)\n    else:\n        message = \"Please select a cuisine to view recipes.\"\n        return render_template('recipe_results.html', message=message)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\nThe code below is from the receipe_results.html, it dynamically display a list of recipes on a webpage. If recipes are available, each one is presented with its name (as a clickable link to the recipe’s page) and its key ingredients. If no recipes are available, a predefined message is displayed instead.\n\n\nLets break down how it works and what each part does:\nThe h1 simply displays the header of the page.\nThe div id=“recipes”&gt;…&lt;/div” encloses the entire section of the code that deals with displaying the recipes or a message in case there are no recipes to display. It is important to note that the id=“recipes” attribute is used to uniquely identify this division in the document, which can be useful for styling or scripting purposes.\nNext up % if recipes % is a Jinja2 conditional statement that checks if there is a recipes variable available and it has a truthy value (e.g., a non-empty list). If recipes is truthy, the block of code inside this condition is executed.\nIf the recipes variable exists and contains items than this lines % for recipe in recipes % , loop iterates over each item in recipes, note each item in the loop is referred to as recipe.\nInside the loop, a div class=“recipe”&gt;…"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "Final Project: Recipe Generator Web App - Eat Your Veggies!\n\n\n\n\n\n\nfinal project\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nGianna Pedroza, Neomi Goodman, Narayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing: How Text Classification can Identify Fake News\n\n\n\n\n\n\nweek 10\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification using Tensorflow and Keras: Cats vs. Dogs\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax\n\n\n\n\n\n\nweek 7\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Web Scraping Movie Recomendations using Scrapy\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: Data Wrangling and Visualization using NOAA Climate Data\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial: Constructing a Data Visualization of the Palmer Penguins Dataset\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hw0/index.html",
    "href": "posts/hw0/index.html",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "",
    "text": "Data visualization is an important skill for understanding and drawing insight from datasets. The Palmer Penguins dataset contains data on 344 penguins of three different species within the Palmer Archipelago in Antarctica. In this tutorial, we will explore how to create an interesting data visualization pf the Palmer Penguins dataset using Python."
  },
  {
    "objectID": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "href": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 1: Import Libraries and Load the Dataset",
    "text": "Step 1: Import Libraries and Load the Dataset\nWe want to import the pandas library in order to create a Dataframe class object containing all of the information from the palmer_penguins.csv file that we are downloading directly from the given url. Then, we need matplotlib in order to create our visualization. Other libraries can be used for visualizing the data besides matplotlib, but this is the most well known.\n\nimport pandas as pd # Contains the class DataFrame\nimport matplotlib.pyplot as plt # For creating simple graphs\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nA csv file is a file formated to contain data that is seperated by commas for the different collumns. We can easily create a pandas DataFrame object by reading in a csv file of the Palmer Penguins dataset from the url given above.\nAdditionally we want to add names for the columns:\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]"
  },
  {
    "objectID": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "href": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 2: Explore and Clean the Data",
    "text": "Step 2: Explore and Clean the Data\nBefore creating a visualization, it’s essential to understand the structure and contents of the dataset. First, we want to simply take a look at the database we have created in order to determine if any rows or columns need to be cleaned.\n\npenguins.head() #Shows the first five rows of the dataframe\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n3\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n\n\n\n\n\nThen, we need to clean out any empty datapoints using the dropna method, and we want to make the dataframe easier to read by simplifying the species name.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\nAdditionally, often times data sets contain columns that are not relevant to our study or visualization so they can be deleted from the dataframe for simplicity. In this case, we will keep all of the columns for the different examples given below."
  },
  {
    "objectID": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "href": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 3: Choose an Appropriate Method of Visualization",
    "text": "Step 3: Choose an Appropriate Method of Visualization\nFor the visualization, there are many different options. Your visualization should be based off of what you are trying to analyze in the data. For example, let’s say that I want to compare the size of the culmen of the three different species. For this, I will be using a scatter plot to visualize the clustering of the three groups.\nFirst, I will create a dictionary of the species and colors they should be associated with. Then, I will create a scatter plot for each species. Also, I want to add some additional features to the scatter plot, axis labels, and a title. This makes the graph easier to read and visually appealing.\n\n# library with associated species and colors\ncolors = {'Adelie': 'blue', 'Chinstrap': 'orange', 'Gentoo': 'green'}\n\n# Create and individual scatter plot for each species\nfor species, col in colors.items():\n    species_data = penguins[penguins['Species'] == species]\n    plt.scatter(species_data['Culmen Length (mm)'], species_data['Culmen Depth (mm)'], label=species, color=col)\n\n# Add aditional features to the plot\nplt.legend(title='Species')\nplt.title('Culmen Size of Penguin Species')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, I could also choose to compare the different body masses of each species using a box plot. First, I will need to import a library that could help me do this, like plotly. Then I can use the box method to create a box plot with the Species as the x-axis and the body mass as the y-axis.\n\n# import the relevant library\nimport plotly.express as px\n\n# Create the box plot and add a title\nbox = px.box(penguins, x = 'Species', y = 'Body Mass (g)', color = 'Species')\nbox.update_layout(title_text=\"Body Mass Distribution of Penguin Species\")\n\n# Show the plot\nbox.show()"
  },
  {
    "objectID": "posts/hw0/index.html#conclusion",
    "href": "posts/hw0/index.html#conclusion",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, you can make many different types of interesting visualization from this dataset that can help you draw different insight. Choosing the right visualization depends on what you’re trying to find, and there are many different libraries and graphs that can help you with that. Once your visualization is created, interpret the results and analyze any patterns or trends observed from the visualization. Feel free to experiment with other types of visualizations and variables from the dataset to create a comprehensive understanding of the data. Remember to share your findings and insights in a clear and concise manner by making your visualizations easy to read and visibly satisfying. Data visualization is not only about creating beautiful plots but also about effectively communicating information."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "Homework 2: Web Scraping Movie Recomendations using Scrapy",
    "section": "",
    "text": "Introduction:\nIn this blog post, I will walk you through the process of creating an web scraper designed to explore movie recomendations based on your favorite movie. This python web scraper collects movie recomendations by extracting information about movie casts from TMDB. The goal is to extract data on actors, movies, and their relationships to build a simple movie recommendation system based on shared actors.\nFirst we must set up the project files in the terminal using:\n    conda activate PIC16B-24W\n    scrapy startproject TMDB_scraper\n    cd TMDB_scraper\nNow we must create a tmdb_spider.py file in the spiders file for our scraper class.\n\n\nUnderstanding the Scraper\nThe web scraper is designed to collect data from The Movie Database (TMDb). It is designed as a class called TmdbSpider in the tmbd_spider.py file containing four methods: init, parse, parse_full_credits, parse_actor_page. To scrape the necessary information from each page we have to use css commands to extract information from the html of the given url.\nparse() This method initiates the scraping process by navigating to the Full Cast & Crew page of a movie. It then calls the parse_full_credits() method to extract actor information.\n\n\n    def parse(self, response):\n        \"\"\"\n        Navigate to the Full Cast & Crew page and call parse_full_credits.\n\n        Args:\n            response (scrapy.http.Response): The response object for the movie page.\n\n        Yields:\n            scrapy.Request: Request to the Full Cast & Crew page.\n        \"\"\"\n        cast_url = f\"{response.url}/cast\"\n        yield scrapy.Request(cast_url, callback=self.parse_full_credits)\n\n\n\nparse_full_credits() This method parses the Full Credits page for the given movie, extracting actor names and their profile page URLs. It yields requests to scrape each actor’s page using the parse_actor_page() method while passing along the actor name. This method scrapes information specifically from the left panel of the page because we want to only include actors and exclude crew, producers, etc.\n\n\n    def parse_full_credits(self, response):\n        \n        \"\"\"\n        Parse the Full Credits page for a movie, extract actor names and their respective profile page URLs,\n        and yield requests to scrape each actor's page.\n\n        Args:\n            response (scrapy.http.Response): The response object for the Full Credits page.\n\n        Yields:\n            scrapy.Request: Request to the actor's page.\n        \"\"\"\n        # selects the lines containing the actor url's and their names\n        actor_selectors = response.css('div.content_wrapper.false section.panel.pad:first-child div.info a[href^=\"/person/\"]')\n        \n        # Loops through the list of actor information\n        for actor_selector in actor_selectors:\n            \n            # Extracts actor names and actor url\n            actor_name = actor_selector.css('::text').extract_first()\n            actor_page_url = urljoin(response.url, actor_selector.css('::attr(href)').extract_first())\n\n            # Debug information\n            #self.logger.info(f'Actor Name: {actor_name}, Actor Page URL Relative: {actor_page_url_relative}')\n\n            # Debug information\n            #self.logger.info(f'Actor Page URL: {actor_page_url}')\n\n            yield scrapy.Request(actor_page_url, callback=self.parse_actor_page, meta={'actor_name': actor_name})\n\n\n\nparse_actor_page() This method parses an actor’s profile page, extracting movie titles and their URLs. It yields information about the actor and the associated movie or TV show. This method scrapes information specifically from the Actor table of the page because we want to only include acting roles and exclude any other roles.\n\n\n    def parse_actor_page(self, response):\n        \n        \"\"\"\n        Parse an actor's profile page, extract movie titles and their respective URLs,\n        and yield information about the actor and the associated movie.\n\n        Args:\n            response (scrapy.http.Response): The response object for the actor's profile page.\n\n        Yields:\n            dict: Information about the actor and the associated movie.\n        \"\"\"\n        \n        actor_name = response.meta.get('actor_name')\n\n        # Extracting movies from lines like '&lt;a class=\"tooltip\" href=\"/movie/1236045\"&gt;\n        #                                      &lt;bdi&gt;Animals&lt;/bdi&gt;'\n        #                                    &lt;/a&gt;\n        h3_cat = response.css('div.credits_list')[0].css('h3::text').getall()\n        num = h3_cat.index(\"Acting\")\n        movie_selectors = response.css('div.credits_list')[0].css('table.card.credits')[num].css('a.tooltip')\n\n        for movie_selector in movie_selectors:\n            # Extract movie title\n            movie_title = movie_selector.css('bdi::text').extract_first().strip()\n             # Extracting movie URL\n            movie_url_relative = movie_selector.css('::attr(href)').extract_first()\n\n            # Join the relative URL with the base URL of the current page\n            movie_url = response.urljoin(movie_url_relative)\n            \n            '''\n            if movie_url + '-' + convert_to_slug(movie_title) == self.start_urls:\n                self.logger.info(f'Skipping movie: {movie_title} (same as original)')\n                continue\n            '''\n\n            # Debug information\n            #self.logger.info(f'Movie Title: {movie_title}')\n            #self.logger.info(f'Movie URL: {movie_url}-{convert_to_slug(movie_title)}')\n            \n            if movie_title and movie_url:\n                yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_title}\n            else:\n                # If movie_title or movie_url is not present, log a warning\n                self.logger.warning('Movie title or URL not found in the selector')\n\n\n\nTo run the scraper, use the following command in the terminal:  scrapy crawl tmdb_spider -o results.csv -a subdir=489-good-will-hunting  Where we replace the subdir parameter with the movie’s subdir you want to start the scraping process from. You can find the subdir by checking the url of the TMBD page for the movie you want recomendations from.\nNow we are returned a results.csv file containing the recomendations gathered from the parsing of the page.\n\n\nResults and Analysis\nFrom the results.csv we want to extract the recomended movies. Logically, the best recomendations will be those with the most number of shared actors from the original movie. We can see below that the csv has two columns, actor, and movie_or_TV_name, so we want to reorganize the dataframe so that it combines repetitions of the same movie and counts the number of shared actors.\n\n# import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv(\"goodwillhunting_recs.csv\")\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPatrick O'Donnell\n2 By 4\n\n\n1\nPatrick O'Donnell\nGood Will Hunting\n\n\n2\nScott William Winters\nA Little Dream\n\n\n3\nScott William Winters\nNarcos: Mexico\n\n\n4\nScott William Winters\nBeautifully Broken\n\n\n...\n...\n...\n\n\n1500\nRobin Williams\nThe Tonight Show Starring Johnny Carson\n\n\n1501\nRobin Williams\nThe Grammy Awards\n\n\n1502\nRobin Williams\nTony Awards\n\n\n1503\nRobin Williams\nThe Oscars\n\n\n1504\nRobin Williams\nToday\n\n\n\n\n1505 rows × 2 columns\n\n\n\n\nprint(df.shape)\n\n(1505, 2)\n\n\nIn order to reorganize the dataframe I have created a function called create_shared_actors_df. It takes the DataFrame obtained from the scraper as input and generates a new DataFrame with information about shared actors for each movie. The resulting DataFrame is sorted in descending order based on the number of shared actors.\n\ndef create_shared_actors_df(input_df):\n    \"\"\"\n    Create a DataFrame with information about shared actors for each movie.\n\n    Args:\n        input_df (pd.DataFrame): The input DataFrame containing actor and movie information.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with columns for movie, num_shared_actors, and shared_actors,\n                      sorted in descending order based on the number of shared actors.\n    \"\"\"\n    # Create an empty list to store dictionaries\n    result_data = []\n\n    # Iterate over unique movies in the input DataFrame\n    unique_movies = input_df['movie_or_TV_name'].unique()\n    for movie in unique_movies:\n        # Filter rows for the current movie\n        movie_rows = input_df[input_df['movie_or_TV_name'] == movie]\n\n        # Get unique actors for the current movie\n        unique_actors = movie_rows['actor'].unique()\n\n        # Find the number of shared actors\n        num_shared_actors = len(unique_actors)\n\n        # Append a dictionary to the result list\n        result_data.append({\n            'Movie/TV Show': movie,\n            'Number of Shared Actors': num_shared_actors,\n            'Shared Actors': unique_actors.tolist()\n        })\n\n    # Create a new DataFrame from the list of dictionaries\n    result_df = pd.DataFrame(result_data)\n    # Exclude the original movie since it will automatically have all of the actors as shared actors\n    result_df = result_df[result_df['Movie/TV Show'] != \"Good Will Hunting\"]\n    \n    # Sort the DataFrame based on the number of shared actors in descending order\n    result_df = result_df.sort_values(by='Number of Shared Actors', ascending=False)\n    result_df = result_df.reset_index(drop=True)\n\n    return result_df\n\n\n# Example usage with the provided DataFrame df\nresult_df = create_shared_actors_df(df)\nresult_df.head()\n\n\n\n\n\n\n\n\nMovie/TV Show\nNumber of Shared Actors\nShared Actors\n\n\n\n\n0\nDue South\n6\n[Philip Williams, James Allodi, David Eisner, ...\n\n\n1\nSaturday Night Live\n5\n[Matt Damon, George Plimpton, Casey Affleck, B...\n\n\n2\nMayday\n5\n[Frank Nakashima, Bruce Hunter, Barna Moricz, ...\n\n\n3\nThe Graham Norton Show\n4\n[Matt Damon, Minnie Driver, Ben Affleck, Robin...\n\n\n4\nThe View\n4\n[Matt Damon, Minnie Driver, Ben Affleck, Robin...\n\n\n\n\n\n\n\n\nVisualization:\nTo visualize the number of shared actors between movies, you can use tools like Matplotlib, plotly, or seaborn in Python. The visualize_shared_actors function creates a visual representation of the number of shared actors in the recomended movies and TV shows using a bar chart from Matplotlib. The bar chart generated highlights movies or TV shows where multiple actors have collaborated, indicating potential relationships or thematic similarities between those productions and the original.\nAlso, the function includes a filter (df[‘Number of Shared Actors’] &gt;= 3) to focus on movies or TV shows with a higher number of shared actors because there are over a thousand recomendations gathered by the scraper, many of which have no shared actors.\n\nimport matplotlib.pyplot as plt\n\ndef visualize_shared_actors(df, overlap):\n    \"\"\"\n    Visualize the number of shared actors in movies/TV shows using a bar chart.\n\n    Arguments:\n        df (pd.DataFrame): DataFrame containing columns 'Movie/TV Show' and 'Number of Shared Actors'.\n\n    Displays a bar chart.\n    \"\"\"\n    \n    shared_df = df[df['Number of Shared Actors'] &gt;= overlap]\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(x = shared_df['Movie/TV Show'], height = shared_df['Number of Shared Actors'], color='skyblue')\n    plt.title('Number of Shared Actors in Movies/TV Shows')\n    plt.xlabel('Movie/TV Show')\n    plt.ylabel('Number of Shared Actors')\n    plt.xticks(rotation=45, ha='right')\n\n    # Display the table\n    plt.show()\n\nHere is a visualization of the shared actors recomendation data using the visualize_shared_actors function:\n\n# Visualize shared actors for movies with two or more shared actors\nvisualize_shared_actors(result_df, 3)\n\n\n\n\n\n\n\n\n\n# Visualize shared actors for movies with two or more shared actors\nvisualize_shared_actors(result_df, 4)\n\n\n\n\n\n\n\n\nThe top movie/tv show recomendations for Good Will Hunting are Due South, Mayday, and War of the Worlds. As we can see one issue that could possibly fixed in the future is exclusing talk shows.\n\n\n\nConclusion\nIn this tutorial, we’ve created a web scraper using Scrapy to extract movie information from TMDb in order to create a recomendation system. By understanding the html structure of the website and using Scrapy, we can gather valuable data for building a movie recommendation system. The visualization at the end allows us to analyze and identify actors with significant contributions to multiple movies for the best recomendations. To further enhance the recomendation system, you could limit the number of actors scraped to only the main cast of the show."
  },
  {
    "objectID": "posts/hw5/index.html",
    "href": "posts/hw5/index.html",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "",
    "text": "Image classification is a foundational skill in machine learning, and with the dynamic duo of Keras and TensorFlow, the process becomes relatively easy. In this tutorial, we will focus on training a machine learning algorithms to differentiate between cats and dogs from a dataset provided by tensorflow datasets which contains image of cats and dogs.\nWe will use techniques like data augmentation to enhance the dataset by introducing variations and distortions, helping models learn more robust patterns. Also, we will see how pre-trained models can help us train our own.\nDownload the necessary libraries:\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nfrom keras import utils, datasets, layers, models\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nFirst, we need to split the dataset into a training, validation, and test set. The training set provides the data on which the model learns patterns and relationships. It typically constitutes the majority of the dataset. Then, the model’s performance is fine-tuned using the validation set, which is smaller but can gauge how well the model generalizes to unseen data. This is important to find issues like overfitting and adjusting parameters. Finally, the test set is the final evaluation, assessing the model’s real-world performance on data it has never encountered during training or validation. The size of the test set is similar to the validation set.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, we need to make sure that all of the images are the same shape and size so we resize all of them to the dimensions (150, 150).\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nNext we split the data to be in batches for faster processing time. We also have to be careful not to run this block more than once or else it will mess up the shape of the dataset. I ran it twice and it made the shape (64, 64, 150, 150, 3) for the images and (64, 64) for the labels, which is not what we want.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/hw5/index.html#part-1-introduction",
    "href": "posts/hw5/index.html#part-1-introduction",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "",
    "text": "Image classification is a foundational skill in machine learning, and with the dynamic duo of Keras and TensorFlow, the process becomes relatively easy. In this tutorial, we will focus on training a machine learning algorithms to differentiate between cats and dogs from a dataset provided by tensorflow datasets which contains image of cats and dogs.\nWe will use techniques like data augmentation to enhance the dataset by introducing variations and distortions, helping models learn more robust patterns. Also, we will see how pre-trained models can help us train our own.\nDownload the necessary libraries:\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nfrom keras import utils, datasets, layers, models\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nFirst, we need to split the dataset into a training, validation, and test set. The training set provides the data on which the model learns patterns and relationships. It typically constitutes the majority of the dataset. Then, the model’s performance is fine-tuned using the validation set, which is smaller but can gauge how well the model generalizes to unseen data. This is important to find issues like overfitting and adjusting parameters. Finally, the test set is the final evaluation, assessing the model’s real-world performance on data it has never encountered during training or validation. The size of the test set is similar to the validation set.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, we need to make sure that all of the images are the same shape and size so we resize all of them to the dimensions (150, 150).\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nNext we split the data to be in batches for faster processing time. We also have to be careful not to run this block more than once or else it will mess up the shape of the dataset. I ran it twice and it made the shape (64, 64, 150, 150, 3) for the images and (64, 64) for the labels, which is not what we want.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/hw5/index.html#part-2-visualize-the-dataset",
    "href": "posts/hw5/index.html#part-2-visualize-the-dataset",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 2: Visualize the Dataset",
    "text": "Part 2: Visualize the Dataset\nTo get a better idea of what the images look like we want to create a grid of the images using the function bellow with their associated labels (0 for cat, 1 for dog).\n\nclass_names = { 0: 'Cat', 1: 'Dog'}\n\nTo get an idea of how to visualize a single image we have to call a single image using [ ] brackets as shown bellow. A single batch has a shape (64, 64, 150, 150, 3) with 64 images and 64 labels where each image is of dimension 150 x 150.\n\n# Assuming `dataset` is your TensorFlow dataset with shape (64, 150, 150, 3)\nfor batch in train_ds.take(1):\n    # Select the first image from the first batch\n    single_image = batch[0].numpy().astype(\"uint8\")\n\n# Remove the batch dimension\nsingle_image = single_image[0]\n\n# Display the image\nplt.imshow(single_image)\nplt.show()\n\n\n\n\n\n\n\n\nNext, the visualize_dataset function is designed to provide a visual representation of images from the given dataset. This function is particularly useful when working with image classification datasets, where each image is associated with a specific label as we described above.The function iterates through the first subset of the dataset (assumed to be a batch). It extracts images and labels from the batch, converting images to NumPy arrays and ensuring they are of type “uint8” (unsigned 8-bit integers). The function then loops through the labels to separate images into two categories cats and dogs. Then it collects three images of cats and dogs for the 2x3 visualization. Then, the function iterates through the two categories and creates a plot using matplotlib.\n\ndef visualize_dataset(dataset, class_names):\n  plt.figure(figsize=(15, 8))\n  for subset in dataset.take(1):\n    # Select the first image from the first batch\n    images = batch[0].numpy().astype(\"uint8\")\n    labels = batch[1].numpy()\n\n    i = 0\n    d_count = 0\n    c_count = 0\n    cats = []\n    dogs = []\n    while c_count &lt; 3:\n      if labels [i] == 0:\n        cats.append(images[i])\n        c_count += 1\n      i += 1\n\n    i = 0\n    while d_count &lt; 3:\n      if labels [i] == 1:\n        dogs.append(images[i])\n        d_count += 1\n      i += 1\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, i + 1)\n      plt.imshow(cats[i])\n      plt.title(class_names[0])\n      plt.axis(\"off\")\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, 3 + i + 1)\n      plt.imshow(dogs[i])\n      plt.title(class_names[1])\n      plt.axis(\"off\")\n\n  plt.show()\n\n\nvisualize_dataset(train_ds, class_names)"
  },
  {
    "objectID": "posts/hw5/index.html#part-3-lable-frequencies",
    "href": "posts/hw5/index.html#part-3-lable-frequencies",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 3: Lable Frequencies",
    "text": "Part 3: Lable Frequencies\nNow, we want to check the frequency of dogs and cats in the training dataset in case there are a greater amount of cats than dog or vice versa. If this is the case, then we can compensate for the imbalance to better train the model.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\n# Initialize counters\ncat_count = 0\ndog_count = 0\n# Iterate through each array in the iterator\nfor label_value in labels_iterator:\n    if label_value == 0:\n        cat_count += 1\n    elif label_value == 1:\n        dog_count += 1\n\n# Display results\nprint(\"Number of cat images:\", cat_count)\nprint(\"Number of dog images:\", dog_count)\n\nNumber of cat images: 4637\nNumber of dog images: 4668\n\n\nAs we can see, there are around the same amount of cat and dog images in the training set. Since the number of cat images is higher than the number of dog images, the baseline model would always predict “cat” and achieve an accuracy equal to the proportion of cat images in the dataset. In this specific example, the baseline accuracy would be:\n\\[ \\text{Baseline Accuracy} = \\frac{\\text{Number of Cat Images}}{\\text{Total Number of Images}} = \\frac{4667}{4667 + 4637} = {0.5016...}\\]\nThis calculation would give you the baseline accuracy, and any machine learning model developed for this task should aim to surpass this baseline accuracy to be considered meaningful and effective."
  },
  {
    "objectID": "posts/hw5/index.html#part-4-first-model",
    "href": "posts/hw5/index.html#part-4-first-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 4: First Model",
    "text": "Part 4: First Model\nFirst we will create a simple sequential layer using some of the layers described below. A Sequential Model in Keras is a linear stack of layers that allows you to create models layer by layer in a step-by-step fashion.\n\nConv2D Layer: Convolutional layers perform the convolution operation, applying filters (also known as kernels) to input data to extract specific features.These layers capture local patterns and detect hierarchical features, allowing the model to learn representations from the input images.\nMaxPooling2D Layer: MaxPooling layers downsample the spatial dimensions of the input data by taking the maximum value in each region of the input covered by the pooling window. They reduces the computational complexity of the model while retaining essential features, helping prevent overfitting and improving the model’s robustness.\nFlatten Layer: The Flatten layer is used to flatten the input data, converting it from a multidimensional tensor to a one-dimensional vector.\nDense Layer: Dense layers connect every neuron in one layer to every neuron in the next layer. These layers enable the model to learn complex patterns by combining features learned by previous layers. The final dense layer produces the output for classification.\nDropout Layer: Dropout layers randomly set a fraction of input units to zero during training, helping prevent overfitting by introducing a form of regularization.\n\n\n# Define the model\nmodel1 = models.Sequential([\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer\n    layers.Flatten(),\n\n    # Dense layers\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n\n    # Output layer\n    layers.Dense(10, activation='softmax')\n])\n\nAfter defining the model we need to compile it and the fit it to the training data.\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 15s 55ms/step - loss: 13.6908 - accuracy: 0.5256 - val_loss: 0.6984 - val_accuracy: 0.5830\nEpoch 2/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.7555 - accuracy: 0.5904 - val_loss: 0.7573 - val_accuracy: 0.6058\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.6809 - accuracy: 0.6366 - val_loss: 0.6557 - val_accuracy: 0.6221\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6158 - accuracy: 0.6793 - val_loss: 0.6656 - val_accuracy: 0.6152\nEpoch 5/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5601 - accuracy: 0.7203 - val_loss: 0.6888 - val_accuracy: 0.6234\nEpoch 6/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5018 - accuracy: 0.7606 - val_loss: 0.7171 - val_accuracy: 0.6152\nEpoch 7/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.4618 - accuracy: 0.7795 - val_loss: 0.7934 - val_accuracy: 0.6204\nEpoch 8/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.4158 - accuracy: 0.8161 - val_loss: 0.8343 - val_accuracy: 0.6187\nEpoch 9/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.4115 - accuracy: 0.8125 - val_loss: 0.8546 - val_accuracy: 0.5985\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3635 - accuracy: 0.8436 - val_loss: 0.9108 - val_accuracy: 0.6062\nEpoch 11/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.3111 - accuracy: 0.8706 - val_loss: 0.9991 - val_accuracy: 0.6169\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2890 - accuracy: 0.8750 - val_loss: 1.1080 - val_accuracy: 0.5851\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.2342 - accuracy: 0.9079 - val_loss: 1.1989 - val_accuracy: 0.5967\nEpoch 14/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2323 - accuracy: 0.9132 - val_loss: 1.1892 - val_accuracy: 0.5989\nEpoch 15/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2011 - accuracy: 0.9220 - val_loss: 1.3048 - val_accuracy: 0.5985\nEpoch 16/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2209 - accuracy: 0.9180 - val_loss: 1.4371 - val_accuracy: 0.5778\nEpoch 17/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1949 - accuracy: 0.9263 - val_loss: 1.5856 - val_accuracy: 0.5881\nEpoch 18/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.1635 - accuracy: 0.9418 - val_loss: 1.7272 - val_accuracy: 0.5993\nEpoch 19/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1665 - accuracy: 0.9387 - val_loss: 1.6080 - val_accuracy: 0.5899\nEpoch 20/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1677 - accuracy: 0.9397 - val_loss: 1.5196 - val_accuracy: 0.5997\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n\n\nBy plotting the training and validation accuracy, we can better visualize the performance of the model and possible overfitting issues.\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy Observation: The accuracy of my model fluctuated during training, reaching around 62% at its peak.\nComparison to Baseline: I achieved a validation accuracy slightly better than the baseline of 50%. While there is an improvement, further optimization can be explored.\nOverfitting Observation: There are signs of overfitting as the training accuracy (around 92.5%) is substantially higher than the validation accuracy (around 59.7%). To mitigate overfitting, additional techniques such as increasing dropout rates, reducing model complexity, or incorporating regularization methods could be explored. Regularization methods like L2 regularization or data augmentation might be useful in this scenario."
  },
  {
    "objectID": "posts/hw5/index.html#part-4-second-model",
    "href": "posts/hw5/index.html#part-4-second-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 4: Second Model",
    "text": "Part 4: Second Model\nAs we saw in part 4, a sequential model as defined was not much more accurate than the baseline. In order to improve the accuracy we can include some data augmentation layers. Data augmentation is when we include modified copies of the same image in the training set; for example, the image rotated or mirrored.\n\ndef augmented(train_ds, data_augmentation):\n  # Display original and augmented images\n  plt.figure(figsize=(10, 10))\n  for image, _ in train_ds.take(1):\n      original_image = image[0]\n      ax = plt.subplot(2, 2, 1)\n      plt.imshow(original_image / 255)\n      plt.axis('off')\n      ax.set_title('Original Image')\n\n  # Display augmented images\n  for i in range(2, 5):\n      ax = plt.subplot(2, 2, i)\n      augmented_image = data_augmentation(tf.expand_dims(original_image, 0))\n      plt.imshow(augmented_image[0] / 255)\n      plt.axis('off')\n      ax.set_title(f'Augmented Image {i - 1}')\n\n  plt.show()\n\nFirst let’s take a look at the images produced by Random Flip:\n\ndata_augmentation1 = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n])\naugmented(train_ds, data_augmentation1)\n\n\n\n\n\n\n\n\nThen at the images produced by RandomRotation:\n\ndata_augmentation2 = tf.keras.Sequential([\n  tf.keras.layers.RandomRotation(0.2),\n])\naugmented(train_ds, data_augmentation2)\n\n\n\n\n\n\n\n\nAs we can see these layers in combination will add some diversity to the dataset, so let’s create new model similar to our first that include these layers and see if it performs better.\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    # Other layers (similar to model1)\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow compile and fit the model to the training data:\n\n# Compile the model\nmodel2.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Assuming you have train_ds and validation datasets\nhistory_model2 = model2.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 37ms/step - loss: 30.6067 - accuracy: 0.4996 - val_loss: 0.8345 - val_accuracy: 0.5482\nEpoch 2/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.8585 - accuracy: 0.5192 - val_loss: 0.8028 - val_accuracy: 0.5623\nEpoch 3/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7927 - accuracy: 0.5352 - val_loss: 0.7837 - val_accuracy: 0.5374\nEpoch 4/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.7638 - accuracy: 0.5507 - val_loss: 0.7010 - val_accuracy: 0.5727\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7311 - accuracy: 0.5596 - val_loss: 2.0641 - val_accuracy: 0.5009\nEpoch 6/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.7474 - accuracy: 0.5458 - val_loss: 0.6861 - val_accuracy: 0.5718\nEpoch 7/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7102 - accuracy: 0.5581 - val_loss: 0.6791 - val_accuracy: 0.5770\nEpoch 8/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.7023 - accuracy: 0.5687 - val_loss: 0.6731 - val_accuracy: 0.5929\nEpoch 9/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6982 - accuracy: 0.5697 - val_loss: 0.6600 - val_accuracy: 0.6109\nEpoch 10/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6894 - accuracy: 0.5846 - val_loss: 0.6720 - val_accuracy: 0.5864\nEpoch 11/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6887 - accuracy: 0.5843 - val_loss: 0.6677 - val_accuracy: 0.5993\nEpoch 12/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6879 - accuracy: 0.5802 - val_loss: 0.6554 - val_accuracy: 0.6294\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6768 - accuracy: 0.6038 - val_loss: 0.6553 - val_accuracy: 0.6234\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6799 - accuracy: 0.6000 - val_loss: 0.6595 - val_accuracy: 0.6088\nEpoch 15/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.6719 - accuracy: 0.5997 - val_loss: 0.6608 - val_accuracy: 0.6156\nEpoch 16/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6723 - accuracy: 0.6020 - val_loss: 0.6576 - val_accuracy: 0.6204\nEpoch 17/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6669 - accuracy: 0.6161 - val_loss: 0.6610 - val_accuracy: 0.6195\nEpoch 18/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6622 - accuracy: 0.6226 - val_loss: 0.6590 - val_accuracy: 0.6113\nEpoch 19/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6629 - accuracy: 0.6169 - val_loss: 0.6552 - val_accuracy: 0.6161\nEpoch 20/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6681 - accuracy: 0.6119 - val_loss: 0.6489 - val_accuracy: 0.6320\n\n\n\n# Plot the accuracy history\nplt.plot(history_model2.history['accuracy'], label='Training Accuracy')\nplt.plot(history_model2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy Observation: The accuracy of model2 fluctuated during training, reaching around 67% at its peak.\nComparison to Baseline and Model1: The model2 achieved a validation accuracy of approximately 67%, which is an improvement compared to the baseline of 50%. Model2’s validation accuracy (67%) is also higher than that of Model1 (approximately 60%). This indicates that the inclusion of data augmentation layers, such as RandomFlip and RandomRotation, in Model2 has contributed to better generalization, resulting in improved performance on the validation set.\nUnderfitting Observation: The training accuracy is around 62%, and the validation accuracy is around 67%. This suggests that the model may not have fully learned the patterns in the training data, and further adjustments to the model complexity or training parameters may be considered to address underfitting."
  },
  {
    "objectID": "posts/hw5/index.html#part-5-third-model",
    "href": "posts/hw5/index.html#part-5-third-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 5: Third Model",
    "text": "Part 5: Third Model\nThe original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. If we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nNow, let’s create our third model including the preprocessor defined above and the augmentation layers from Model2:\n\nmodel3 = models.Sequential([\n    preprocessor,\n\n    # augmentation layers\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    layers.Conv2D(64, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(256, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(512, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='LeakyReLU'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Assuming you have train_ds and validation datasets\nhistory_model3 = model3.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 23s 126ms/step - loss: 0.7978 - accuracy: 0.5451 - val_loss: 0.6467 - val_accuracy: 0.6440\nEpoch 2/20\n146/146 [==============================] - 17s 117ms/step - loss: 0.6438 - accuracy: 0.6317 - val_loss: 0.5906 - val_accuracy: 0.7094\nEpoch 3/20\n146/146 [==============================] - 17s 117ms/step - loss: 0.6120 - accuracy: 0.6680 - val_loss: 0.6085 - val_accuracy: 0.6913\nEpoch 4/20\n146/146 [==============================] - 17s 119ms/step - loss: 0.5734 - accuracy: 0.7011 - val_loss: 0.5141 - val_accuracy: 0.7558\nEpoch 5/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.5674 - accuracy: 0.7088 - val_loss: 0.5014 - val_accuracy: 0.7640\nEpoch 6/20\n146/146 [==============================] - 18s 122ms/step - loss: 0.5259 - accuracy: 0.7366 - val_loss: 0.4718 - val_accuracy: 0.7777\nEpoch 7/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5026 - accuracy: 0.7507 - val_loss: 0.4399 - val_accuracy: 0.8018\nEpoch 8/20\n146/146 [==============================] - 19s 128ms/step - loss: 0.4816 - accuracy: 0.7726 - val_loss: 0.4284 - val_accuracy: 0.8113\nEpoch 9/20\n146/146 [==============================] - 17s 120ms/step - loss: 0.4671 - accuracy: 0.7818 - val_loss: 0.4052 - val_accuracy: 0.8216\nEpoch 10/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4399 - accuracy: 0.7942 - val_loss: 0.3989 - val_accuracy: 0.8272\nEpoch 11/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4157 - accuracy: 0.8096 - val_loss: 0.3489 - val_accuracy: 0.8439\nEpoch 12/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3912 - accuracy: 0.8249 - val_loss: 0.3832 - val_accuracy: 0.8396\nEpoch 13/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3876 - accuracy: 0.8265 - val_loss: 0.3481 - val_accuracy: 0.8487\nEpoch 14/20\n146/146 [==============================] - 17s 120ms/step - loss: 0.3707 - accuracy: 0.8314 - val_loss: 0.3360 - val_accuracy: 0.8543\nEpoch 15/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3362 - accuracy: 0.8538 - val_loss: 0.3201 - val_accuracy: 0.8629\nEpoch 16/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3464 - accuracy: 0.8493 - val_loss: 0.3033 - val_accuracy: 0.8775\nEpoch 17/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3349 - accuracy: 0.8557 - val_loss: 0.3154 - val_accuracy: 0.8672\nEpoch 18/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3177 - accuracy: 0.8585 - val_loss: 0.3163 - val_accuracy: 0.8667\nEpoch 19/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.2919 - accuracy: 0.8749 - val_loss: 0.3044 - val_accuracy: 0.8835\nEpoch 20/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3015 - accuracy: 0.8692 - val_loss: 0.2917 - val_accuracy: 0.8809\n\n\n\nplt.plot(history_model3.history['accuracy'], label='Training Accuracy')\nplt.plot(history_model3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 3')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy: The validation accuracy of model3 during training is 88.09%.\nComparison to Model1: Model3’s validation accuracy is significantly higher than the accuracy achieved with model1, which had a validation accuracy of 68.2%. This indicates a notable improvement in model performance.\nOverfitting Observation: While there is still a slight gap between training and validation accuracy, it is significantly reduced compared to model1. Model3 demonstrates improved generalization to unseen data, suggesting reduced overfitting."
  },
  {
    "objectID": "posts/hw5/index.html#part-6-fourth-model",
    "href": "posts/hw5/index.html#part-6-fourth-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 6: Fourth Model",
    "text": "Part 6: Fourth Model\nMany people have attempted to classify images in the past and have worked out models for similar situations, so we can also test these on our model. First, we need to access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model. The preexisting model we will use is MobileNetV3Large and configure it as a layer in our model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=(150,150,3),\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nNow, let’s define our fourth model with the augmentation layers from before but using the new base_model_layer and a few simple layers.\n\nmodel4 = models.Sequential([\n    # augmentation layers\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(2, activation='softmax'),  # outputs the final classification\n])\n\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\nAfter compiling the model we can view it’s details using the summary function.\n\nmodel4.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_5 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_5 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_2 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d (Glob  (None, 960)               0         \n alMaxPooling2D)                                                 \n                                                                 \n dropout_14 (Dropout)        (None, 960)               0         \n                                                                 \n dense_21 (Dense)            (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nhistory_model4 = model4.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 71ms/step - loss: 1.4050 - accuracy: 0.8559 - val_loss: 0.4345 - val_accuracy: 0.9493\nEpoch 2/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.7502 - accuracy: 0.9138 - val_loss: 0.2115 - val_accuracy: 0.9721\nEpoch 3/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.5701 - accuracy: 0.9249 - val_loss: 0.2074 - val_accuracy: 0.9690\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5094 - accuracy: 0.9303 - val_loss: 0.1844 - val_accuracy: 0.9716\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.4405 - accuracy: 0.9309 - val_loss: 0.2069 - val_accuracy: 0.9695\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4494 - accuracy: 0.9321 - val_loss: 0.1849 - val_accuracy: 0.9699\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3517 - accuracy: 0.9369 - val_loss: 0.1731 - val_accuracy: 0.9716\nEpoch 8/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3356 - accuracy: 0.9343 - val_loss: 0.1214 - val_accuracy: 0.9712\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3250 - accuracy: 0.9335 - val_loss: 0.1176 - val_accuracy: 0.9725\nEpoch 10/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2862 - accuracy: 0.9379 - val_loss: 0.2065 - val_accuracy: 0.9544\nEpoch 11/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2800 - accuracy: 0.9391 - val_loss: 0.1199 - val_accuracy: 0.9690\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3356 - accuracy: 0.9324 - val_loss: 0.1601 - val_accuracy: 0.9682\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2712 - accuracy: 0.9351 - val_loss: 0.1385 - val_accuracy: 0.9660\nEpoch 14/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.2485 - accuracy: 0.9364 - val_loss: 0.1757 - val_accuracy: 0.9553\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2672 - accuracy: 0.9342 - val_loss: 0.1276 - val_accuracy: 0.9673\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2988 - accuracy: 0.9322 - val_loss: 0.1017 - val_accuracy: 0.9733\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2871 - accuracy: 0.9320 - val_loss: 0.1249 - val_accuracy: 0.9733\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2589 - accuracy: 0.9346 - val_loss: 0.1283 - val_accuracy: 0.9656\nEpoch 19/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3059 - accuracy: 0.9283 - val_loss: 0.1895 - val_accuracy: 0.9617\nEpoch 20/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2999 - accuracy: 0.9335 - val_loss: 0.1299 - val_accuracy: 0.9725\n\n\n\nplt.plot(history_model4.history['accuracy'], label='Training Accuracy')\nplt.plot(history_model4.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 4')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy of Model4 During Training: The validation accuracy of model4 during training consistently exceeded 97%.\nComparison to Model1: Model4’s validation accuracy is significantly higher than the accuracy achieved with model1, which struggled to surpass 80%. Model4’s superior performance suggests that leveraging a pre-existing model (MobileNetV3Large) and fine-tuning it for the specific task can lead to better results.\nOverfitting in Model4: Overfitting appears to be minimal in model4, as the validation accuracy closely tracks the training accuracy. The incorporation of a pre-trained base model likely contributed to this improved generalization."
  },
  {
    "objectID": "posts/hw5/index.html#conclusion",
    "href": "posts/hw5/index.html#conclusion",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Conclusion",
    "text": "Conclusion\nAs we can see, model4 is the most accurate model so we will test it on the test data for our final step:\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n37/37 [==============================] - 3s 77ms/step - loss: 0.1467 - accuracy: 0.9652\nTest Accuracy: 96.52%\n\n\nEach model brought its own set of techniques and improvements, showcasing the versatility of deep learning in computer vision tasks. The final model has an accuracy of 96.5%.\nIn conclusion, these models emphasized the significance of data preprocessing, architectural choices, and the influence of transfer learning. The flexibility of Keras and TensorFlow allowed us to experiment, iterate, and build a good image classification model."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]