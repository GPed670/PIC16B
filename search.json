[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog!",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome!\n\nThis is a photo I took in Kauai, Hawaii this past summer."
  },
  {
    "objectID": "posts/hw6/index.html",
    "href": "posts/hw6/index.html",
    "title": "Natural Language Processing: How Text Classification can Identify Fake News",
    "section": "",
    "text": "With so much information at hand, it is hard to distinguishing the truth from lies. The rise of fake news, has blurred the lines between fact and fiction. Text classification is a powerful technique that can be used to identify fake news. These models are often called NLPs, or Natural Language Processors.\nFor this tutorial, we will be using data from the article:\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\nWhich can be accessed through Kaggle. Each row of the data corresponds to an article with the title column giving the title of the article, and the text column giving the full article text. The fake column is 0 if the article is true and 1 if the article contains fake news.\nBefore writing any code we need to import some essential libraries for our data manipulation and the creation of our models. Additionally, we will need to install and install and updated version of keras.\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 6.2 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\nNext, we need to download our training set, using the url provided, to a pandas dataframe.\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(train_url)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn order to clean our data for easier processing we want to remove unnecessary words like “and”, “or”, and “the” that don’t contribute much to the meaning of the text. We also want to make everything lowercase. Finally we want to put our dataframe into a tensorflow Dataset object. We can do all of this by defining a function make_dataset:\nAdditionally, we can easily import a list of stopwords using the snipet bellow.\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n\ndef make_dataset(df):\n    \"\"\"\n    Preprocesses the input DataFrame for text classification.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing 'title', 'text', and 'fake' columns.\n\n    Returns:\n    - tf.data.Dataset: A TensorFlow Dataset containing preprocessed text and label tensors.\n    \"\"\"\n\n    # Make text all lowercase\n    df['text'] = df['text'].apply(lambda x: x.lower())\n\n    # Remove stopwords\n    stop = stopwords.words('english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n    # Make a tf Dataset\n    title_tensor = tf.constant(df['title'].values, dtype=tf.string)\n    text_tensor = tf.constant(df['text'].values, dtype=tf.string)\n    fake_tensor = tf.constant(df['fake'].values, dtype=tf.int32)\n\n    dataset = tf.data.Dataset.from_tensor_slices(({\"title\": title_tensor, \"text\": text_tensor}, fake_tensor))\n\n    # Batch the dataset\n    dataset = dataset.batch(100)\n\n    return dataset\n\n\nds = make_dataset(df)\n\nNext, we want to split the dataset into a training set and a validation set with a 80-20 ratio. We can do this by using the take and skip methods of the tensorflow Dataset object. When passed a number n, take will take only the first n items in the dataset, and skip will skip the first n items.\n\n# Shuffle the data and determine the validation set size\nds = ds.shuffle(buffer_size = len(ds), reshuffle_each_iteration=False)\nval_size = int(0.2 * len(ds))\n\n# Split the dataset\nval = ds.take(val_size)\ntrain = ds.skip(val_size)\n\n\nlen(train), len(val)\n\n(180, 45)\n\n\nIn order to get a good idea of the performance of our machine learning models we can compare them to the base rate. The base rate provides a baseline understanding of the distribution of fake news. We can calculate the base rate by comparing the number of fake vs true articles.\n\nfake_count = 0\ntotal_count = 0\n\nfor _, labels in train:\n    fake_count += tf.reduce_sum(labels).numpy()  # Sum of 0s and 1s where 1 represents 'fake'\n    total_count += len(labels)\n\nbase_rate = fake_count / total_count\n\nprint(\"Base rate:\", base_rate)\n\nBase rate: 0.5240403365089977\n\n\nAs we can see a random guess would have around a 52% probability of being correct in determining if an article is fake or not."
  },
  {
    "objectID": "posts/hw6/index.html#model-creation",
    "href": "posts/hw6/index.html#model-creation",
    "title": "Natural Language Processing: How Text Classification can Identify Fake News",
    "section": "Model Creation:",
    "text": "Model Creation:\nWe will be creating three models to determine the answer to the question: “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?” 1. A model only using the title as input. 2. A model only using the test as imput. 3. A model using both as input.\nIn order to answer this question we will be using the Functional API. The Functional API allows you to define models with complex multiple inputs and outputs, shared layers, and non-linear connectivity patterns. These features are essential for the three models we want to create using our dataset\nTo start off, understanding and representing textual data in a machine-readable format is an important initial step for text classification. We can use TextVectorization layer in TensorFlow to achieve this.\nTo use TextVectorization we must first import these functions.\n\nimport keras\nimport re\nimport string\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\n\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\nvectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\nThe standardization function within the TextVectorization layer converts text to lowercase and removes punctuation, offering a uniform representation that lessens the impact of irrelevant information in the dataset. TextVectorization also building a vocabulary from the training data. This vocabulary is limited to a specified number of tokens, providing a manageable structure to help control complexity and prevent overfitting.\n\nshared_embedding_layer = layers.Embedding(size_vocabulary, 3, name=\"embedding\")\n\nWe also must add this embedding layer. The Embedding layer is a transformer that converts words into dense vectors, known as word embeddings. These vectors encode semantic relationships and contextual information, providing a representation of words within the vocabulary.\nBoth of these layers will be shared layers for both of our inputs, title and text, thanks to our use of the Functional API.\n\nModel 1: Article Titles Only\nIn this TensorFlow Keras model designed for text classification, the Functional API is employed to construct a neural network architecture for discerning between real and fake news based on input titles.\nThe model begins with an input layer, ‘title_input,’ representing the titles of the texts and specifying what it expects for its input. The next steps involve using text vectorization, converting the titles into integer sequences using the shared vectorization layer from before. The shared embedding layer is then applied to transform these integer-encoded words into dense vectors, allowing the model to capture relationships between words.\nNext, Dropout layers are strategically inserted to mitigate overfitting, and Global average pooling ensures a fixed-length output, regardless of input length for generalization. Then a series of Dense layers are used.\n\n# Assuming you have already defined the title_input\ntitle_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n\n# Create the model for title input\ntitle_features = vectorize_layer(title_input)\ntitle_features = shared_embedding_layer(title_features)\ntitle_features = layers.Dropout(0.5)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dense(64, activation='relu')(title_features)\ntitle_features = layers.Dropout(0.3)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\n# Define the output layer for title input\ntitle_output = layers.Dense(1, activation='sigmoid', name=\"title_output\")(title_features)\n\n# Create the model\nmodel1 = tf.keras.Model(inputs=title_input, outputs=title_output)\nmodel1.summary()\n\nModel: \"model_19\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding (Embedding)       (None, 500, 3)            6000      \n                                                                 \n dropout_30 (Dropout)        (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d_1  (None, 3)                 0         \n 3 (GlobalAveragePooling1D)                                      \n                                                                 \n dense_24 (Dense)            (None, 64)                256       \n                                                                 \n dropout_31 (Dropout)        (None, 64)                0         \n                                                                 \n dense_25 (Dense)            (None, 32)                2080      \n                                                                 \n title_output (Dense)        (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 8369 (32.69 KB)\nTrainable params: 8369 (32.69 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nThe summary function can be used to view the structure of model1, but, even better, we can use the utils function as shown below. The visualization provides an overview of the model’s architecture, including the connectivity between layers, the shapes of the tensors, and the names of each layer.\n\nfrom keras import utils\n\n\n# Visualize the structure of the model\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nFinaly we can compile an fit our data to the model:\n\n# Compile and fit the model\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory1 = model1.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 [==============================] - 4s 14ms/step - loss: 0.5357 - accuracy: 0.8509 - val_loss: 0.2936 - val_accuracy: 0.9362\nEpoch 2/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2215 - accuracy: 0.9276 - val_loss: 0.1525 - val_accuracy: 0.9464\nEpoch 3/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1695 - accuracy: 0.9357 - val_loss: 0.1384 - val_accuracy: 0.9469\nEpoch 4/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1606 - accuracy: 0.9375 - val_loss: 0.1396 - val_accuracy: 0.9469\nEpoch 5/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1565 - accuracy: 0.9402 - val_loss: 0.1370 - val_accuracy: 0.9473\nEpoch 6/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1522 - accuracy: 0.9407 - val_loss: 0.1337 - val_accuracy: 0.9491\nEpoch 7/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1503 - accuracy: 0.9419 - val_loss: 0.1335 - val_accuracy: 0.9493\nEpoch 8/20\n180/180 [==============================] - 6s 30ms/step - loss: 0.1505 - accuracy: 0.9417 - val_loss: 0.1328 - val_accuracy: 0.9498\nEpoch 9/20\n180/180 [==============================] - 5s 27ms/step - loss: 0.1497 - accuracy: 0.9418 - val_loss: 0.1310 - val_accuracy: 0.9513\nEpoch 10/20\n180/180 [==============================] - 4s 21ms/step - loss: 0.1441 - accuracy: 0.9448 - val_loss: 0.1321 - val_accuracy: 0.9507\nEpoch 11/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1467 - accuracy: 0.9428 - val_loss: 0.1397 - val_accuracy: 0.9471\nEpoch 12/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1437 - accuracy: 0.9451 - val_loss: 0.1371 - val_accuracy: 0.9502\nEpoch 13/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1479 - accuracy: 0.9437 - val_loss: 0.1334 - val_accuracy: 0.9509\nEpoch 14/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1424 - accuracy: 0.9453 - val_loss: 0.1292 - val_accuracy: 0.9511\nEpoch 15/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1387 - accuracy: 0.9458 - val_loss: 0.1284 - val_accuracy: 0.9524\nEpoch 16/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1416 - accuracy: 0.9466 - val_loss: 0.1279 - val_accuracy: 0.9522\nEpoch 17/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.1397 - accuracy: 0.9466 - val_loss: 0.1310 - val_accuracy: 0.9518\nEpoch 18/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.1420 - accuracy: 0.9451 - val_loss: 0.1344 - val_accuracy: 0.9509\nEpoch 19/20\n180/180 [==============================] - 4s 23ms/step - loss: 0.1401 - accuracy: 0.9463 - val_loss: 0.1279 - val_accuracy: 0.9527\nEpoch 20/20\n180/180 [==============================] - 4s 23ms/step - loss: 0.1371 - accuracy: 0.9480 - val_loss: 0.1295 - val_accuracy: 0.9524\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\nIn the visualization below, we can see the results of our training in a better format. As we can see, the model achieves a validation accuracy average around 95% using only the titles of the articles as input.\n\n# Visualize the results of the training using matplotlib\nplt.plot(history1.history['accuracy'], label='Training Accuracy')\nplt.plot(history1.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel 2: Article Text Only\nUsing the same structure of layers for the second model but using the article text instead, we can further determine the answer to our question.\n\n# Create the input layer for the article text\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n\n# Create the model for text input\ntext_features = vectorize_layer(text_input)\ntext_features = shared_embedding_layer(text_features)\ntext_features = layers.Dropout(0.5)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dense(64, activation='relu')(text_features)\ntext_features = layers.Dropout(0.3)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# Create the text_output final layer\ntext_output = layers.Dense(1, activation='sigmoid', name=\"text_output\")(text_features)\n\nmodel2 = tf.keras.Model(inputs=text_input, outputs=text_output)\nmodel2.summary()\n\nModel: \"model_20\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding (Embedding)       (None, 500, 3)            6000      \n                                                                 \n dropout_32 (Dropout)        (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d_1  (None, 3)                 0         \n 4 (GlobalAveragePooling1D)                                      \n                                                                 \n dense_26 (Dense)            (None, 64)                256       \n                                                                 \n dropout_33 (Dropout)        (None, 64)                0         \n                                                                 \n dense_27 (Dense)            (None, 32)                2080      \n                                                                 \n text_output (Dense)         (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 8369 (32.69 KB)\nTrainable params: 8369 (32.69 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n# Visualize the structure of the model\nutils.plot_model(model2, \"model2.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we compile and fit the model to the training data:\n\n# Compile and fit the model\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory2 = model2.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 [==============================] - 5s 21ms/step - loss: 0.3855 - accuracy: 0.8710 - val_loss: 0.1533 - val_accuracy: 0.9569\nEpoch 2/20\n180/180 [==============================] - 4s 24ms/step - loss: 0.1393 - accuracy: 0.9528 - val_loss: 0.1336 - val_accuracy: 0.9593\nEpoch 3/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1274 - accuracy: 0.9598 - val_loss: 0.1295 - val_accuracy: 0.9580\nEpoch 4/20\n180/180 [==============================] - 3s 18ms/step - loss: 0.1197 - accuracy: 0.9612 - val_loss: 0.1262 - val_accuracy: 0.9582\nEpoch 5/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.1167 - accuracy: 0.9633 - val_loss: 0.1241 - val_accuracy: 0.9591\nEpoch 6/20\n180/180 [==============================] - 3s 19ms/step - loss: 0.1102 - accuracy: 0.9658 - val_loss: 0.1197 - val_accuracy: 0.9609\nEpoch 7/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1066 - accuracy: 0.9668 - val_loss: 0.1201 - val_accuracy: 0.9600\nEpoch 8/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.1061 - accuracy: 0.9674 - val_loss: 0.1147 - val_accuracy: 0.9640\nEpoch 9/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1043 - accuracy: 0.9685 - val_loss: 0.1127 - val_accuracy: 0.9642\nEpoch 10/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.1003 - accuracy: 0.9693 - val_loss: 0.1097 - val_accuracy: 0.9669\nEpoch 11/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.0990 - accuracy: 0.9704 - val_loss: 0.1084 - val_accuracy: 0.9664\nEpoch 12/20\n180/180 [==============================] - 3s 19ms/step - loss: 0.0971 - accuracy: 0.9688 - val_loss: 0.1079 - val_accuracy: 0.9653\nEpoch 13/20\n180/180 [==============================] - 6s 31ms/step - loss: 0.0949 - accuracy: 0.9702 - val_loss: 0.1036 - val_accuracy: 0.9698\nEpoch 14/20\n180/180 [==============================] - 5s 28ms/step - loss: 0.0928 - accuracy: 0.9709 - val_loss: 0.1035 - val_accuracy: 0.9669\nEpoch 15/20\n180/180 [==============================] - 4s 22ms/step - loss: 0.0910 - accuracy: 0.9710 - val_loss: 0.1001 - val_accuracy: 0.9702\nEpoch 16/20\n180/180 [==============================] - 8s 41ms/step - loss: 0.0897 - accuracy: 0.9717 - val_loss: 0.0978 - val_accuracy: 0.9700\nEpoch 17/20\n180/180 [==============================] - 5s 25ms/step - loss: 0.0868 - accuracy: 0.9725 - val_loss: 0.0972 - val_accuracy: 0.9696\nEpoch 18/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0852 - accuracy: 0.9731 - val_loss: 0.0950 - val_accuracy: 0.9707\nEpoch 19/20\n180/180 [==============================] - 3s 18ms/step - loss: 0.0826 - accuracy: 0.9725 - val_loss: 0.0935 - val_accuracy: 0.9713\nEpoch 20/20\n180/180 [==============================] - 4s 24ms/step - loss: 0.0840 - accuracy: 0.9730 - val_loss: 0.0919 - val_accuracy: 0.9702\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\nAs we can see bellow, this model achieves a better validation accuracy average than the first model. This is probably because there is more information given by the article txt than from the title, so more relationships between words can be determined. Additionally, there is some overfitting but not much more than what is expected.\n\nplt.plot(history2.history['accuracy'], label='Training Accuracy')\nplt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 2: Texts')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel 3: Article Title and Text\nFinally, we will use both the title and text in this model. Since we are using Functional API, we can use the layers we created for each input in the previous models and combine them for this model using a concatinate layer as shown below:\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\n\nIn order to prevent overfitting and to stabalize the model I have added some more Dense and Dropout layers:\n\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dropout(0.3)(main)\noutput = layers.Dense(1, name = \"genre\")(main)\n\nThen, we define the model with both the title input and text input:\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\n\nmodel3.summary()\n\nModel: \"model_21\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n title (InputLayer)          [(None, 1)]                  0         []                            \n                                                                                                  \n text (InputLayer)           [(None, 1)]                  0         []                            \n                                                                                                  \n text_vectorization_1 (Text  (None, 500)                  0         ['title[0][0]',               \n Vectorization)                                                      'text[0][0]']                \n                                                                                                  \n embedding (Embedding)       (None, 500, 3)               6000      ['text_vectorization_1[6][0]',\n                                                                     'text_vectorization_1[7][0]']\n                                                                                                  \n dropout_30 (Dropout)        (None, 500, 3)               0         ['embedding[4][0]']           \n                                                                                                  \n dropout_32 (Dropout)        (None, 500, 3)               0         ['embedding[5][0]']           \n                                                                                                  \n global_average_pooling1d_1  (None, 3)                    0         ['dropout_30[0][0]']          \n 3 (GlobalAveragePooling1D)                                                                       \n                                                                                                  \n global_average_pooling1d_1  (None, 3)                    0         ['dropout_32[0][0]']          \n 4 (GlobalAveragePooling1D)                                                                       \n                                                                                                  \n dense_24 (Dense)            (None, 64)                   256       ['global_average_pooling1d_13[\n                                                                    0][0]']                       \n                                                                                                  \n dense_26 (Dense)            (None, 64)                   256       ['global_average_pooling1d_14[\n                                                                    0][0]']                       \n                                                                                                  \n dropout_31 (Dropout)        (None, 64)                   0         ['dense_24[0][0]']            \n                                                                                                  \n dropout_33 (Dropout)        (None, 64)                   0         ['dense_26[0][0]']            \n                                                                                                  \n dense_25 (Dense)            (None, 32)                   2080      ['dropout_31[0][0]']          \n                                                                                                  \n dense_27 (Dense)            (None, 32)                   2080      ['dropout_33[0][0]']          \n                                                                                                  \n concatenate_7 (Concatenate  (None, 64)                   0         ['dense_25[0][0]',            \n )                                                                   'dense_27[0][0]']            \n                                                                                                  \n dense_28 (Dense)            (None, 32)                   2080      ['concatenate_7[0][0]']       \n                                                                                                  \n dense_29 (Dense)            (None, 32)                   1056      ['dense_28[0][0]']            \n                                                                                                  \n dropout_34 (Dropout)        (None, 32)                   0         ['dense_29[0][0]']            \n                                                                                                  \n genre (Dense)               (None, 1)                    33        ['dropout_34[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 13841 (54.07 KB)\nTrainable params: 13841 (54.07 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\nThrough this visualization of the model architecture, we can see how Functional API implements the layers of the different imputs and finally combines everything using the concatinatelayer. This visualization really highlights the advantiges of using Functional API to create an complex neural network.\n\nutils.plot_model(model3, \"model3.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNext, we can compile and fit the model to the training data:\n\nmodel3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory3 = model3.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 [==============================] - 7s 27ms/step - loss: 0.2234 - accuracy: 0.9525 - val_loss: 0.1513 - val_accuracy: 0.9711\nEpoch 2/20\n180/180 [==============================] - 7s 39ms/step - loss: 0.1134 - accuracy: 0.9801 - val_loss: 0.1010 - val_accuracy: 0.9813\nEpoch 3/20\n180/180 [==============================] - 6s 33ms/step - loss: 0.0888 - accuracy: 0.9809 - val_loss: 0.1286 - val_accuracy: 0.9811\nEpoch 4/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.1332 - accuracy: 0.9699 - val_loss: 0.1016 - val_accuracy: 0.9807\nEpoch 5/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0871 - accuracy: 0.9829 - val_loss: 0.1087 - val_accuracy: 0.9816\nEpoch 6/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.1119 - accuracy: 0.9792 - val_loss: 0.1112 - val_accuracy: 0.9809\nEpoch 7/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0789 - accuracy: 0.9840 - val_loss: 0.0750 - val_accuracy: 0.9820\nEpoch 8/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0767 - accuracy: 0.9846 - val_loss: 0.0985 - val_accuracy: 0.9820\nEpoch 9/20\n180/180 [==============================] - 8s 44ms/step - loss: 0.0792 - accuracy: 0.9857 - val_loss: 0.1151 - val_accuracy: 0.9789\nEpoch 10/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0667 - accuracy: 0.9850 - val_loss: 0.1028 - val_accuracy: 0.9818\nEpoch 11/20\n180/180 [==============================] - 6s 33ms/step - loss: 0.0825 - accuracy: 0.9828 - val_loss: 0.0987 - val_accuracy: 0.9824\nEpoch 12/20\n180/180 [==============================] - 5s 30ms/step - loss: 0.1185 - accuracy: 0.9678 - val_loss: 0.1219 - val_accuracy: 0.9780\nEpoch 13/20\n180/180 [==============================] - 6s 35ms/step - loss: 0.0811 - accuracy: 0.9826 - val_loss: 0.1243 - val_accuracy: 0.9811\nEpoch 14/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0759 - accuracy: 0.9832 - val_loss: 0.0861 - val_accuracy: 0.9811\nEpoch 15/20\n180/180 [==============================] - 6s 35ms/step - loss: 0.1334 - accuracy: 0.9572 - val_loss: 0.1055 - val_accuracy: 0.9796\nEpoch 16/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0757 - accuracy: 0.9836 - val_loss: 0.1145 - val_accuracy: 0.9804\nEpoch 17/20\n180/180 [==============================] - 6s 32ms/step - loss: 0.0948 - accuracy: 0.9802 - val_loss: 0.1085 - val_accuracy: 0.9820\nEpoch 18/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0715 - accuracy: 0.9841 - val_loss: 0.0987 - val_accuracy: 0.9800\nEpoch 19/20\n180/180 [==============================] - 6s 31ms/step - loss: 0.0646 - accuracy: 0.9858 - val_loss: 0.1047 - val_accuracy: 0.9791\nEpoch 20/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.0826 - accuracy: 0.9842 - val_loss: 0.1163 - val_accuracy: 0.9818\n\n\nAs we can see through the visualization, this model achieves the highest average validation accuracy of 98%. This is better than both of the validation accuracies of model1 and model2. Hoever, we can see that the training accuracy fluctuates a lot and oscilates between overfitting and underfitting.\n\nplt.plot(history3.history['accuracy'], label='Training Accuracy')\nplt.plot(history3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTesting on the Best Model:\nAs we concluded above, model3 has teh best average validation accuracy so we will use it to evaluate the test data.\nFirst, we need to dowload the test data and put it into the proper format using the make_dataset function we defined previously.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\n\n# Read the CSV file into a DataFrame\ndf_test = pd.read_csv(test_url)\ndf_test.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n420\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\n14902\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\n322\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\n16108\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\n10304\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntest_ds = make_dataset(df_test)\n\nNext we can evaluate the test data using the model. As can be seen below, it achieves an accuracy of 98.22% similar to the average of the validation accuracy.\n\ntest_loss, test_accuracy = model3.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n225/225 [==============================] - 3s 15ms/step - loss: 0.1150 - accuracy: 0.9822\nTest Accuracy: 98.22%\n\n\n\n\nVisualize Embedding:\nFinally, we want to visualize the embedding done by our shared embedding layer in our model3. This will give us insight into the relationship our model determines between words in the vocabulary.\nIn order to visualize this we can use sklearn’s PCA, Principal Component Analysis. PCA transform a dataset with correlated features into a new set of uncorrelated features, known as principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain in the data.\n\nweights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nThen, we can use a plotly scatterplot to visualize these relationships:\n\nimport plotly.express as px\nimport numpy as np\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\",\n                 title='Word Embeddings Visualization')\n\nfig.update_layout(\n    xaxis=dict(title='Principal Component 1'),\n    yaxis=dict(title='Principal Component 2'),\n)\n\nfig.show()\n\n\n\n\nIn the visualization, words that are close together might have similar semantic meanings or play a significant role in distinguishing between real and fake news. For example, “god,” “elections,” and “donald” are all closely grouped on the lower half of the center vertical axis and “gay,” “transgender,” and “democrat” are grouped near the center on the horizontal axis which might suggest that the model might classify them as having similar semantic meanings. These are often topics that are grouped together because of the political meanings they are associated with, so it would not be surprising if these are often found in articles together."
  },
  {
    "objectID": "posts/hw6/index.html#conclusion",
    "href": "posts/hw6/index.html#conclusion",
    "title": "Natural Language Processing: How Text Classification can Identify Fake News",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn conclusion, natural language processors and text classification can be a powerful tool for determining if an article contains fake news or not. Model3 resulted in a 98% test accuracy using both the article title and the article text. This was achieved by using Functional API features such as shared layers and multiple inputs and by using embeding and word processing to define the semantic relationships between different words. So in answer to the original question, it is most effective to focus on both the title and text to determine if an article is fake news."
  },
  {
    "objectID": "posts/hw4/index.html",
    "href": "posts/hw4/index.html",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "",
    "text": "Building upon the one-dimensional heat diffusion example from our lecture notes, where we represented one-dimensional heat diffusion as a sequence of matrix-vector multiplications, now we will foccus on modeling 2-D heat diffusion.\nTwo-dimensional heat equation: \\[\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2 f}{\\partial x^2 }+ \\frac{\\partial^2 f}{\\partial y^2 }\\;.\\]\nIn a discrete approximation, we can write this as \\[x_i = i \\Delta x,\\;\\; y_j = j \\Delta y,\\;\\; t_k = k \\Delta t\\;,\\] for \\[i = 0, \\cdots, N-1; j = 0, \\cdots, N-1; k = 0, 1, 2 \\cdots\\] Let \\[u_{i, j}^k = f(x_i, y_j, t_k)\\] then we can write the update equation in real-time as \\[u_{i, j}^{k+1} \\approx u_{i, j}^k + \\epsilon \\left(u_{i+1, j}^k  + u_{i-1, j}^k + u_{i, j+1}^k + u_{i, j-1}^k - 4 u_{i, j}^k\\right),\\] where epsilon is a small parameter.\n# We will use:\nN = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/hw4/index.html#part-1-matrix-multiplication-method",
    "href": "posts/hw4/index.html#part-1-matrix-multiplication-method",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "Part 1: Matrix Multiplication Method",
    "text": "Part 1: Matrix Multiplication Method\nAs described above let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution. Each iteration of the update is created by advance_time_matvecmul using matrix-vector multiplication.\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    # update u\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThen using the indexing of u we create a matrix A that has the size of N^2 x N^2, without all-zero rows or all-zero columns. We use the get_A function to create the matrix.\n\n\ndef get_A(N):\n    \"\"\"\n    Takes the value N as the argument and returns the corresponding matrix A\n    Args:\n        N: simulation size\n        \n    Returns:\n        A: the corresponding matrix\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    \n    # add together arrays to create N*N matrix\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    \n    # return matrix\n    return A\n\n\n\nNow we will initialize and visualize u0, the initial condition of 1 unit of heat at the midpoint of the visualization. We will also create the matric A using get_A.\n\n# Initialize A and other parameters as needed\nA = get_A(N)\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\nplt.imshow(u0)\nplt.title('Initial Condition')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nNow we will use a time-iteration loop for simulating heat diffusion in a 2D space using matrix-vector multiplication. The simulation is performed for a total of 2700 iterations, with results visualized at every 300th interval. The advance_time_matvecmul function updates the temperature distribution at each iteration based on the discrete heat diffusion equation.\n%%timeit -r 1 -n 1: This line is a Jupyter magic command (%%timeit) that measures the execution time of the subsequent code. -r 1 specifies a single run, and -n 1 indicates a single execution per run. The execution time is measured using the %timeit magic command, providing insights into the computational efficiency of the simulation.\n\n%%timeit -r 1 -n 1\nu = u0\nsubplot_num = 1\nsolution_array1 = [u0.copy()]\n\nfor t in range(1, 2701):\n    if t % 300 == 0:\n        solution_array1.append(u.copy())\n        plt.subplot(3, 3, subplot_num)\n        plt.imshow(u)\n        plt.title(f'Iteration {t}')\n        subplot_num += 1\n    u = advance_time_matvecmul(A, u, epsilon)\n\n2min 10s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nHere is the result of the loop we ran and we can see the visualizations model the diffusion of the heat from the 300th iteration to the 2700th. As we can see it took around two minutes for the code to create the solutions which is a long time, so we can try other methods to see if we can create solutions faster."
  },
  {
    "objectID": "posts/hw4/index.html#part-2-jax-sparce-matrix-method",
    "href": "posts/hw4/index.html#part-2-jax-sparce-matrix-method",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "Part 2: JAX Sparce Matrix Method",
    "text": "Part 2: JAX Sparce Matrix Method\nNow we want the improve the performance inefficiency observed in Part 1. This optimization strategy involves using the sparse matrix data structure with the batched coordinate (BCOO) format, which efficiently handles matrices with many zero elements. The JAX package is utilized for this purpose. The goal is to redefine the matrix A in sparse format using the function get_sparse_A(N) and then repeat the simulation from Part 1 using the sparse matrix representation along with the JIT-compiled version of the advance_time_matvecmul function. This approach significantly reduces space usage and accelerates the update computations.\n\nfrom jax import jit\nimport jax.numpy as jnp\n\n\n\ndef get_sparse_A(N):\n    \"\"\"\n    Generates a sparse matrix in BCOO format from a dense matrix.\n\n    Parameters:\n    - N (int): The size of the square dense matrix to be generated.\n\n    Returns:\n    sparse.BCOO: A sparse matrix in BCOO format.\n    \"\"\"\n    \n    # Get dense matrix A\n    dense_A = get_A(N)\n\n    # Convert dense matrix to sparse BCOO format\n    A_sp_matrix = sparse.BCOO.fromdense(dense_A)\n\n    return A_sp_matrix\n\n\n\nUsing the similar code as with Part 1 but replacing get_A with get_sparse_A and using the jit version of advance_time_matvecmul:\n\nA = get_sparse_A(N)\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n\n%%timeit -r 1 -n 1\nu = jnp.array(u0)\nsubplot_num = 1\nsolution_array2 = [u0.copy()]\n\nfor t in range(1, 2701):\n    if t % 300 == 0:\n        solution_array2.append(u.copy())\n        plt.subplot(3, 3, subplot_num)\n        plt.imshow(u)\n        plt.title(f'Iteration {t}')\n        subplot_num += 1\n    u = advance_time_matvecmul_jit(A, u, epsilon)\n\n1.77 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nWe can see that using this method we obtain the same visualizations; however, the it only took around 2 seconds to run in comparison to the 2 minutes of Part 1. This is much better, but we can still improve the efficiency."
  },
  {
    "objectID": "posts/hw4/index.html#part-3-direct-operation-with-numpy-method",
    "href": "posts/hw4/index.html#part-3-direct-operation-with-numpy-method",
    "title": "Homework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax",
    "section": "Part 3: Direct Operation with Numpy Method",
    "text": "Part 3: Direct Operation with Numpy Method\nAlternatively, instead of relying on matrix-vector multiplication, we can use vectorized array operations, specifically the np.roll() function, to advance the solution by one timestep. This approach is particularly relevant for problems like the heat equation, where the use of matrix-vector multiplication may not be a necessity in terms of computation. The function, advance_time_numpy(u, epsilon), is designed to advance the solution using a padded array and np.roll().\n\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of a 2D grid-based diffusion process over a given time step using NumPy.\n\n    Args:\n        u (numpy.ndarray): The input 2D array representing the current state of the simulation.\n        epsilon (float): The diffusion coefficient.\n\n    Returns:\n        numpy.ndarray: The updated 2D array representing the state of the simulation after the time step.\n    \"\"\"\n    \n    # Pad the input array with zeros to form an (N+2) x (N+2) array internally\n    pad_u = np.pad(u, 1, mode='constant', constant_values=0)\n\n    # Roll the padded array along both axes to simulate diffusion\n    u_shift_right = np.roll(pad_u, shift=1, axis=1)\n    u_shift_left = np.roll(pad_u, shift=-1, axis=1)\n    u_shift_up = np.roll(pad_u, shift=-1, axis=0)\n    u_shift_down = np.roll(pad_u, shift=1, axis=0)\n\n    # Update the solution using vectorized array operations and return to NxN\n    u_shifted = (u_shift_right + u_shift_left + u_shift_up + u_shift_down)[1:-1, 1:-1]\n    u_new = u + epsilon * (u_shifted - 4 * u)\n\n    return u_new\n\n\n\n\npad_u = np.pad(u, 1, mode=‘constant’, constant_values=0): Pads the input array u with zeros to form an (N+2) x (N+2) array internally. The padding ensures that the edges of the grid are considered during diffusion.\nnp.roll(pad_u, shift=1, axis=1): Rolls the padded array to the right along the second axis. This is used for all directions of the array.\n\nUsing the similar code as with Part 1 and 2 but updating the solution u using advance_time_numpy:\n\n%%timeit -r 1 -n 1\nu = u0\nsubplot_num = 1\nsolution_array3 = [u0.copy()]\n\nfor t in range(1, 2701):\n    if t % 300 == 0:\n        solution_array3.append(u.copy())\n        plt.subplot(3, 3, subplot_num)\n        plt.imshow(u)\n        plt.title(f'Iteration {t}')\n        subplot_num += 1\n    u = advance_time_numpy(u, epsilon)\n\n986 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nWe can see from the time mesurement that this method is slightly faster than the previous method by almost 1 second."
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "",
    "text": "Structured Query Language, or SQL for short, is a programming language designed for managing and manipulating relational databases. In a relational database, data is stored in tables, which consist of rows and columns. The relationships between tables are defined by using keys, such as primary keys and foreign keys. A primary key is a unique identifier for each record in a table. It ensures that each row can be uniquely identified and allows for the creation of relationships between tables. A foreign key is a field in one table that refers to the primary key in another table. SQL establishes relationships between tables, allowing data to be linked across tables using these keys. It provides a way for users to interact with databases, to retrieve and manipulate data. Users retrieve data from databases by writing queries which specify conditions to filter and sort the data.\n\n\n\nUsing databases can be more efficient than working only with Pandas DataFrames because databases offer long-term storange and handle large datasets more efficiently.\n\n# import necessary libraries\nimport pandas as pd\nimport sqlite3\n\n\nconn = sqlite3.connect(\"climate2.db\") # create a database in current directory called climate.db\n\n\n# create a dataframe iterator for the temperature data \ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf_iter\n\n&lt;pandas.io.parsers.readers.TextFileReader at 0x14a375f3550&gt;\n\n\nUsing pd.read_csv() with the chunksize parameter is used when dealing with large datasets that may not fit into memory. It allows you to read and process the data in smaller, manageable chunks rather than loading the entire dataset at once.\ndf_iter returns an iterator, enabling you to process the data iteratively. You can perform operations on each chunk without loading the entire dataset into memory. df_iter is initialized with pd.read_csv(), specifying the file path and chunksize. It returns an iterator object where each element corresponds to a chunk of data with the specified number of rows (chunksize).\nTo access each chunk, you can iterate over df_iter using the next() magic method. The actual data is read and processed on-the-fly during the iteration which allows you to work with one chunk at a time.\n\ndf = df_iter.__next__()\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ndf.shape\n\n(100000, 14)\n\n\nThe prepare_df function below reorganizes the structure of the DataFrame (df) and performs some data manipulation steps so that each temperature reading has its own row with the corresponding station, month, and year. The transformed DataFrame has a new structure with the columns “ID,” “Year,” “Month,” and “Temp.”\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf = prepare_df(df)\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\nThis next portion is a loop that iterates over chunks of a DataFrame iterator (df_iter). It transforms each chunk by using the prepare_df function and then writes the resulting DataFrame to a SQL table named “temperatures.” The following blocks of code use the station-metadata.csv and the fips-country-codes.csv files to create the stations and countries tables in the same SQL database.\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\n\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    \nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\nstations = pd.read_csv(\"fips-10-4-to-iso-country-codes.csv\")\nstations.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nHere we can see the results of the previous commands and can see all of the different tables and their categories in the database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\nconn.commit()  #commit() is a method used to finalize and confirm any \n               #        changes made during a series of database operations within a transaction.\nconn.close()   #close() is a method used to terminate the connection to the database"
  },
  {
    "objectID": "posts/hw1/index.html#visualization-1-average-temperature-over-time",
    "href": "posts/hw1/index.html#visualization-1-average-temperature-over-time",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 1: Average Temperature Over Time",
    "text": "Visualization 1: Average Temperature Over Time\nThe average_temperature_line_plot function creates a line plot using Plotly to visualize the average temperature over time for a specific country within a given year range. The function queries db_file using the avg_temp_years_query function to retrieve temperature data for a specified country and time range and calculates the average temperature for each year. Then it creates a line graph using the plotly method line.\n\nfrom climate_database import avg_temp_years_query\nprint(inspect.getsource(avg_temp_years_query))\n\ndef avg_temp_years_query(db_file, country, year_begin, year_end):\n    \n    \"\"\"\n    Query the climate database to calculate the average temperature for each year within a specified country and time range.\n\n    Parameters:\n    - db_file (str): The path to the SQLite database file.\n    - country (str): The name of the country for which average temperature data is requested.\n    - year_begin (int): The starting year of the desired time range.\n    - year_end (int): The ending year of the desired time range.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing average temperature data for each year, including columns 'Year' and 'avg_temp'.\n\n    Note:\n    - The function performs a SQL query to join temperature, station, and country data based on common IDs.\n    - It filters data based on the specified country and time range.\n    - The resulting DataFrame includes information about the average temperature for each year.\n    \"\"\"\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n        SELECT t.Year, AVG(t.Temp) AS avg_temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.ID\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY year\n    \"\"\"\n    \n    # Execute the query and fetch the results into a Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    \n    return df\n\n\n\n\nfrom climate_database import average_temperature_line_plot\nprint(inspect.getsource(average_temperature_line_plot))\n\ndef average_temperature_line_plot(db_file, country, year_begin, year_end):\n    \n    \"\"\"\n    Generate a line plot using Plotly Express to visualize the average temperature over time for a specified country.\n\n    Parameters:\n    - db_file (str): The path to the SQLite database file.\n    - country (str): The name of the country for which average temperature data is visualized.\n    - year_begin (int): The starting year of the desired time range.\n    - year_end (int): The ending year of the desired time range.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A line plot representing the average temperature over time for the specified country.\n\n    Note:\n    - The function uses the avg_temp_years_query function to retrieve average temperature data.\n    - The 'avg_temp' column is rounded to 2 decimal places for clarity.\n    - The line plot is created using Plotly Express with the x-axis representing years and y-axis representing average temperature.\n    - The plot has a title indicating the country and the specified time range.\n    \"\"\"\n    \n    # Get data using the query function\n    data = avg_temp_years_query(db_file, country, year_begin, year_end)\n    \n    data['avg_temp'] = data['avg_temp'].round(2)\n\n    # Create line plot with facets using Plotly Express\n    fig = px.line(data, \n                  x='Year', \n                  y='avg_temp',\n                  labels={'avg_temp': 'Average Temperature (°C)'},\n                  title=f'Average Temperature Over Time in {country} ({year_begin}-{year_end})')\n\n    return fig\n\n\n\nThe line plot allows users to observe trends and fluctuations in the average temperature over time. It provides insights into the climate patterns of the selected country during the specified period. Here is an example of the increase in temperature over the years in Thailand. This type of visualization could be useful for observing climate change and how it affects different countries.\n\nfig_line = average_temperature_line_plot(\"climate2.db\", \"Thailand\", 1980, 2020)\nfig_line.show()"
  },
  {
    "objectID": "posts/hw1/index.html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "href": "posts/hw1/index.html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 2: Bar Chart of Average Temperatures Each Month",
    "text": "Visualization 2: Bar Chart of Average Temperatures Each Month\nThe monthly_temperature_bar_chart creates a multifaceted bar chart that shows the monthly temparute for a given country throughout a range of years. In order to create this bar chart we must first find the global average temperature in order to range the colors of the bars correctly.\nThe global_average_temperature function simplifies the process of retrieving and calculating the average temperature across all records in the “temperatures” table of an SQLite database\n\nfrom climate_database import global_average_temperature\nprint(inspect.getsource(global_average_temperature))\n\ndef global_average_temperature(db_file):\n    \"\"\"\n    Calculate the global average temperature from a database.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n\n    Returns:\n    - float: Global average temperature in Celsius.\n    \"\"\"\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # SQL query to calculate the global average temperature\n    query = \"\"\"\n        SELECT AVG(Temp) AS global_avg_temp\n        FROM temperatures\n    \"\"\"\n\n    # Execute the query and fetch the result\n    result = conn.execute(query).fetchone()\n    \n    # Close the database connection\n    conn.close()\n\n    # Extract the global average temperature from the result\n    global_avg_temp = result[0]\n\n    return global_avg_temp\n\n\n\nThen we can query the database and create a bar chart with avg_monthly_temperature_bar_chart for the average temoerature of each month in a given range of years.\nFirst, the function connectes to the database specified by db_file. Then, it uses an SQL query to retrieve the temperature for each month within the specified country and year range. The query is executed using the pd.read_sql_query method, which putts the data into a DataFrame (df). Next, the month numbers are mapped to the corresponding month names. The function calls the global_average_temperature function to calculate the global average temperature from the entire dataset, so that it can center the color scheme correctly with, with white being the average, red being hotter temperatures, and blue being cooler temperstures.Then it uses the plotly bar function to create a multifaceted bar chart using the facet_col parameter.\n\nfrom climate_database import monthly_temperature_bar_chart\nprint(inspect.getsource(monthly_temperature_bar_chart))\n\ndef global_average_temperature(db_file):\n    \"\"\"\n    Calculate the global average temperature from a database.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n\n    Returns:\n    - float: Global average temperature in Celsius.\n    \"\"\"\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # SQL query to calculate the global average temperature\n    query = \"\"\"\n        SELECT AVG(Temp) AS global_avg_temp\n        FROM temperatures\n    \"\"\"\n\n    # Execute the query and fetch the result\n    result = conn.execute(query).fetchone()\n    \n    # Close the database connection\n    conn.close()\n\n    # Extract the global average temperature from the result\n    global_avg_temp = result[0]\n\n    return global_avg_temp\n\n\n\nIn summary, this function connects to the database, retrieves monthly temperature data for a specified country and year range, and creates an interactive, multifacted bar chart using Plotly, where the color of each bar represents the temperature value. The resulting visualization provides an representation of how monthly temperatures vary over a range of years for the selected country.\nThis visualization could help answer the question “How does the average monthly temperature vary over the specified years for a particular country?” This visualization provides insights into the seasonal temperature changes in different countries over certain years. By examining the color of the bars, users can identify periods of higher or lower average temperatures throughout the specified time range.\nBelow is an example for the function in use to find the temperatures of each month in Finland between 2000 and 2020 and Egypt between 1996 and 1999:\n\nfig = monthly_temperature_bar_chart('climate2.db', 'Finland', 2000, 2003)\nfig.show()\n\n                                                \n\n\n\nfig = monthly_temperature_bar_chart('climate2.db', 'Egypt', 1996, 1999)\nfig.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "Natural Language Processing: How Text Classification can Identify Fake News\n\n\n\n\n\n\nweek 10\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification using Tensorflow and Keras: Cats vs. Dogs\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Different Methods of Modeling 2-D Heat Diffusion using Numpy and Jax\n\n\n\n\n\n\nweek 7\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Web Scraping Movie Recomendations using Scrapy\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: Data Wrangling and Visualization using NOAA Climate Data\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial: Constructing a Data Visualization of the Palmer Penguins Dataset\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "posts/hw0/index.html",
    "href": "posts/hw0/index.html",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "",
    "text": "Data visualization is an important skill for understanding and drawing insight from datasets. The Palmer Penguins dataset contains data on 344 penguins of three different species within the Palmer Archipelago in Antarctica. In this tutorial, we will explore how to create an interesting data visualization pf the Palmer Penguins dataset using Python."
  },
  {
    "objectID": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "href": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 1: Import Libraries and Load the Dataset",
    "text": "Step 1: Import Libraries and Load the Dataset\nWe want to import the pandas library in order to create a Dataframe class object containing all of the information from the palmer_penguins.csv file that we are downloading directly from the given url. Then, we need matplotlib in order to create our visualization. Other libraries can be used for visualizing the data besides matplotlib, but this is the most well known.\n\nimport pandas as pd # Contains the class DataFrame\nimport matplotlib.pyplot as plt # For creating simple graphs\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nA csv file is a file formated to contain data that is seperated by commas for the different collumns. We can easily create a pandas DataFrame object by reading in a csv file of the Palmer Penguins dataset from the url given above.\nAdditionally we want to add names for the columns:\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]"
  },
  {
    "objectID": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "href": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 2: Explore and Clean the Data",
    "text": "Step 2: Explore and Clean the Data\nBefore creating a visualization, it’s essential to understand the structure and contents of the dataset. First, we want to simply take a look at the database we have created in order to determine if any rows or columns need to be cleaned.\n\npenguins.head() #Shows the first five rows of the dataframe\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n3\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n\n\n\n\n\nThen, we need to clean out any empty datapoints using the dropna method, and we want to make the dataframe easier to read by simplifying the species name.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\nAdditionally, often times data sets contain columns that are not relevant to our study or visualization so they can be deleted from the dataframe for simplicity. In this case, we will keep all of the columns for the different examples given below."
  },
  {
    "objectID": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "href": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 3: Choose an Appropriate Method of Visualization",
    "text": "Step 3: Choose an Appropriate Method of Visualization\nFor the visualization, there are many different options. Your visualization should be based off of what you are trying to analyze in the data. For example, let’s say that I want to compare the size of the culmen of the three different species. For this, I will be using a scatter plot to visualize the clustering of the three groups.\nFirst, I will create a dictionary of the species and colors they should be associated with. Then, I will create a scatter plot for each species. Also, I want to add some additional features to the scatter plot, axis labels, and a title. This makes the graph easier to read and visually appealing.\n\n# library with associated species and colors\ncolors = {'Adelie': 'blue', 'Chinstrap': 'orange', 'Gentoo': 'green'}\n\n# Create and individual scatter plot for each species\nfor species, col in colors.items():\n    species_data = penguins[penguins['Species'] == species]\n    plt.scatter(species_data['Culmen Length (mm)'], species_data['Culmen Depth (mm)'], label=species, color=col)\n\n# Add aditional features to the plot\nplt.legend(title='Species')\nplt.title('Culmen Size of Penguin Species')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, I could also choose to compare the different body masses of each species using a box plot. First, I will need to import a library that could help me do this, like plotly. Then I can use the box method to create a box plot with the Species as the x-axis and the body mass as the y-axis.\n\n# import the relevant library\nimport plotly.express as px\n\n# Create the box plot and add a title\nbox = px.box(penguins, x = 'Species', y = 'Body Mass (g)', color = 'Species')\nbox.update_layout(title_text=\"Body Mass Distribution of Penguin Species\")\n\n# Show the plot\nbox.show()"
  },
  {
    "objectID": "posts/hw0/index.html#conclusion",
    "href": "posts/hw0/index.html#conclusion",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, you can make many different types of interesting visualization from this dataset that can help you draw different insight. Choosing the right visualization depends on what you’re trying to find, and there are many different libraries and graphs that can help you with that. Once your visualization is created, interpret the results and analyze any patterns or trends observed from the visualization. Feel free to experiment with other types of visualizations and variables from the dataset to create a comprehensive understanding of the data. Remember to share your findings and insights in a clear and concise manner by making your visualizations easy to read and visibly satisfying. Data visualization is not only about creating beautiful plots but also about effectively communicating information."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "Homework 2: Web Scraping Movie Recomendations using Scrapy",
    "section": "",
    "text": "Introduction:\nIn this blog post, I will walk you through the process of creating an web scraper designed to explore movie recomendations based on your favorite movie. This python web scraper collects movie recomendations by extracting information about movie casts from TMDB. The goal is to extract data on actors, movies, and their relationships to build a simple movie recommendation system based on shared actors.\nFirst we must set up the project files in the terminal using:\n    conda activate PIC16B-24W\n    scrapy startproject TMDB_scraper\n    cd TMDB_scraper\nNow we must create a tmdb_spider.py file in the spiders file for our scraper class.\n\n\nUnderstanding the Scraper\nThe web scraper is designed to collect data from The Movie Database (TMDb). It is designed as a class called TmdbSpider in the tmbd_spider.py file containing four methods: init, parse, parse_full_credits, parse_actor_page. To scrape the necessary information from each page we have to use css commands to extract information from the html of the given url.\nparse() This method initiates the scraping process by navigating to the Full Cast & Crew page of a movie. It then calls the parse_full_credits() method to extract actor information.\n\n\n    def parse(self, response):\n        \"\"\"\n        Navigate to the Full Cast & Crew page and call parse_full_credits.\n\n        Args:\n            response (scrapy.http.Response): The response object for the movie page.\n\n        Yields:\n            scrapy.Request: Request to the Full Cast & Crew page.\n        \"\"\"\n        cast_url = f\"{response.url}/cast\"\n        yield scrapy.Request(cast_url, callback=self.parse_full_credits)\n\n\n\nparse_full_credits() This method parses the Full Credits page for the given movie, extracting actor names and their profile page URLs. It yields requests to scrape each actor’s page using the parse_actor_page() method while passing along the actor name. This method scrapes information specifically from the left panel of the page because we want to only include actors and exclude crew, producers, etc.\n\n\n    def parse_full_credits(self, response):\n        \n        \"\"\"\n        Parse the Full Credits page for a movie, extract actor names and their respective profile page URLs,\n        and yield requests to scrape each actor's page.\n\n        Args:\n            response (scrapy.http.Response): The response object for the Full Credits page.\n\n        Yields:\n            scrapy.Request: Request to the actor's page.\n        \"\"\"\n        # selects the lines containing the actor url's and their names\n        actor_selectors = response.css('div.content_wrapper.false section.panel.pad:first-child div.info a[href^=\"/person/\"]')\n        \n        # Loops through the list of actor information\n        for actor_selector in actor_selectors:\n            \n            # Extracts actor names and actor url\n            actor_name = actor_selector.css('::text').extract_first()\n            actor_page_url = urljoin(response.url, actor_selector.css('::attr(href)').extract_first())\n\n            # Debug information\n            #self.logger.info(f'Actor Name: {actor_name}, Actor Page URL Relative: {actor_page_url_relative}')\n\n            # Debug information\n            #self.logger.info(f'Actor Page URL: {actor_page_url}')\n\n            yield scrapy.Request(actor_page_url, callback=self.parse_actor_page, meta={'actor_name': actor_name})\n\n\n\nparse_actor_page() This method parses an actor’s profile page, extracting movie titles and their URLs. It yields information about the actor and the associated movie or TV show. This method scrapes information specifically from the Actor table of the page because we want to only include acting roles and exclude any other roles.\n\n\n    def parse_actor_page(self, response):\n        \n        \"\"\"\n        Parse an actor's profile page, extract movie titles and their respective URLs,\n        and yield information about the actor and the associated movie.\n\n        Args:\n            response (scrapy.http.Response): The response object for the actor's profile page.\n\n        Yields:\n            dict: Information about the actor and the associated movie.\n        \"\"\"\n        \n        actor_name = response.meta.get('actor_name')\n\n        # Extracting movies from lines like '&lt;a class=\"tooltip\" href=\"/movie/1236045\"&gt;\n        #                                      &lt;bdi&gt;Animals&lt;/bdi&gt;'\n        #                                    &lt;/a&gt;\n        h3_cat = response.css('div.credits_list')[0].css('h3::text').getall()\n        num = h3_cat.index(\"Acting\")\n        movie_selectors = response.css('div.credits_list')[0].css('table.card.credits')[num].css('a.tooltip')\n\n        for movie_selector in movie_selectors:\n            # Extract movie title\n            movie_title = movie_selector.css('bdi::text').extract_first().strip()\n             # Extracting movie URL\n            movie_url_relative = movie_selector.css('::attr(href)').extract_first()\n\n            # Join the relative URL with the base URL of the current page\n            movie_url = response.urljoin(movie_url_relative)\n            \n            '''\n            if movie_url + '-' + convert_to_slug(movie_title) == self.start_urls:\n                self.logger.info(f'Skipping movie: {movie_title} (same as original)')\n                continue\n            '''\n\n            # Debug information\n            #self.logger.info(f'Movie Title: {movie_title}')\n            #self.logger.info(f'Movie URL: {movie_url}-{convert_to_slug(movie_title)}')\n            \n            if movie_title and movie_url:\n                yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_title}\n            else:\n                # If movie_title or movie_url is not present, log a warning\n                self.logger.warning('Movie title or URL not found in the selector')\n\n\n\nTo run the scraper, use the following command in the terminal:  scrapy crawl tmdb_spider -o results.csv -a subdir=489-good-will-hunting  Where we replace the subdir parameter with the movie’s subdir you want to start the scraping process from. You can find the subdir by checking the url of the TMBD page for the movie you want recomendations from.\nNow we are returned a results.csv file containing the recomendations gathered from the parsing of the page.\n\n\nResults and Analysis\nFrom the results.csv we want to extract the recomended movies. Logically, the best recomendations will be those with the most number of shared actors from the original movie. We can see below that the csv has two columns, actor, and movie_or_TV_name, so we want to reorganize the dataframe so that it combines repetitions of the same movie and counts the number of shared actors.\n\n# import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv(\"goodwillhunting_recs.csv\")\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPatrick O'Donnell\n2 By 4\n\n\n1\nPatrick O'Donnell\nGood Will Hunting\n\n\n2\nScott William Winters\nA Little Dream\n\n\n3\nScott William Winters\nNarcos: Mexico\n\n\n4\nScott William Winters\nBeautifully Broken\n\n\n...\n...\n...\n\n\n1500\nRobin Williams\nThe Tonight Show Starring Johnny Carson\n\n\n1501\nRobin Williams\nThe Grammy Awards\n\n\n1502\nRobin Williams\nTony Awards\n\n\n1503\nRobin Williams\nThe Oscars\n\n\n1504\nRobin Williams\nToday\n\n\n\n\n1505 rows × 2 columns\n\n\n\n\nprint(df.shape)\n\n(1505, 2)\n\n\nIn order to reorganize the dataframe I have created a function called create_shared_actors_df. It takes the DataFrame obtained from the scraper as input and generates a new DataFrame with information about shared actors for each movie. The resulting DataFrame is sorted in descending order based on the number of shared actors.\n\ndef create_shared_actors_df(input_df):\n    \"\"\"\n    Create a DataFrame with information about shared actors for each movie.\n\n    Args:\n        input_df (pd.DataFrame): The input DataFrame containing actor and movie information.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with columns for movie, num_shared_actors, and shared_actors,\n                      sorted in descending order based on the number of shared actors.\n    \"\"\"\n    # Create an empty list to store dictionaries\n    result_data = []\n\n    # Iterate over unique movies in the input DataFrame\n    unique_movies = input_df['movie_or_TV_name'].unique()\n    for movie in unique_movies:\n        # Filter rows for the current movie\n        movie_rows = input_df[input_df['movie_or_TV_name'] == movie]\n\n        # Get unique actors for the current movie\n        unique_actors = movie_rows['actor'].unique()\n\n        # Find the number of shared actors\n        num_shared_actors = len(unique_actors)\n\n        # Append a dictionary to the result list\n        result_data.append({\n            'Movie/TV Show': movie,\n            'Number of Shared Actors': num_shared_actors,\n            'Shared Actors': unique_actors.tolist()\n        })\n\n    # Create a new DataFrame from the list of dictionaries\n    result_df = pd.DataFrame(result_data)\n    # Exclude the original movie since it will automatically have all of the actors as shared actors\n    result_df = result_df[result_df['Movie/TV Show'] != \"Good Will Hunting\"]\n    \n    # Sort the DataFrame based on the number of shared actors in descending order\n    result_df = result_df.sort_values(by='Number of Shared Actors', ascending=False)\n    result_df = result_df.reset_index(drop=True)\n\n    return result_df\n\n\n# Example usage with the provided DataFrame df\nresult_df = create_shared_actors_df(df)\nresult_df.head()\n\n\n\n\n\n\n\n\nMovie/TV Show\nNumber of Shared Actors\nShared Actors\n\n\n\n\n0\nDue South\n6\n[Philip Williams, James Allodi, David Eisner, ...\n\n\n1\nSaturday Night Live\n5\n[Matt Damon, George Plimpton, Casey Affleck, B...\n\n\n2\nMayday\n5\n[Frank Nakashima, Bruce Hunter, Barna Moricz, ...\n\n\n3\nThe Graham Norton Show\n4\n[Matt Damon, Minnie Driver, Ben Affleck, Robin...\n\n\n4\nThe View\n4\n[Matt Damon, Minnie Driver, Ben Affleck, Robin...\n\n\n\n\n\n\n\n\nVisualization:\nTo visualize the number of shared actors between movies, you can use tools like Matplotlib, plotly, or seaborn in Python. The visualize_shared_actors function creates a visual representation of the number of shared actors in the recomended movies and TV shows using a bar chart from Matplotlib. The bar chart generated highlights movies or TV shows where multiple actors have collaborated, indicating potential relationships or thematic similarities between those productions and the original.\nAlso, the function includes a filter (df[‘Number of Shared Actors’] &gt;= 3) to focus on movies or TV shows with a higher number of shared actors because there are over a thousand recomendations gathered by the scraper, many of which have no shared actors.\n\nimport matplotlib.pyplot as plt\n\ndef visualize_shared_actors(df, overlap):\n    \"\"\"\n    Visualize the number of shared actors in movies/TV shows using a bar chart.\n\n    Arguments:\n        df (pd.DataFrame): DataFrame containing columns 'Movie/TV Show' and 'Number of Shared Actors'.\n\n    Displays a bar chart.\n    \"\"\"\n    \n    shared_df = df[df['Number of Shared Actors'] &gt;= overlap]\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(x = shared_df['Movie/TV Show'], height = shared_df['Number of Shared Actors'], color='skyblue')\n    plt.title('Number of Shared Actors in Movies/TV Shows')\n    plt.xlabel('Movie/TV Show')\n    plt.ylabel('Number of Shared Actors')\n    plt.xticks(rotation=45, ha='right')\n\n    # Display the table\n    plt.show()\n\nHere is a visualization of the shared actors recomendation data using the visualize_shared_actors function:\n\n# Visualize shared actors for movies with two or more shared actors\nvisualize_shared_actors(result_df, 3)\n\n\n\n\n\n\n\n\n\n# Visualize shared actors for movies with two or more shared actors\nvisualize_shared_actors(result_df, 4)\n\n\n\n\n\n\n\n\nThe top movie/tv show recomendations for Good Will Hunting are Due South, Mayday, and War of the Worlds. As we can see one issue that could possibly fixed in the future is exclusing talk shows.\n\n\n\nConclusion\nIn this tutorial, we’ve created a web scraper using Scrapy to extract movie information from TMDb in order to create a recomendation system. By understanding the html structure of the website and using Scrapy, we can gather valuable data for building a movie recommendation system. The visualization at the end allows us to analyze and identify actors with significant contributions to multiple movies for the best recomendations. To further enhance the recomendation system, you could limit the number of actors scraped to only the main cast of the show."
  },
  {
    "objectID": "posts/hw5/index.html",
    "href": "posts/hw5/index.html",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "",
    "text": "Image classification is a foundational skill in machine learning, and with the dynamic duo of Keras and TensorFlow, the process becomes relatively easy. In this tutorial, we will focus on training a machine learning algorithms to differentiate between cats and dogs from a dataset provided by tensorflow datasets which contains image of cats and dogs.\nWe will use techniques like data augmentation to enhance the dataset by introducing variations and distortions, helping models learn more robust patterns. Also, we will see how pre-trained models can help us train our own.\nDownload the necessary libraries:\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nfrom keras import utils, datasets, layers, models\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nFirst, we need to split the dataset into a training, validation, and test set. The training set provides the data on which the model learns patterns and relationships. It typically constitutes the majority of the dataset. Then, the model’s performance is fine-tuned using the validation set, which is smaller but can gauge how well the model generalizes to unseen data. This is important to find issues like overfitting and adjusting parameters. Finally, the test set is the final evaluation, assessing the model’s real-world performance on data it has never encountered during training or validation. The size of the test set is similar to the validation set.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, we need to make sure that all of the images are the same shape and size so we resize all of them to the dimensions (150, 150).\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nNext we split the data to be in batches for faster processing time. We also have to be careful not to run this block more than once or else it will mess up the shape of the dataset. I ran it twice and it made the shape (64, 64, 150, 150, 3) for the images and (64, 64) for the labels, which is not what we want.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/hw5/index.html#part-1-introduction",
    "href": "posts/hw5/index.html#part-1-introduction",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "",
    "text": "Image classification is a foundational skill in machine learning, and with the dynamic duo of Keras and TensorFlow, the process becomes relatively easy. In this tutorial, we will focus on training a machine learning algorithms to differentiate between cats and dogs from a dataset provided by tensorflow datasets which contains image of cats and dogs.\nWe will use techniques like data augmentation to enhance the dataset by introducing variations and distortions, helping models learn more robust patterns. Also, we will see how pre-trained models can help us train our own.\nDownload the necessary libraries:\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nfrom keras import utils, datasets, layers, models\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nFirst, we need to split the dataset into a training, validation, and test set. The training set provides the data on which the model learns patterns and relationships. It typically constitutes the majority of the dataset. Then, the model’s performance is fine-tuned using the validation set, which is smaller but can gauge how well the model generalizes to unseen data. This is important to find issues like overfitting and adjusting parameters. Finally, the test set is the final evaluation, assessing the model’s real-world performance on data it has never encountered during training or validation. The size of the test set is similar to the validation set.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, we need to make sure that all of the images are the same shape and size so we resize all of them to the dimensions (150, 150).\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nNext we split the data to be in batches for faster processing time. We also have to be careful not to run this block more than once or else it will mess up the shape of the dataset. I ran it twice and it made the shape (64, 64, 150, 150, 3) for the images and (64, 64) for the labels, which is not what we want.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/hw5/index.html#part-2-visualize-the-dataset",
    "href": "posts/hw5/index.html#part-2-visualize-the-dataset",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 2: Visualize the Dataset",
    "text": "Part 2: Visualize the Dataset\nTo get a better idea of what the images look like we want to create a grid of the images using the function bellow with their associated labels (0 for cat, 1 for dog).\n\nclass_names = { 0: 'Cat', 1: 'Dog'}\n\nTo get an idea of how to visualize a single image we have to call a single image using [ ] brackets as shown bellow. A single batch has a shape (64, 64, 150, 150, 3) with 64 images and 64 labels where each image is of dimension 150 x 150.\n\n# Assuming `dataset` is your TensorFlow dataset with shape (64, 150, 150, 3)\nfor batch in train_ds.take(1):\n    # Select the first image from the first batch\n    single_image = batch[0].numpy().astype(\"uint8\")\n\n# Remove the batch dimension\nsingle_image = single_image[0]\n\n# Display the image\nplt.imshow(single_image)\nplt.show()\n\n\n\n\n\n\n\n\nNext, the visualize_dataset function is designed to provide a visual representation of images from the given dataset. This function is particularly useful when working with image classification datasets, where each image is associated with a specific label as we described above.The function iterates through the first subset of the dataset (assumed to be a batch). It extracts images and labels from the batch, converting images to NumPy arrays and ensuring they are of type “uint8” (unsigned 8-bit integers). The function then loops through the labels to separate images into two categories cats and dogs. Then it collects three images of cats and dogs for the 2x3 visualization. Then, the function iterates through the two categories and creates a plot using matplotlib.\n\ndef visualize_dataset(dataset, class_names):\n  plt.figure(figsize=(15, 8))\n  for subset in dataset.take(1):\n    # Select the first image from the first batch\n    images = batch[0].numpy().astype(\"uint8\")\n    labels = batch[1].numpy()\n\n    i = 0\n    d_count = 0\n    c_count = 0\n    cats = []\n    dogs = []\n    while c_count &lt; 3:\n      if labels [i] == 0:\n        cats.append(images[i])\n        c_count += 1\n      i += 1\n\n    i = 0\n    while d_count &lt; 3:\n      if labels [i] == 1:\n        dogs.append(images[i])\n        d_count += 1\n      i += 1\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, i + 1)\n      plt.imshow(cats[i])\n      plt.title(class_names[0])\n      plt.axis(\"off\")\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, 3 + i + 1)\n      plt.imshow(dogs[i])\n      plt.title(class_names[1])\n      plt.axis(\"off\")\n\n  plt.show()\n\n\nvisualize_dataset(train_ds, class_names)"
  },
  {
    "objectID": "posts/hw5/index.html#part-3-lable-frequencies",
    "href": "posts/hw5/index.html#part-3-lable-frequencies",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 3: Lable Frequencies",
    "text": "Part 3: Lable Frequencies\nNow, we want to check the frequency of dogs and cats in the training dataset in case there are a greater amount of cats than dog or vice versa. If this is the case, then we can compensate for the imbalance to better train the model.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\n# Initialize counters\ncat_count = 0\ndog_count = 0\n# Iterate through each array in the iterator\nfor label_value in labels_iterator:\n    if label_value == 0:\n        cat_count += 1\n    elif label_value == 1:\n        dog_count += 1\n\n# Display results\nprint(\"Number of cat images:\", cat_count)\nprint(\"Number of dog images:\", dog_count)\n\nNumber of cat images: 4637\nNumber of dog images: 4668\n\n\nAs we can see, there are around the same amount of cat and dog images in the training set. Since the number of cat images is higher than the number of dog images, the baseline model would always predict “cat” and achieve an accuracy equal to the proportion of cat images in the dataset. In this specific example, the baseline accuracy would be:\n\\[ \\text{Baseline Accuracy} = \\frac{\\text{Number of Cat Images}}{\\text{Total Number of Images}} = \\frac{4667}{4667 + 4637} = {0.5016...}\\]\nThis calculation would give you the baseline accuracy, and any machine learning model developed for this task should aim to surpass this baseline accuracy to be considered meaningful and effective."
  },
  {
    "objectID": "posts/hw5/index.html#part-4-first-model",
    "href": "posts/hw5/index.html#part-4-first-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 4: First Model",
    "text": "Part 4: First Model\nFirst we will create a simple sequential layer using some of the layers described below. A Sequential Model in Keras is a linear stack of layers that allows you to create models layer by layer in a step-by-step fashion.\n\nConv2D Layer: Convolutional layers perform the convolution operation, applying filters (also known as kernels) to input data to extract specific features.These layers capture local patterns and detect hierarchical features, allowing the model to learn representations from the input images.\nMaxPooling2D Layer: MaxPooling layers downsample the spatial dimensions of the input data by taking the maximum value in each region of the input covered by the pooling window. They reduces the computational complexity of the model while retaining essential features, helping prevent overfitting and improving the model’s robustness.\nFlatten Layer: The Flatten layer is used to flatten the input data, converting it from a multidimensional tensor to a one-dimensional vector.\nDense Layer: Dense layers connect every neuron in one layer to every neuron in the next layer. These layers enable the model to learn complex patterns by combining features learned by previous layers. The final dense layer produces the output for classification.\nDropout Layer: Dropout layers randomly set a fraction of input units to zero during training, helping prevent overfitting by introducing a form of regularization.\n\n\n# Define the model\nmodel1 = models.Sequential([\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer\n    layers.Flatten(),\n\n    # Dense layers\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n\n    # Output layer\n    layers.Dense(10, activation='softmax')\n])\n\nAfter defining the model we need to compile it and the fit it to the training data.\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 15s 55ms/step - loss: 13.6908 - accuracy: 0.5256 - val_loss: 0.6984 - val_accuracy: 0.5830\nEpoch 2/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.7555 - accuracy: 0.5904 - val_loss: 0.7573 - val_accuracy: 0.6058\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.6809 - accuracy: 0.6366 - val_loss: 0.6557 - val_accuracy: 0.6221\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6158 - accuracy: 0.6793 - val_loss: 0.6656 - val_accuracy: 0.6152\nEpoch 5/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.5601 - accuracy: 0.7203 - val_loss: 0.6888 - val_accuracy: 0.6234\nEpoch 6/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5018 - accuracy: 0.7606 - val_loss: 0.7171 - val_accuracy: 0.6152\nEpoch 7/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.4618 - accuracy: 0.7795 - val_loss: 0.7934 - val_accuracy: 0.6204\nEpoch 8/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.4158 - accuracy: 0.8161 - val_loss: 0.8343 - val_accuracy: 0.6187\nEpoch 9/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.4115 - accuracy: 0.8125 - val_loss: 0.8546 - val_accuracy: 0.5985\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3635 - accuracy: 0.8436 - val_loss: 0.9108 - val_accuracy: 0.6062\nEpoch 11/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.3111 - accuracy: 0.8706 - val_loss: 0.9991 - val_accuracy: 0.6169\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2890 - accuracy: 0.8750 - val_loss: 1.1080 - val_accuracy: 0.5851\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.2342 - accuracy: 0.9079 - val_loss: 1.1989 - val_accuracy: 0.5967\nEpoch 14/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2323 - accuracy: 0.9132 - val_loss: 1.1892 - val_accuracy: 0.5989\nEpoch 15/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2011 - accuracy: 0.9220 - val_loss: 1.3048 - val_accuracy: 0.5985\nEpoch 16/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.2209 - accuracy: 0.9180 - val_loss: 1.4371 - val_accuracy: 0.5778\nEpoch 17/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1949 - accuracy: 0.9263 - val_loss: 1.5856 - val_accuracy: 0.5881\nEpoch 18/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.1635 - accuracy: 0.9418 - val_loss: 1.7272 - val_accuracy: 0.5993\nEpoch 19/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1665 - accuracy: 0.9387 - val_loss: 1.6080 - val_accuracy: 0.5899\nEpoch 20/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1677 - accuracy: 0.9397 - val_loss: 1.5196 - val_accuracy: 0.5997\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n\n\nBy plotting the training and validation accuracy, we can better visualize the performance of the model and possible overfitting issues.\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy Observation: The accuracy of my model fluctuated during training, reaching around 62% at its peak.\nComparison to Baseline: I achieved a validation accuracy slightly better than the baseline of 50%. While there is an improvement, further optimization can be explored.\nOverfitting Observation: There are signs of overfitting as the training accuracy (around 92.5%) is substantially higher than the validation accuracy (around 59.7%). To mitigate overfitting, additional techniques such as increasing dropout rates, reducing model complexity, or incorporating regularization methods could be explored. Regularization methods like L2 regularization or data augmentation might be useful in this scenario."
  },
  {
    "objectID": "posts/hw5/index.html#part-4-second-model",
    "href": "posts/hw5/index.html#part-4-second-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 4: Second Model",
    "text": "Part 4: Second Model\nAs we saw in part 4, a sequential model as defined was not much more accurate than the baseline. In order to improve the accuracy we can include some data augmentation layers. Data augmentation is when we include modified copies of the same image in the training set; for example, the image rotated or mirrored.\n\ndef augmented(train_ds, data_augmentation):\n  # Display original and augmented images\n  plt.figure(figsize=(10, 10))\n  for image, _ in train_ds.take(1):\n      original_image = image[0]\n      ax = plt.subplot(2, 2, 1)\n      plt.imshow(original_image / 255)\n      plt.axis('off')\n      ax.set_title('Original Image')\n\n  # Display augmented images\n  for i in range(2, 5):\n      ax = plt.subplot(2, 2, i)\n      augmented_image = data_augmentation(tf.expand_dims(original_image, 0))\n      plt.imshow(augmented_image[0] / 255)\n      plt.axis('off')\n      ax.set_title(f'Augmented Image {i - 1}')\n\n  plt.show()\n\nFirst let’s take a look at the images produced by Random Flip:\n\ndata_augmentation1 = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n])\naugmented(train_ds, data_augmentation1)\n\n\n\n\n\n\n\n\nThen at the images produced by RandomRotation:\n\ndata_augmentation2 = tf.keras.Sequential([\n  tf.keras.layers.RandomRotation(0.2),\n])\naugmented(train_ds, data_augmentation2)\n\n\n\n\n\n\n\n\nAs we can see these layers in combination will add some diversity to the dataset, so let’s create new model similar to our first that include these layers and see if it performs better.\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    # Other layers (similar to model1)\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow compile and fit the model to the training data:\n\n# Compile the model\nmodel2.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Assuming you have train_ds and validation datasets\nhistory_model2 = model2.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 37ms/step - loss: 30.6067 - accuracy: 0.4996 - val_loss: 0.8345 - val_accuracy: 0.5482\nEpoch 2/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.8585 - accuracy: 0.5192 - val_loss: 0.8028 - val_accuracy: 0.5623\nEpoch 3/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7927 - accuracy: 0.5352 - val_loss: 0.7837 - val_accuracy: 0.5374\nEpoch 4/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.7638 - accuracy: 0.5507 - val_loss: 0.7010 - val_accuracy: 0.5727\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7311 - accuracy: 0.5596 - val_loss: 2.0641 - val_accuracy: 0.5009\nEpoch 6/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.7474 - accuracy: 0.5458 - val_loss: 0.6861 - val_accuracy: 0.5718\nEpoch 7/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7102 - accuracy: 0.5581 - val_loss: 0.6791 - val_accuracy: 0.5770\nEpoch 8/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.7023 - accuracy: 0.5687 - val_loss: 0.6731 - val_accuracy: 0.5929\nEpoch 9/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6982 - accuracy: 0.5697 - val_loss: 0.6600 - val_accuracy: 0.6109\nEpoch 10/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6894 - accuracy: 0.5846 - val_loss: 0.6720 - val_accuracy: 0.5864\nEpoch 11/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6887 - accuracy: 0.5843 - val_loss: 0.6677 - val_accuracy: 0.5993\nEpoch 12/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6879 - accuracy: 0.5802 - val_loss: 0.6554 - val_accuracy: 0.6294\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6768 - accuracy: 0.6038 - val_loss: 0.6553 - val_accuracy: 0.6234\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6799 - accuracy: 0.6000 - val_loss: 0.6595 - val_accuracy: 0.6088\nEpoch 15/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.6719 - accuracy: 0.5997 - val_loss: 0.6608 - val_accuracy: 0.6156\nEpoch 16/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6723 - accuracy: 0.6020 - val_loss: 0.6576 - val_accuracy: 0.6204\nEpoch 17/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6669 - accuracy: 0.6161 - val_loss: 0.6610 - val_accuracy: 0.6195\nEpoch 18/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6622 - accuracy: 0.6226 - val_loss: 0.6590 - val_accuracy: 0.6113\nEpoch 19/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6629 - accuracy: 0.6169 - val_loss: 0.6552 - val_accuracy: 0.6161\nEpoch 20/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6681 - accuracy: 0.6119 - val_loss: 0.6489 - val_accuracy: 0.6320\n\n\n\n# Plot the accuracy history\nplt.plot(history_model2.history['accuracy'], label='Training Accuracy')\nplt.plot(history_model2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy Observation: The accuracy of model2 fluctuated during training, reaching around 67% at its peak.\nComparison to Baseline and Model1: The model2 achieved a validation accuracy of approximately 67%, which is an improvement compared to the baseline of 50%. Model2’s validation accuracy (67%) is also higher than that of Model1 (approximately 60%). This indicates that the inclusion of data augmentation layers, such as RandomFlip and RandomRotation, in Model2 has contributed to better generalization, resulting in improved performance on the validation set.\nUnderfitting Observation: The training accuracy is around 62%, and the validation accuracy is around 67%. This suggests that the model may not have fully learned the patterns in the training data, and further adjustments to the model complexity or training parameters may be considered to address underfitting."
  },
  {
    "objectID": "posts/hw5/index.html#part-5-third-model",
    "href": "posts/hw5/index.html#part-5-third-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 5: Third Model",
    "text": "Part 5: Third Model\nThe original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. If we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nNow, let’s create our third model including the preprocessor defined above and the augmentation layers from Model2:\n\nmodel3 = models.Sequential([\n    preprocessor,\n\n    # augmentation layers\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    layers.Conv2D(64, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(256, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(512, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='LeakyReLU'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Assuming you have train_ds and validation datasets\nhistory_model3 = model3.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 23s 126ms/step - loss: 0.7978 - accuracy: 0.5451 - val_loss: 0.6467 - val_accuracy: 0.6440\nEpoch 2/20\n146/146 [==============================] - 17s 117ms/step - loss: 0.6438 - accuracy: 0.6317 - val_loss: 0.5906 - val_accuracy: 0.7094\nEpoch 3/20\n146/146 [==============================] - 17s 117ms/step - loss: 0.6120 - accuracy: 0.6680 - val_loss: 0.6085 - val_accuracy: 0.6913\nEpoch 4/20\n146/146 [==============================] - 17s 119ms/step - loss: 0.5734 - accuracy: 0.7011 - val_loss: 0.5141 - val_accuracy: 0.7558\nEpoch 5/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.5674 - accuracy: 0.7088 - val_loss: 0.5014 - val_accuracy: 0.7640\nEpoch 6/20\n146/146 [==============================] - 18s 122ms/step - loss: 0.5259 - accuracy: 0.7366 - val_loss: 0.4718 - val_accuracy: 0.7777\nEpoch 7/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5026 - accuracy: 0.7507 - val_loss: 0.4399 - val_accuracy: 0.8018\nEpoch 8/20\n146/146 [==============================] - 19s 128ms/step - loss: 0.4816 - accuracy: 0.7726 - val_loss: 0.4284 - val_accuracy: 0.8113\nEpoch 9/20\n146/146 [==============================] - 17s 120ms/step - loss: 0.4671 - accuracy: 0.7818 - val_loss: 0.4052 - val_accuracy: 0.8216\nEpoch 10/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4399 - accuracy: 0.7942 - val_loss: 0.3989 - val_accuracy: 0.8272\nEpoch 11/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4157 - accuracy: 0.8096 - val_loss: 0.3489 - val_accuracy: 0.8439\nEpoch 12/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3912 - accuracy: 0.8249 - val_loss: 0.3832 - val_accuracy: 0.8396\nEpoch 13/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3876 - accuracy: 0.8265 - val_loss: 0.3481 - val_accuracy: 0.8487\nEpoch 14/20\n146/146 [==============================] - 17s 120ms/step - loss: 0.3707 - accuracy: 0.8314 - val_loss: 0.3360 - val_accuracy: 0.8543\nEpoch 15/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3362 - accuracy: 0.8538 - val_loss: 0.3201 - val_accuracy: 0.8629\nEpoch 16/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3464 - accuracy: 0.8493 - val_loss: 0.3033 - val_accuracy: 0.8775\nEpoch 17/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3349 - accuracy: 0.8557 - val_loss: 0.3154 - val_accuracy: 0.8672\nEpoch 18/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3177 - accuracy: 0.8585 - val_loss: 0.3163 - val_accuracy: 0.8667\nEpoch 19/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.2919 - accuracy: 0.8749 - val_loss: 0.3044 - val_accuracy: 0.8835\nEpoch 20/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3015 - accuracy: 0.8692 - val_loss: 0.2917 - val_accuracy: 0.8809\n\n\n\nplt.plot(history_model3.history['accuracy'], label='Training Accuracy')\nplt.plot(history_model3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 3')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy: The validation accuracy of model3 during training is 88.09%.\nComparison to Model1: Model3’s validation accuracy is significantly higher than the accuracy achieved with model1, which had a validation accuracy of 68.2%. This indicates a notable improvement in model performance.\nOverfitting Observation: While there is still a slight gap between training and validation accuracy, it is significantly reduced compared to model1. Model3 demonstrates improved generalization to unseen data, suggesting reduced overfitting."
  },
  {
    "objectID": "posts/hw5/index.html#part-6-fourth-model",
    "href": "posts/hw5/index.html#part-6-fourth-model",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Part 6: Fourth Model",
    "text": "Part 6: Fourth Model\nMany people have attempted to classify images in the past and have worked out models for similar situations, so we can also test these on our model. First, we need to access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model. The preexisting model we will use is MobileNetV3Large and configure it as a layer in our model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=(150,150,3),\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nNow, let’s define our fourth model with the augmentation layers from before but using the new base_model_layer and a few simple layers.\n\nmodel4 = models.Sequential([\n    # augmentation layers\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(2, activation='softmax'),  # outputs the final classification\n])\n\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\nAfter compiling the model we can view it’s details using the summary function.\n\nmodel4.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_5 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_5 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_2 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d (Glob  (None, 960)               0         \n alMaxPooling2D)                                                 \n                                                                 \n dropout_14 (Dropout)        (None, 960)               0         \n                                                                 \n dense_21 (Dense)            (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nhistory_model4 = model4.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 71ms/step - loss: 1.4050 - accuracy: 0.8559 - val_loss: 0.4345 - val_accuracy: 0.9493\nEpoch 2/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.7502 - accuracy: 0.9138 - val_loss: 0.2115 - val_accuracy: 0.9721\nEpoch 3/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.5701 - accuracy: 0.9249 - val_loss: 0.2074 - val_accuracy: 0.9690\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5094 - accuracy: 0.9303 - val_loss: 0.1844 - val_accuracy: 0.9716\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.4405 - accuracy: 0.9309 - val_loss: 0.2069 - val_accuracy: 0.9695\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4494 - accuracy: 0.9321 - val_loss: 0.1849 - val_accuracy: 0.9699\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3517 - accuracy: 0.9369 - val_loss: 0.1731 - val_accuracy: 0.9716\nEpoch 8/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3356 - accuracy: 0.9343 - val_loss: 0.1214 - val_accuracy: 0.9712\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3250 - accuracy: 0.9335 - val_loss: 0.1176 - val_accuracy: 0.9725\nEpoch 10/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2862 - accuracy: 0.9379 - val_loss: 0.2065 - val_accuracy: 0.9544\nEpoch 11/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2800 - accuracy: 0.9391 - val_loss: 0.1199 - val_accuracy: 0.9690\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3356 - accuracy: 0.9324 - val_loss: 0.1601 - val_accuracy: 0.9682\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2712 - accuracy: 0.9351 - val_loss: 0.1385 - val_accuracy: 0.9660\nEpoch 14/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.2485 - accuracy: 0.9364 - val_loss: 0.1757 - val_accuracy: 0.9553\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2672 - accuracy: 0.9342 - val_loss: 0.1276 - val_accuracy: 0.9673\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2988 - accuracy: 0.9322 - val_loss: 0.1017 - val_accuracy: 0.9733\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2871 - accuracy: 0.9320 - val_loss: 0.1249 - val_accuracy: 0.9733\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2589 - accuracy: 0.9346 - val_loss: 0.1283 - val_accuracy: 0.9656\nEpoch 19/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3059 - accuracy: 0.9283 - val_loss: 0.1895 - val_accuracy: 0.9617\nEpoch 20/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2999 - accuracy: 0.9335 - val_loss: 0.1299 - val_accuracy: 0.9725\n\n\n\nplt.plot(history_model4.history['accuracy'], label='Training Accuracy')\nplt.plot(history_model4.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 4')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nValidation Accuracy of Model4 During Training: The validation accuracy of model4 during training consistently exceeded 97%.\nComparison to Model1: Model4’s validation accuracy is significantly higher than the accuracy achieved with model1, which struggled to surpass 80%. Model4’s superior performance suggests that leveraging a pre-existing model (MobileNetV3Large) and fine-tuning it for the specific task can lead to better results.\nOverfitting in Model4: Overfitting appears to be minimal in model4, as the validation accuracy closely tracks the training accuracy. The incorporation of a pre-trained base model likely contributed to this improved generalization."
  },
  {
    "objectID": "posts/hw5/index.html#conclusion",
    "href": "posts/hw5/index.html#conclusion",
    "title": "Image Classification using Tensorflow and Keras: Cats vs. Dogs",
    "section": "Conclusion",
    "text": "Conclusion\nAs we can see, model4 is the most accurate model so we will test it on the test data for our final step:\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n37/37 [==============================] - 3s 77ms/step - loss: 0.1467 - accuracy: 0.9652\nTest Accuracy: 96.52%\n\n\nEach model brought its own set of techniques and improvements, showcasing the versatility of deep learning in computer vision tasks. The final model has an accuracy of 96.5%.\nIn conclusion, these models emphasized the significance of data preprocessing, architectural choices, and the influence of transfer learning. The flexibility of Keras and TensorFlow allowed us to experiment, iterate, and build a good image classification model."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]