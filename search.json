[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog!",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome!\n\nThis is a photo I took in Kauai, Hawaii this past summer."
  },
  {
    "objectID": "posts/hw1/index .html",
    "href": "posts/hw1/index .html",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "",
    "text": "Structured Query Language, or SQL for short, is a programming language designed for managing and manipulating relational databases. In a relational database, data is stored in tables, which consist of rows and columns. The relationships between tables are defined by using keys, such as primary keys and foreign keys. A primary key is a unique identifier for each record in a table. It ensures that each row can be uniquely identified and allows for the creation of relationships between tables. A foreign key is a field in one table that refers to the primary key in another table. SQL establishes relationships between tables, allowing data to be linked across tables using these keys. It provides a way for users to interact with databases, to retrieve and manipulate data. Users retrieve data from databases by writing queries which specify conditions to filter and sort the data.\n\n\n\nUsing databases can be more efficient than working only with Pandas DataFrames because databases offer long-term storange and handle large datasets more efficiently.\n\n# import necessary libraries\nimport pandas as pd\nimport sqlite3\n\n\nconn = sqlite3.connect(\"climate.db\") # create a database in current directory called climate.db\n\n\n# create a dataframe iterator for the temperature data \ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf_iter\n\n&lt;pandas.io.parsers.readers.TextFileReader at 0x2c15312c590&gt;\n\n\nUsing pd.read_csv() with the chunksize parameter is used when dealing with large datasets that may not fit into memory. It allows you to read and process the data in smaller, manageable chunks rather than loading the entire dataset at once.\ndf_iter returns an iterator, enabling you to process the data iteratively. You can perform operations on each chunk without loading the entire dataset into memory. df_iter is initialized with pd.read_csv(), specifying the file path and chunksize. It returns an iterator object where each element corresponds to a chunk of data with the specified number of rows (chunksize).\nTo access each chunk, you can iterate over df_iter using the next() magic method. The actual data is read and processed on-the-fly during the iteration which allows you to work with one chunk at a time.\n\ndf = df_iter.__next__()\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ndf.shape\n\n(100000, 14)\n\n\nThe prepare_df function below reorganizes the structure of the DataFrame (df) and performs some data manipulation steps so that each temperature reading has its own row with the corresponding station, month, and year. The transformed DataFrame has a new structure with the columns “ID,” “Year,” “Month,” and “Temp.”\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf = prepare_df(df)\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\nThis next portion is a loop that iterates over chunks of a DataFrame iterator (df_iter). It transforms each chunk by using the prepare_df function and then writes the resulting DataFrame to a SQL table named “temperatures.” The following blocks of code use the station-metadata.csv and the fips-country-codes.csv files to create the stations and countries tables in the same SQL database.\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\n\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    \nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\nstations = pd.read_csv(\"fips-10-4-to-iso-country-codes.csv\")\nstations.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nHere we can see the results of the previous commands and can see all of the different tables and their categories in the database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\nconn.commit()  #commit() is a method used to finalize and confirm any \n               #        changes made during a series of database operations within a transaction.\nconn.close()   #close() is a method used to terminate the connection to the database"
  },
  {
    "objectID": "posts/hw1/index .html#visualization-1-average-temperature-over-time",
    "href": "posts/hw1/index .html#visualization-1-average-temperature-over-time",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 1: Average Temperature Over Time",
    "text": "Visualization 1: Average Temperature Over Time\nThe average_temperature_line_plot function creates a line plot using Plotly to visualize the average temperature over time for a specific country within a given year range. The function queries db_file using the avg_temp_years_query function to retrieve temperature data for a specified country and time range and calculates the average temperature for each year. Then it creates a line graph using the plotly method line.\n\nfrom climate_database import avg_temp_years_query\nprint(inspect.getsource(avg_temp_years_query))\n\ndef avg_temp_years_query(db_file, country, year_begin, year_end):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n        SELECT t.Year, AVG(t.Temp) AS avg_temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.ID\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY year\n    \"\"\"\n    \n    # Execute the query and fetch the results into a Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    \n    return df\n\n\n\n\nfrom climate_database import average_temperature_line_plot\nprint(inspect.getsource(average_temperature_line_plot))\n\ndef average_temperature_line_plot(db_file, country, year_begin, year_end):\n    # Get data using the query function\n    data = avg_temp_years_query(db_file, country, year_begin, year_end)\n    \n    data['avg_temp'] = data['avg_temp'].round(2)\n\n    # Create line plot with facets using Plotly Express\n    fig = px.line(data, \n                  x='Year', \n                  y='avg_temp',\n                  labels={'avg_temp': 'Average Temperature (°C)'},\n                  title=f'Average Temperature Over Time in {country} ({year_begin}-{year_end})')\n\n    return fig\n\n\n\nThe line plot allows users to observe trends and fluctuations in the average temperature over time. It provides insights into the climate patterns of the selected country during the specified period. Here is an example of the increase in temperature over the years in Thailand. This type of visualization could be useful for observing climate change and how it affects different countries.\n\nfig_line = average_temperature_line_plot(\"climate.db\", \"Thailand\", 1980, 2020)\nfig_line.show()"
  },
  {
    "objectID": "posts/hw1/index .html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "href": "posts/hw1/index .html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 2: Bar Chart of Average Temperatures Each Month",
    "text": "Visualization 2: Bar Chart of Average Temperatures Each Month\nThe average_monthly_temperature_bar_chart created the average monthly temparute for a given month throughout a range of years. In order to create this bar chart we must first find the global average temperature in order to range the colors of the bars correctly.\nThe global_average_temperature function simplifies the process of retrieving and calculating the average temperature across all records in the “temperatures” table of an SQLite database\n\nfrom climate_database import global_average_temperature\nprint(inspect.getsource(global_average_temperature))\n\ndef global_average_temperature(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # SQL query to calculate the global average temperature\n    query = \"\"\"\n        SELECT AVG(Temp) AS global_avg_temp\n        FROM temperatures\n    \"\"\"\n\n    # Execute the query and fetch the result\n    result = conn.execute(query).fetchone()\n    \n    # Close the database connection\n    conn.close()\n\n    # Extract the global average temperature from the result\n    global_avg_temp = result[0]\n\n    return global_avg_temp\n\n\n\nThen we can query the database and create a bar chart with avg_monthly_temperature_bar_chart for the average temoerature of each month in a given range of years.\nFirst, the function connectes to the SQLite database specified by db_file. Then, it uses an SQL query to retrieve the average temperature for each month within the specified country and year range. The query is executed using the pd.read_sql_query method, which putts the data into a DataFrame (df). Next, the month numbers are mapped to the corresponding month names. The function calls the global_average_temperature function to calculate the global average temperature from the entire dataset, so that it can center the color scheme correctly with, with white being the average, red being hotter temperatures, and blue being cooler temperstures.Then it uses the plotly bar function to create a bar chart.\n\nfrom climate_database import avg_monthly_temperature_bar_chart\nprint(inspect.getsource(avg_monthly_temperature_bar_chart))\n\ndef avg_monthly_temperature_bar_chart(db_file, country, year_begin, year_end):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    query = f\"\"\"\n        SELECT t.Month, AVG(t.Temp) AS avg_temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.ID\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY t.Month\n    \"\"\"\n\n    # Execute the query and fetch the results into a Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    df['avg_temp'] = df['avg_temp'].round(2)\n\n    # Map month numbers to month names\n    month_mapping = {\n        1: 'January', 2: 'February', 3: 'March', 4: 'April',\n        5: 'May', 6: 'June', 7: 'July', 8: 'August',\n        9: 'September', 10: 'October', 11: 'November', 12: 'December'\n    }\n    df['Month'] = df['Month'].map(month_mapping)\n\n    \n    # Calculate a global maximum absolute value for centering and scaling the color scale\n    global_avg = global_average_temperature(db_file)\n    \n    # Create a bar chart using Plotly method bar\n    fig = px.bar(df, \n                 x='Month', \n                 y='avg_temp',\n                 labels={'avg_temp': 'Average Temperature (°C)'},\n                 title=f'Average Monthly Temperature in {country} ({year_begin}-{year_end})',\n                 color='avg_temp',\n                 color_continuous_scale='RdBu_r',\n                 color_continuous_midpoint=global_avg,\n                 range_color=[global_avg + 30, global_avg-30])\n\n    return fig\n\n\n\nIn summary, this function connects to our SQLite database, retrieves average monthly temperature data for a specified country and year range, and creates an interactive bar chart using Plotly, where the color of each bar represents the temperature value. The resulting visualization provides an representation of how average monthly temperatures vary over time for the selected country.\nThis visualization could help answer the question “How does the average monthly temperature vary over the specified years for a particular country?” This visualization provides insights into the seasonal temperature changes in different countries over certain years. By examining the color of the bars, users can identify periods of higher or lower average temperatures throughout the specified time range.\nBelow is an example for the function in use to find the average temperatures of each month in Finland between 2000 and 2020:\n\nfig = avg_monthly_temperature_bar_chart('climate.db', 'Finland', 2000, 2020)\nfig.show()\n\n                                                \n\n\n\nfig = avg_monthly_temperature_bar_chart('climate.db', 'Egypt', 2000, 2020)\nfig.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial: Constructing a Data Visualization of the Palmer Penguins Dataset\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "posts/hw0/index.html",
    "href": "posts/hw0/index.html",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "",
    "text": "Data visualization is an important skill for understanding and drawing insight from datasets. The Palmer Penguins dataset contains data on 344 penguins of three different species within the Palmer Archipelago in Antarctica. In this tutorial, we will explore how to create an interesting data visualization pf the Palmer Penguins dataset using Python."
  },
  {
    "objectID": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "href": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 1: Import Libraries and Load the Dataset",
    "text": "Step 1: Import Libraries and Load the Dataset\nWe want to import the pandas library in order to create a Dataframe class object containing all of the information from the palmer_penguins.csv file that we are downloading directly from the given url. Then, we need matplotlib in order to create our visualization. Other libraries can be used for visualizing the data besides matplotlib, but this is the most well known.\n\nimport pandas as pd # Contains the class DataFrame\nimport matplotlib.pyplot as plt # For creating simple graphs\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nA csv file is a file formated to contain data that is seperated by commas for the different collumns. We can easily create a pandas DataFrame object by reading in a csv file of the Palmer Penguins dataset from the url given above.\nAdditionally we want to add names for the columns:\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]"
  },
  {
    "objectID": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "href": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 2: Explore and Clean the Data",
    "text": "Step 2: Explore and Clean the Data\nBefore creating a visualization, it’s essential to understand the structure and contents of the dataset. First, we want to simply take a look at the database we have created in order to determine if any rows or columns need to be cleaned.\n\npenguins.head() #Shows the first five rows of the dataframe\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n3\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n\n\n\n\n\nThen, we need to clean out any empty datapoints using the dropna method, and we want to make the dataframe easier to read by simplifying the species name.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\nAdditionally, often times data sets contain columns that are not relevant to our study or visualization so they can be deleted from the dataframe for simplicity. In this case, we will keep all of the columns for the different examples given below."
  },
  {
    "objectID": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "href": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 3: Choose an Appropriate Method of Visualization",
    "text": "Step 3: Choose an Appropriate Method of Visualization\nFor the visualization, there are many different options. Your visualization should be based off of what you are trying to analyze in the data. For example, let’s say that I want to compare the size of the culmen of the three different species. For this, I will be using a scatter plot to visualize the clustering of the three groups.\nFirst, I will create a dictionary of the species and colors they should be associated with. Then, I will create a scatter plot for each species. Also, I want to add some additional features to the scatter plot, axis labels, and a title. This makes the graph easier to read and visually appealing.\n\n# library with associated species and colors\ncolors = {'Adelie': 'blue', 'Chinstrap': 'orange', 'Gentoo': 'green'}\n\n# Create and individual scatter plot for each species\nfor species, col in colors.items():\n    species_data = penguins[penguins['Species'] == species]\n    plt.scatter(species_data['Culmen Length (mm)'], species_data['Culmen Depth (mm)'], label=species, color=col)\n\n# Add aditional features to the plot\nplt.legend(title='Species')\nplt.title('Culmen Size of Penguin Species')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, I could also choose to compare the different body masses of each species using a box plot. First, I will need to import a library that could help me do this, like plotly. Then I can use the box method to create a box plot with the Species as the x-axis and the body mass as the y-axis.\n\n# import the relevant library\nimport plotly.express as px\n\n# Create the box plot and add a title\nbox = px.box(penguins, x = 'Species', y = 'Body Mass (g)', color = 'Species')\nbox.update_layout(title_text=\"Body Mass Distribution of Penguin Species\")\n\n# Show the plot\nbox.show()"
  },
  {
    "objectID": "posts/hw0/index.html#conclusion",
    "href": "posts/hw0/index.html#conclusion",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, you can make many different types of interesting visualization from this dataset that can help you draw different insight. Choosing the right visualization depends on what you’re trying to find, and there are many different libraries and graphs that can help you with that. Once your visualization is created, interpret the results and analyze any patterns or trends observed from the visualization. Feel free to experiment with other types of visualizations and variables from the dataset to create a comprehensive understanding of the data. Remember to share your findings and insights in a clear and concise manner by making your visualizations easy to read and visibly satisfying. Data visualization is not only about creating beautiful plots but also about effectively communicating information."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]