[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog!",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome!\n\nThis is a photo I took in Kauai, Hawaii this past summer."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "Homework 2: Web Scraping Movie Recomendations using Scrapy",
    "section": "",
    "text": "Introduction:\nIn this blog post, I will walk you through the process of creating an web scraper designed to explore movie recomendations based on your favorite movie. This python web scraper collects movie recomendations by extracting information about movie casts from TMDB. The goal is to extract data on actors, movies, and their relationships to build a simple movie recommendation system based on shared actors.\nFirst we must set up the project files in the terminal using:\n    conda activate PIC16B-24W\n    scrapy startproject TMDB_scraper\n    cd TMDB_scraper\nNow we must create a tmdb_spider.py file in the spiders file for our scraper class.\n\n\nUnderstanding the Scraper\nThe web scraper is designed to collect data from The Movie Database (TMDb). It is designed as a class called TmdbSpider in the tmbd_spider.py file containing four methods: init, parse, parse_full_credits, parse_actor_page. To scrape the necessary information from each page we have to use css commands to extract information from the html of the given url.\nparse() This method initiates the scraping process by navigating to the Full Cast & Crew page of a movie. It then calls the parse_full_credits() method to extract actor information.\n\n\n    def parse(self, response):\n        \"\"\"\n        Navigate to the Full Cast & Crew page and call parse_full_credits.\n\n        Args:\n            response (scrapy.http.Response): The response object for the movie page.\n\n        Yields:\n            scrapy.Request: Request to the Full Cast & Crew page.\n        \"\"\"\n        cast_url = f\"{response.url}/cast\"\n        yield scrapy.Request(cast_url, callback=self.parse_full_credits)\n\n\n\nparse_full_credits() This method parses the Full Credits page for the given movie, extracting actor names and their profile page URLs. It yields requests to scrape each actor’s page using the parse_actor_page() method while passing along the actor name. This method scrapes information specifically from the left panel of the page because we want to only include actors and exclude crew, producers, etc.\n\n\n    def parse_full_credits(self, response):\n        \n        \"\"\"\n        Parse the Full Credits page for a movie, extract actor names and their respective profile page URLs,\n        and yield requests to scrape each actor's page.\n\n        Args:\n            response (scrapy.http.Response): The response object for the Full Credits page.\n\n        Yields:\n            scrapy.Request: Request to the actor's page.\n        \"\"\"\n        # selects the lines containing the actor url's and their names\n        actor_selectors = response.css('div.content_wrapper.false section.panel.pad:first-child div.info a[href^=\"/person/\"]')\n        \n        # Loops through the list of actor information\n        for actor_selector in actor_selectors:\n            \n            # Extracts actor names and actor url\n            actor_name = actor_selector.css('::text').extract_first()\n            actor_page_url = urljoin(response.url, actor_selector.css('::attr(href)').extract_first())\n\n            # Debug information\n            #self.logger.info(f'Actor Name: {actor_name}, Actor Page URL Relative: {actor_page_url_relative}')\n\n            # Debug information\n            #self.logger.info(f'Actor Page URL: {actor_page_url}')\n\n            yield scrapy.Request(actor_page_url, callback=self.parse_actor_page, meta={'actor_name': actor_name})\n\n\n\nparse_actor_page() This method parses an actor’s profile page, extracting movie titles and their URLs. It yields information about the actor and the associated movie or TV show. This method scrapes information specifically from the Actor table of the page because we want to only include acting roles and exclude any other roles.\n\n\n    def parse_actor_page(self, response):\n        \n        \"\"\"\n        Parse an actor's profile page, extract movie titles and their respective URLs,\n        and yield information about the actor and the associated movie.\n\n        Args:\n            response (scrapy.http.Response): The response object for the actor's profile page.\n\n        Yields:\n            dict: Information about the actor and the associated movie.\n        \"\"\"\n        \n        actor_name = response.meta.get('actor_name')\n\n        # Extracting movies from lines like '&lt;a class=\"tooltip\" href=\"/movie/1236045\"&gt;\n        #                                      &lt;bdi&gt;Animals&lt;/bdi&gt;'\n        #                                    &lt;/a&gt;\n        h3_cat = response.css('div.credits_list')[0].css('h3::text').getall()\n        num = h3_cat.index(\"Acting\")\n        movie_selectors = response.css('div.credits_list')[0].css('table.card.credits')[num].css('a.tooltip')\n\n        for movie_selector in movie_selectors:\n            # Extract movie title\n            movie_title = movie_selector.css('bdi::text').extract_first().strip()\n             # Extracting movie URL\n            movie_url_relative = movie_selector.css('::attr(href)').extract_first()\n\n            # Join the relative URL with the base URL of the current page\n            movie_url = response.urljoin(movie_url_relative)\n            \n            '''\n            if movie_url + '-' + convert_to_slug(movie_title) == self.start_urls:\n                self.logger.info(f'Skipping movie: {movie_title} (same as original)')\n                continue\n            '''\n\n            # Debug information\n            #self.logger.info(f'Movie Title: {movie_title}')\n            #self.logger.info(f'Movie URL: {movie_url}-{convert_to_slug(movie_title)}')\n            \n            if movie_title and movie_url:\n                yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_title}\n            else:\n                # If movie_title or movie_url is not present, log a warning\n                self.logger.warning('Movie title or URL not found in the selector')\n\n\n\nTo run the scraper, use the following command in the terminal:  scrapy crawl tmdb_spider -o results.csv -a subdir=489-good-will-hunting  Where we replace the subdir parameter with the movie’s subdir you want to start the scraping process from. You can find the subdir by checking the url of the TMBD page for the movie you want recomendations from.\nNow we are returned a results.csv file containing the recomendations gathered from the parsing of the page.\n\n\nResults and Analysis\nFrom the results.csv we want to extract the recomended movies. Logically, the best recomendations will be those with the most number of shared actors from the original movie. We can see below that the csv has two columns, actor, and movie_or_TV_name, so we want to reorganize the dataframe so that it combines repetitions of the same movie and counts the number of shared actors.\n\n# import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv(\"goodwillhunting_recs.csv\")\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPatrick O'Donnell\n2 By 4\n\n\n1\nPatrick O'Donnell\nGood Will Hunting\n\n\n2\nScott William Winters\nA Little Dream\n\n\n3\nScott William Winters\nNarcos: Mexico\n\n\n4\nScott William Winters\nBeautifully Broken\n\n\n...\n...\n...\n\n\n1500\nRobin Williams\nThe Tonight Show Starring Johnny Carson\n\n\n1501\nRobin Williams\nThe Grammy Awards\n\n\n1502\nRobin Williams\nTony Awards\n\n\n1503\nRobin Williams\nThe Oscars\n\n\n1504\nRobin Williams\nToday\n\n\n\n\n1505 rows × 2 columns\n\n\n\n\nprint(df.shape)\n\n(1505, 2)\n\n\nIn order to reorganize the dataframe I have created a function called create_shared_actors_df. It takes the DataFrame obtained from the scraper as input and generates a new DataFrame with information about shared actors for each movie. The resulting DataFrame is sorted in descending order based on the number of shared actors.\n\ndef create_shared_actors_df(input_df):\n    \"\"\"\n    Create a DataFrame with information about shared actors for each movie.\n\n    Args:\n        input_df (pd.DataFrame): The input DataFrame containing actor and movie information.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with columns for movie, num_shared_actors, and shared_actors,\n                      sorted in descending order based on the number of shared actors.\n    \"\"\"\n    # Create an empty list to store dictionaries\n    result_data = []\n\n    # Iterate over unique movies in the input DataFrame\n    unique_movies = input_df['movie_or_TV_name'].unique()\n    for movie in unique_movies:\n        # Filter rows for the current movie\n        movie_rows = input_df[input_df['movie_or_TV_name'] == movie]\n\n        # Get unique actors for the current movie\n        unique_actors = movie_rows['actor'].unique()\n\n        # Find the number of shared actors\n        num_shared_actors = len(unique_actors)\n\n        # Append a dictionary to the result list\n        result_data.append({\n            'Movie/TV Show': movie,\n            'Number of Shared Actors': num_shared_actors,\n            'Shared Actors': unique_actors.tolist()\n        })\n\n    # Create a new DataFrame from the list of dictionaries\n    result_df = pd.DataFrame(result_data)\n    # Exclude the original movie since it will automatically have all of the actors as shared actors\n    result_df = result_df[result_df['Movie/TV Show'] != \"Good Will Hunting\"]\n    \n    # Sort the DataFrame based on the number of shared actors in descending order\n    result_df = result_df.sort_values(by='Number of Shared Actors', ascending=False)\n    result_df = result_df.reset_index(drop=True)\n\n    return result_df\n\n\n# Example usage with the provided DataFrame df\nresult_df = create_shared_actors_df(df)\nresult_df.head()\n\n\n\n\n\n\n\n\nMovie/TV Show\nNumber of Shared Actors\nShared Actors\n\n\n\n\n0\nDue South\n6\n[Philip Williams, James Allodi, David Eisner, ...\n\n\n1\nSaturday Night Live\n5\n[Matt Damon, George Plimpton, Casey Affleck, B...\n\n\n2\nMayday\n5\n[Frank Nakashima, Bruce Hunter, Barna Moricz, ...\n\n\n3\nThe Graham Norton Show\n4\n[Matt Damon, Minnie Driver, Ben Affleck, Robin...\n\n\n4\nThe View\n4\n[Matt Damon, Minnie Driver, Ben Affleck, Robin...\n\n\n\n\n\n\n\n\nVisualization:\nTo visualize the number of shared actors between movies, you can use tools like Matplotlib, plotly, or seaborn in Python. The visualize_shared_actors function creates a visual representation of the number of shared actors in the recomended movies and TV shows using a bar chart from Matplotlib. The bar chart generated highlights movies or TV shows where multiple actors have collaborated, indicating potential relationships or thematic similarities between those productions and the original.\nAlso, the function includes a filter (df[‘Number of Shared Actors’] &gt;= 3) to focus on movies or TV shows with a higher number of shared actors because there are over a thousand recomendations gathered by the scraper, many of which have no shared actors.\n\nimport matplotlib.pyplot as plt\n\ndef visualize_shared_actors(df, overlap):\n    \"\"\"\n    Visualize the number of shared actors in movies/TV shows using a bar chart.\n\n    Arguments:\n        df (pd.DataFrame): DataFrame containing columns 'Movie/TV Show' and 'Number of Shared Actors'.\n\n    Displays a bar chart.\n    \"\"\"\n    \n    shared_df = df[df['Number of Shared Actors'] &gt;= overlap]\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(x = shared_df['Movie/TV Show'], height = shared_df['Number of Shared Actors'], color='skyblue')\n    plt.title('Number of Shared Actors in Movies/TV Shows')\n    plt.xlabel('Movie/TV Show')\n    plt.ylabel('Number of Shared Actors')\n    plt.xticks(rotation=45, ha='right')\n\n    # Display the table\n    plt.show()\n\nHere is a visualization of the shared actors recomendation data using the visualize_shared_actors function:\n\n# Visualize shared actors for movies with two or more shared actors\nvisualize_shared_actors(result_df, 3)\n\n\n\n\n\n\n\n\n\n# Visualize shared actors for movies with two or more shared actors\nvisualize_shared_actors(result_df, 4)\n\n\n\n\n\n\n\n\nThe top movie/tv show recomendations for Good Will Hunting are Due South, Mayday, and War of the Worlds. As we can see one issue that could possibly fixed in the future is exclusing talk shows.\n\n\n\nConclusion\nIn this tutorial, we’ve created a web scraper using Scrapy to extract movie information from TMDb in order to create a recomendation system. By understanding the html structure of the website and using Scrapy, we can gather valuable data for building a movie recommendation system. The visualization at the end allows us to analyze and identify actors with significant contributions to multiple movies for the best recomendations. To further enhance the recomendation system, you could limit the number of actors scraped to only the main cast of the show."
  },
  {
    "objectID": "posts/hw0/index.html",
    "href": "posts/hw0/index.html",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "",
    "text": "Data visualization is an important skill for understanding and drawing insight from datasets. The Palmer Penguins dataset contains data on 344 penguins of three different species within the Palmer Archipelago in Antarctica. In this tutorial, we will explore how to create an interesting data visualization pf the Palmer Penguins dataset using Python."
  },
  {
    "objectID": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "href": "posts/hw0/index.html#step-1-import-libraries-and-load-the-dataset",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 1: Import Libraries and Load the Dataset",
    "text": "Step 1: Import Libraries and Load the Dataset\nWe want to import the pandas library in order to create a Dataframe class object containing all of the information from the palmer_penguins.csv file that we are downloading directly from the given url. Then, we need matplotlib in order to create our visualization. Other libraries can be used for visualizing the data besides matplotlib, but this is the most well known.\n\nimport pandas as pd # Contains the class DataFrame\nimport matplotlib.pyplot as plt # For creating simple graphs\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nA csv file is a file formated to contain data that is seperated by commas for the different collumns. We can easily create a pandas DataFrame object by reading in a csv file of the Palmer Penguins dataset from the url given above.\nAdditionally we want to add names for the columns:\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]"
  },
  {
    "objectID": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "href": "posts/hw0/index.html#step-2-explore-and-clean-the-data",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 2: Explore and Clean the Data",
    "text": "Step 2: Explore and Clean the Data\nBefore creating a visualization, it’s essential to understand the structure and contents of the dataset. First, we want to simply take a look at the database we have created in order to determine if any rows or columns need to be cleaned.\n\npenguins.head() #Shows the first five rows of the dataframe\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n3\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n\n\n\n\n\nThen, we need to clean out any empty datapoints using the dropna method, and we want to make the dataframe easier to read by simplifying the species name.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\nAdditionally, often times data sets contain columns that are not relevant to our study or visualization so they can be deleted from the dataframe for simplicity. In this case, we will keep all of the columns for the different examples given below."
  },
  {
    "objectID": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "href": "posts/hw0/index.html#step-3-choose-an-appropriate-method-of-visualization",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Step 3: Choose an Appropriate Method of Visualization",
    "text": "Step 3: Choose an Appropriate Method of Visualization\nFor the visualization, there are many different options. Your visualization should be based off of what you are trying to analyze in the data. For example, let’s say that I want to compare the size of the culmen of the three different species. For this, I will be using a scatter plot to visualize the clustering of the three groups.\nFirst, I will create a dictionary of the species and colors they should be associated with. Then, I will create a scatter plot for each species. Also, I want to add some additional features to the scatter plot, axis labels, and a title. This makes the graph easier to read and visually appealing.\n\n# library with associated species and colors\ncolors = {'Adelie': 'blue', 'Chinstrap': 'orange', 'Gentoo': 'green'}\n\n# Create and individual scatter plot for each species\nfor species, col in colors.items():\n    species_data = penguins[penguins['Species'] == species]\n    plt.scatter(species_data['Culmen Length (mm)'], species_data['Culmen Depth (mm)'], label=species, color=col)\n\n# Add aditional features to the plot\nplt.legend(title='Species')\nplt.title('Culmen Size of Penguin Species')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, I could also choose to compare the different body masses of each species using a box plot. First, I will need to import a library that could help me do this, like plotly. Then I can use the box method to create a box plot with the Species as the x-axis and the body mass as the y-axis.\n\n# import the relevant library\nimport plotly.express as px\n\n# Create the box plot and add a title\nbox = px.box(penguins, x = 'Species', y = 'Body Mass (g)', color = 'Species')\nbox.update_layout(title_text=\"Body Mass Distribution of Penguin Species\")\n\n# Show the plot\nbox.show()"
  },
  {
    "objectID": "posts/hw0/index.html#conclusion",
    "href": "posts/hw0/index.html#conclusion",
    "title": "Tutorial: Constructing a Data Visualization of the Palmer Penguins Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, you can make many different types of interesting visualization from this dataset that can help you draw different insight. Choosing the right visualization depends on what you’re trying to find, and there are many different libraries and graphs that can help you with that. Once your visualization is created, interpret the results and analyze any patterns or trends observed from the visualization. Feel free to experiment with other types of visualizations and variables from the dataset to create a comprehensive understanding of the data. Remember to share your findings and insights in a clear and concise manner by making your visualizations easy to read and visibly satisfying. Data visualization is not only about creating beautiful plots but also about effectively communicating information."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Gianna Pedroza",
    "section": "",
    "text": "University of California, Los Angeles | Los Angeles, CA B.S. in Applied Mathematics, Data Science Engineering Minor | Sept 2021 - Present"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "Homework 2: Web Scraping Movie Recomendations using Scrapy\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: Data Wrangling and Visualization using NOAA Climate Data\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial: Constructing a Data Visualization of the Palmer Penguins Dataset\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nGianna Pedroza\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "",
    "text": "import pandas as pd\nimport sqlite3"
  },
  {
    "objectID": "posts/hw1/index.html#visualization-1-average-temperature-over-time",
    "href": "posts/hw1/index.html#visualization-1-average-temperature-over-time",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 1: Average Temperature Over Time",
    "text": "Visualization 1: Average Temperature Over Time\n\ndef avgTemp_over_time(db_file, country, year_begin, year_end):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n        SELECT t.Year, AVG(t.Temp) AS avg_temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.ID\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY year\n    \"\"\"\n    \n    # Execute the query and fetch the results into a Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    \n    return df\n\ndef average_temperature_line_plot(db_file, country, year_begin, year_end):\n    # Get data using the query function\n    data = avgTemp_over_time(db_file, country, year_begin, year_end)\n\n    # Create line plot with facets using Plotly Express\n    fig = px.line(data, \n                  x='Year', \n                  y='avg_temp',\n                  labels={'avg_temp': 'Average Temperature (°C)'},\n                  title=f'Average Temperature Over Time in {country} ({year_begin}-{year_end})')\n\n    return fig\n\n\nfig_line = average_temperature_line_plot(\"climate.db\", \"Thailand\", 1980, 2020)\nfig_line.show()"
  },
  {
    "objectID": "posts/hw1/index.html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "href": "posts/hw1/index.html#visualization-2-bar-chart-of-average-temperatures-each-month",
    "title": "Homework 1: Data Wrangling and Visualization using NOAA Climate Data",
    "section": "Visualization 2: Bar Chart of Average Temperatures Each Month",
    "text": "Visualization 2: Bar Chart of Average Temperatures Each Month\n\ndef avg_monthly_temperature_bar_chart(db_file, country, year_begin, year_end):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    query = f\"\"\"\n        SELECT t.Month, AVG(t.Temp) AS avg_temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.ID\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY t.Month\n    \"\"\"\n\n    # Execute the query and fetch the results into a Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Map month numbers to month names for better visualization\n    month_mapping = {\n        1: 'January', 2: 'February', 3: 'March', 4: 'April',\n        5: 'May', 6: 'June', 7: 'July', 8: 'August',\n        9: 'September', 10: 'October', 11: 'November', 12: 'December'\n    }\n    df['Month'] = df['Month'].map(month_mapping)\n\n    # Create a bar chart using Plotly\n    fig = px.bar(df, \n                 x='Month', \n                 y='avg_temp',\n                 labels={'avg_temp': 'Average Temperature (°C)'},\n                 title=f'Average Monthly Temperature in {country} ({year_begin}-{year_end})',\n                 color='avg_temp',\n                 color_continuous_scale='RdBu_r')\n\n    return fig\n\nIn summary, this script connects to our SQLite database, fetches average monthly temperature data for a specified country and year range, and creates an interactive bar chart using Plotly, where the color of each bar represents the temperature value. The resulting visualization provides an representation of how average monthly temperatures vary over time for the selected country.\nThis visualization could help answer the question “How does the average monthly temperature vary over the specified years for a particular country?” This visualization provides insights into the monthly temperature patterns for a given country, allowing users to observe the seasonal temperature changes. By examining the color of the bars, viewers can identify periods of higher or lower average temperatures throughout the specified time range.\nBelow is an example for the function in use to find the average temperatures of each month in Finland between 2000 and 2020:\n\nfig = avg_monthly_temperature_bar_chart('climate.db', 'Finland', 2000, 2020)\nfig.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]